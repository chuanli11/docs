Following is a step by step guide for getting a new researcher up to speed with Run:AI and Kubernetes.

## Change of Paradigms: from Docker to Kubernetes 

As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This [document](docker-to-runai.md) is an attempt to help the researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI.

## Setup the Run:AI Command-Line Interface

Run:AI CLI needs to be installed on the researcher machine. This [document](cli-install.md) provides step by step instructions.

## Provide the Researcher with a GPU Quota

To submit workloads with Run:AI, the researcher must be provided with a "project" which contains a GPU quota. Please see [Working with Projects](../Admin-User-Interface-Setup/Working-with-Projects.md) document on how to create projects and set a quota.

## Provide access to the Run:AI Administration UI

Some organizations would want to provide researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI (<a href="https://app.run.ai" target="_self">app.run.ai)</a>. See [this](https://support.run.ai/hc/en-us/articles/360011591340-Working-with-Admin-UI-Users) document for further information on how to provide access. 

## Schedule an Onboarding Session

It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. 

Researcher onboarding material also appears in the [Researcher Onboarding Presentation](../../Researcher/Presentations/Researcher-Onboarding-Presentation.md).

 
 
 