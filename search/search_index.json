{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Run:AI Documentation Library \u00b6 Welcome to the Run:AI documentation area. For an introduction about what is the Run:AI Platform see Run:AI platform on the run.ai website The Run:AI documentation is targeting three personas: Run:AI Administrator - Responsible for the setup and the day to day administration of the product. Administrator documentation can be found here . Researcher - Using Run:AI to submit jobs. Researcher documentation can be found here . Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here . Example Docker Images \u00b6 Code for the Docker images referred to in these docs is available here . How to get Support \u00b6 To get support use the following channels: Write to support@run.ai . On our website at https://run.ai , under 'Support' use the support form. On the bottom right of the Administrator user interface https://app.run.ai , use the 'Help' widget. On the bottom right of this page , use the Help widget.","title":"Overview"},{"location":"#runai-documentation-library","text":"Welcome to the Run:AI documentation area. For an introduction about what is the Run:AI Platform see Run:AI platform on the run.ai website The Run:AI documentation is targeting three personas: Run:AI Administrator - Responsible for the setup and the day to day administration of the product. Administrator documentation can be found here . Researcher - Using Run:AI to submit jobs. Researcher documentation can be found here . Developer - Using various APIs to manipulate Jobs and integrate with other systems. Developer documentation can be found here .","title":"Run:AI Documentation Library"},{"location":"#example-docker-images","text":"Code for the Docker images referred to in these docs is available here .","title":"Example Docker Images"},{"location":"#how-to-get-support","text":"To get support use the following channels: Write to support@run.ai . On our website at https://run.ai , under 'Support' use the support form. On the bottom right of the Administrator user interface https://app.run.ai , use the 'Help' widget. On the bottom right of this page , use the Help widget.","title":"How to get Support"},{"location":"Administrator/overview-administrator/","text":"Overview: Administrator Documentation \u00b6 The role of Administrators is to set up Run:AI and perform day to day monitoring and maintenance. As part of the Administrator documentation you will find: Cluster Setup . How to setup and modify a GPU cluster with Run:AI Researcher Setup How to setup Researchers to work with Run:AI. Setting and maintaining the cluster via the Administrator User Interface . Introductory Presentations .","title":"Overview"},{"location":"Administrator/overview-administrator/#overview-administrator-documentation","text":"The role of Administrators is to set up Run:AI and perform day to day monitoring and maintenance. As part of the Administrator documentation you will find: Cluster Setup . How to setup and modify a GPU cluster with Run:AI Researcher Setup How to setup Researchers to work with Run:AI. Setting and maintaining the cluster via the Administrator User Interface . Introductory Presentations .","title":"Overview: Administrator Documentation"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/","text":"Introduction \u00b6 Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax. Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress. Ingress \u00b6 Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Setup \u00b6 Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster. A Run:AI cluster is installed by: Accessing the Administrator User Interface Clusters area at https://app.run.ai/clusters Downloading a YAML file runai-operator.yaml and then Applying it to Kubernetes. You must edit the YAML file. Search for localLoadBalancer : localLoadBalancer enabled : true ipRangeFrom : 10.0.2.1 ipRangeTo : 10.0.2.2 Set enabled to true and set the IP range appropriately. To add or change a load balancer after the system has been installed run: kubectl edit runaiconfig runai -n runai Search for localLoadBalancer and edit the above. Then run: kubectl rollout restart deployment runai-metallb-controller -n runai To apply the changes Usage \u00b6 The Researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 --interactive --service-type=ingress \\ --port 8888:8888 --command -- start-notebook.sh --NotebookApp.base_url=test-ingress After submitting a Job through the Run:AI CLI, run: runai list jobs You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the Job name and the port (e.g. https://10.255.174.13/test-ingress-8888 . For further details see CLI command runai submit and Launch an Interactive Workload Quickstart . Alternatives \u00b6 Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <NODE-IP>:<NODE-PORT> regardless of which node the container actually resides in. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress (see example in link below) - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding (see example in link below) - Simple port forwarding allows access to the container via localhost:<Port>. See https://kubernetes.io/docs/concepts/services-networking/service for further details. See Also \u00b6 To learn how to use port forwarding see Quickstart document: Launch an Interactive Build Workload with Connected Ports .","title":"Allow external access to Containers"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#introduction","text":"Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way Researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax. Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#ingress","text":"Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here .","title":"Ingress"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#setup","text":"Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster. A Run:AI cluster is installed by: Accessing the Administrator User Interface Clusters area at https://app.run.ai/clusters Downloading a YAML file runai-operator.yaml and then Applying it to Kubernetes. You must edit the YAML file. Search for localLoadBalancer : localLoadBalancer enabled : true ipRangeFrom : 10.0.2.1 ipRangeTo : 10.0.2.2 Set enabled to true and set the IP range appropriately. To add or change a load balancer after the system has been installed run: kubectl edit runaiconfig runai -n runai Search for localLoadBalancer and edit the above. Then run: kubectl rollout restart deployment runai-metallb-controller -n runai To apply the changes","title":"Setup"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#usage","text":"The Researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 --interactive --service-type=ingress \\ --port 8888:8888 --command -- start-notebook.sh --NotebookApp.base_url=test-ingress After submitting a Job through the Run:AI CLI, run: runai list jobs You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the Job name and the port (e.g. https://10.255.174.13/test-ingress-8888 . For further details see CLI command runai submit and Launch an Interactive Workload Quickstart .","title":"Usage"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#alternatives","text":"Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service from outside the cluster by requesting <NODE-IP>:<NODE-PORT> regardless of which node the container actually resides in. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress (see example in link below) - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding (see example in link below) - Simple port forwarding allows access to the container via localhost:<Port>. See https://kubernetes.io/docs/concepts/services-networking/service for further details.","title":"Alternatives"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#see-also","text":"To learn how to use port forwarding see Quickstart document: Launch an Interactive Build Workload with Connected Ports .","title":"See Also"},{"location":"Administrator/Cluster-Setup/cli-admin-install/","text":"Install the Run:AI Administrator Command-line Interface \u00b6 The Run:AI Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:AI Cluster. The tool is currently in beta. The instructions below will guide you through the process of installing the Administrator CLI. Prerequisites \u00b6 Run:AI Administrator CLI runs on Mac and Linux. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster Kubernetes Configuration \u00b6 The Run:AI Administrator CLI requires a Kubernetes profile with cluster administrative rights. Installation \u00b6 Download the latest release from the Run:AI releases page Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai-adm version Updating the Run:AI Administrator CLI \u00b6 To update the CLI to the latest version run: sudo runai-adm update","title":"Install Administrator CLI"},{"location":"Administrator/Cluster-Setup/cli-admin-install/#install-the-runai-administrator-command-line-interface","text":"The Run:AI Administrator Command-line Interface (Administrator CLI) allows performing administrative tasks on the Run:AI Cluster. The tool is currently in beta. The instructions below will guide you through the process of installing the Administrator CLI.","title":"Install the Run:AI Administrator Command-line Interface"},{"location":"Administrator/Cluster-Setup/cli-admin-install/#prerequisites","text":"Run:AI Administrator CLI runs on Mac and Linux. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster","title":"Prerequisites"},{"location":"Administrator/Cluster-Setup/cli-admin-install/#kubernetes-configuration","text":"The Run:AI Administrator CLI requires a Kubernetes profile with cluster administrative rights.","title":"Kubernetes Configuration"},{"location":"Administrator/Cluster-Setup/cli-admin-install/#installation","text":"Download the latest release from the Run:AI releases page Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai-adm version","title":"Installation"},{"location":"Administrator/Cluster-Setup/cli-admin-install/#updating-the-runai-administrator-cli","text":"To update the CLI to the latest version run: sudo runai-adm update","title":"Updating the Run:AI Administrator CLI"},{"location":"Administrator/Cluster-Setup/cluster-delete/","text":"Deleting a Cluster Installation \u00b6 To delete a Run:AI Cluster installation while retaining existing running jobs, run the following command: runai-adm uninstall The command will not delete existing Jobs submitted by users.","title":"Cluster Delete"},{"location":"Administrator/Cluster-Setup/cluster-delete/#deleting-a-cluster-installation","text":"To delete a Run:AI Cluster installation while retaining existing running jobs, run the following command: runai-adm uninstall The command will not delete existing Jobs submitted by users.","title":"Deleting a Cluster Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/","text":"Below are instructions on how to install Run:AI cluster. Before installing, please review the installation prerequisites here: Run AI GPU Cluster Prerequisites . Step 1: NVIDIA \u00b6 On each machine with GPUs run the following steps 1.1 - 1.4: Step 1.1 Install NVIDIA Drivers \u00b6 If NVIDIA drivers are not already installed on your GPU machines, please install them now. After installing NVIDIA drivers, reboot the machine. Then verify that the installation succeeded by running: nvidia-smi Step 1.2: Install Docker \u00b6 Install Docker by following the steps here: https://docs.docker.com/engine/install/ . Specifically, you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Step 1.3: Install NVIDIA Docker \u00b6 To install NVIDIA Docker on Debian-based distributions (such as Ubuntu), run the following: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For RHEL-based distributions, run: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For a detailed review of the above instructions, see the NVIDIA Docker installation instructions . Step 1.4: Make NVIDIA Docker the default docker runtime \u00b6 Set the NVIDIA runtime as the default Docker runtime on your node. Edit the docker daemon config file at /etc/docker/daemon.json and add the default-runtime key as follows: { \"default-runtime\" : \"nvidia\" , \"runtimes\" : { \"nvidia\" : { \"path\" : \"/usr/bin/nvidia-container-runtime\" , \"runtimeArgs\" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd Step 2: Install Kubernetes \u00b6 There are several good ways to install Kubernetes. A full list can be found here: https://kubernetes.io/docs/setup/ . Two good alternatives: Native installation. For simple Kubernetes installation, the easiest and fastest way to setup Kubernetes is through a Native Kubernetes Installation . Kubespray https://kubespray.io/ . Kubespray uses Ansible scripts. Download the latest stable version of Kubespray from: https://github.com/kubernetes-sigs/kubespray . Note Run:AI is customizing the NVIDIA Kubernetes device plugin . Do not install this software as it is installed by Run:AI. Some best practices on Kubernetes configuration can be found here: Kubernetes Cluster Configuration Best Practices . The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to a functioning Kubernetes cluster. Step 3: Install Run:AI \u00b6 Step 3.1: Install Run:AI \u00b6 Log in to Run:AI Admin UI at https://app.run.ai. Use credentials provided by Run:AI Customer Support. If no clusters are configured, you will see a dialog with instructions on how to install a Run:AI cluster. If a cluster has already been configured, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Please read the next section before proceeding. Step 3.2: Customize Installation \u00b6 The Run:AI Admin UI cluster creation wizard asks you to download a YAML file runai-operator-<cluster-name>.yaml . You must then apply the file to Kubernetes. Before applying to Kubernetes, you may need to edit this file. Examples: Set aside an IP address for ingress access to containers (e.g. for Jupyter Notebooks, PyCharm, VisualStudio Code). See: Allow external access to Containers . Note that you can access containers via port forwarding without requiring an ingress point. Allow outbound internet connectivity in a proxied environment. See: Installing Run:AI with an Internet Proxy Server . Step 4: Verify your Installation \u00b6 Go to https://app.run.ai/dashboards/now . Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line. For a more extensive verification of cluster health, see Determining the health of a cluster . Step 5: (Optional) Set Node Roles \u00b6 When installing a production cluster you may want to: Set one or more Run:AI system nodes. These are nodes dedicated to Run:AI software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:AI to specific nodes in the cluster. To perform these tasks. See Set Node Roles . Next Steps \u00b6 Set up Admin UI Users Working with Admin UI Users . Set up Projects Working with Projects . Set up Researchers to work with the Run:AI Command-line interface (CLI). See Installing the Run AI Command-line Interface on how to install the CLI for users. Set up Project-based Researcher Access Control .","title":"Cluster Install"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-1-nvidia","text":"On each machine with GPUs run the following steps 1.1 - 1.4:","title":"Step 1: NVIDIA"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-11-install-nvidia-drivers","text":"If NVIDIA drivers are not already installed on your GPU machines, please install them now. After installing NVIDIA drivers, reboot the machine. Then verify that the installation succeeded by running: nvidia-smi","title":"Step 1.1 Install NVIDIA Drivers"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-12-install-docker","text":"Install Docker by following the steps here: https://docs.docker.com/engine/install/ . Specifically, you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh","title":"Step 1.2: Install Docker"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-13-install-nvidia-docker","text":"To install NVIDIA Docker on Debian-based distributions (such as Ubuntu), run the following: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For RHEL-based distributions, run: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For a detailed review of the above instructions, see the NVIDIA Docker installation instructions .","title":"Step 1.3: Install NVIDIA Docker"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-14-make-nvidia-docker-the-default-docker-runtime","text":"Set the NVIDIA runtime as the default Docker runtime on your node. Edit the docker daemon config file at /etc/docker/daemon.json and add the default-runtime key as follows: { \"default-runtime\" : \"nvidia\" , \"runtimes\" : { \"nvidia\" : { \"path\" : \"/usr/bin/nvidia-container-runtime\" , \"runtimeArgs\" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd","title":"Step 1.4: Make NVIDIA Docker the default docker runtime"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-2-install-kubernetes","text":"There are several good ways to install Kubernetes. A full list can be found here: https://kubernetes.io/docs/setup/ . Two good alternatives: Native installation. For simple Kubernetes installation, the easiest and fastest way to setup Kubernetes is through a Native Kubernetes Installation . Kubespray https://kubespray.io/ . Kubespray uses Ansible scripts. Download the latest stable version of Kubespray from: https://github.com/kubernetes-sigs/kubespray . Note Run:AI is customizing the NVIDIA Kubernetes device plugin . Do not install this software as it is installed by Run:AI. Some best practices on Kubernetes configuration can be found here: Kubernetes Cluster Configuration Best Practices . The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to a functioning Kubernetes cluster.","title":"Step 2: Install Kubernetes"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-3-install-runai","text":"","title":"Step 3: Install Run:AI"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-31-install-runai","text":"Log in to Run:AI Admin UI at https://app.run.ai. Use credentials provided by Run:AI Customer Support. If no clusters are configured, you will see a dialog with instructions on how to install a Run:AI cluster. If a cluster has already been configured, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Please read the next section before proceeding.","title":"Step 3.1: Install Run:AI"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-32-customize-installation","text":"The Run:AI Admin UI cluster creation wizard asks you to download a YAML file runai-operator-<cluster-name>.yaml . You must then apply the file to Kubernetes. Before applying to Kubernetes, you may need to edit this file. Examples: Set aside an IP address for ingress access to containers (e.g. for Jupyter Notebooks, PyCharm, VisualStudio Code). See: Allow external access to Containers . Note that you can access containers via port forwarding without requiring an ingress point. Allow outbound internet connectivity in a proxied environment. See: Installing Run:AI with an Internet Proxy Server .","title":"Step 3.2: Customize Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-4-verify-your-installation","text":"Go to https://app.run.ai/dashboards/now . Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line. For a more extensive verification of cluster health, see Determining the health of a cluster .","title":"Step 4: Verify your Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-5-optional-set-node-roles","text":"When installing a production cluster you may want to: Set one or more Run:AI system nodes. These are nodes dedicated to Run:AI software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:AI to specific nodes in the cluster. To perform these tasks. See Set Node Roles .","title":"Step 5: (Optional) Set Node Roles"},{"location":"Administrator/Cluster-Setup/cluster-install/#next-steps","text":"Set up Admin UI Users Working with Admin UI Users . Set up Projects Working with Projects . Set up Researchers to work with the Run:AI Command-line interface (CLI). See Installing the Run AI Command-line Interface on how to install the CLI for users. Set up Project-based Researcher Access Control .","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/","text":"Below are the prerequisites of a cluster installed with Run:AI. Kubernetes Software \u00b6 Run:AI requires Kubernetes 1.16 or above. Kubernetes 1.18 is recommended (as of October 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16. NVIDIA Driver \u00b6 Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency . Hardware Requirements \u00b6 (Production only) Dedicated Run:AI System Nodes: To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain at least one, dedicated worker machine, designated for Run:AI Software: 4 CPUs 8GB of RAM 50GB of Disk space Shared data volume: Run:AI uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, so as to access training data and code as well as save checkpoints, weights, and other machine-learning related artifacts. The Run:AI system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). NFS is usually the preferred method for Researchers which may require multi-read/write capabilities. Docker Registry With Run:AI, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-premise (preferably on a dedicated machine). Run:AI can assist with setting up the repository. Kubernetes : Though out of scope for this document, Production Kubernetes installation requires separate nodes for the Kubernetes master. Network Requirements \u00b6 Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied: During Installation \u00b6 Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443 Post Installation \u00b6 In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443 User requirements \u00b6 Usage of containers and images: The individual Researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software.","title":"Prerequisites"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#kubernetes-software","text":"Run:AI requires Kubernetes 1.16 or above. Kubernetes 1.18 is recommended (as of October 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16.","title":"Kubernetes Software"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#nvidia-driver","text":"Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency .","title":"NVIDIA Driver"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#hardware-requirements","text":"(Production only) Dedicated Run:AI System Nodes: To reduce downtime and save CPU cycles on expensive GPU Machines, we recommend that production deployments will contain at least one, dedicated worker machine, designated for Run:AI Software: 4 CPUs 8GB of RAM 50GB of Disk space Shared data volume: Run:AI uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, so as to access training data and code as well as save checkpoints, weights, and other machine-learning related artifacts. The Run:AI system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). NFS is usually the preferred method for Researchers which may require multi-read/write capabilities. Docker Registry With Run:AI, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-premise (preferably on a dedicated machine). Run:AI can assist with setting up the repository. Kubernetes : Though out of scope for this document, Production Kubernetes installation requires separate nodes for the Kubernetes master.","title":"Hardware Requirements"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#network-requirements","text":"Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied:","title":"Network Requirements"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#during-installation","text":"Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443","title":"During Installation"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#post-installation","text":"In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443","title":"Post Installation"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#user-requirements","text":"Usage of containers and images: The individual Researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software.","title":"User requirements"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/","text":"This section is a step by step guide for setting up a Run:AI cluster. A Run:AI cluster is installed on top of a Kubernetes cluster. A Run:AI cluster connects to the Run:AI backend on the cloud. The backend provides a control point as well as a monitoring and control user interface for Administrators. A customer may have multiple Run:AI Clusters, all connecting to a single backend. For additional details see the Run:AI system components Documents \u00b6 Review Run:AI cluster prerequisites . Step by step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions. In Addition, you can use our Quick installation guide which installs Kubernetes together with Run:AI on a single node. Customization \u00b6 As part of the installation process, you will download a Run:AI Operator YAML file and apply it to the cluster. Before applying the file, you will often need to customize the file. For a list of customization see Cluster Install, section 3.2 Advanced Setup \u00b6 For advanced scenarios such as Researcher authentication & authorization , limiting the installation to specific cluster nodes or enforcing none-root containers see the Advanced section. Next Steps \u00b6 After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup .","title":"Introduction"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#documents","text":"Review Run:AI cluster prerequisites . Step by step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions. In Addition, you can use our Quick installation guide which installs Kubernetes together with Run:AI on a single node.","title":"Documents"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#customization","text":"As part of the installation process, you will download a Run:AI Operator YAML file and apply it to the cluster. Before applying the file, you will often need to customize the file. For a list of customization see Cluster Install, section 3.2","title":"Customization"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#advanced-setup","text":"For advanced scenarios such as Researcher authentication & authorization , limiting the installation to specific cluster nodes or enforcing none-root containers see the Advanced section.","title":"Advanced Setup"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#next-steps","text":"After setting up the cluster, you may want to start setting up Researchers. See: Researcher Setup .","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/","text":"Troubleshooting \u00b6 Determining Cluster Health \u00b6 Following are a set of tests to run in order to determine cluster health: 1. Verify that the Run:AI services are running \u00b6 Run: kubectl get pods -n runai Verify that all pods are in Running status. Run: kubectl get deployments -n runai kubectl get sts -n runai Check that all items (deployments and StatefulSets alike) are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:AI daemon-sets run on all nodes. Others run only on nodes which contain GPUs. Verify that for all daemon-sets the desired number is equal to current and to ready . 2. Verify that data is sent to the cloud \u00b6 Log in to https://app.run.ai/dashboards/now Verify that all metrics in the overview dashboard are showing. Specifically the list of nodes and the numeric indicators Go to Projects and create a new Project. Find the new Project using the CLI command: runai list projects 3. Submit a Job \u00b6 Submitting a Job will allow you to verify that the Run:AI scheduling service is in order. Make sure that the Project you have created has a quota of at least 1 GPU Run: runai config project <project-name> runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the Job is a Running state when running: runai list jobs Verify that the Job is showing in the Jobs area in app.run.ai/jobs Symptom: Metrics are not showing on Overview Dashboard \u00b6 Some or all metrics are not showing in https://app.run.ai/dashboards/now Typical root causes: NVIDIA prerequisites have not been met. Firewall related issues. Internal clock is not synced. NVIDIA prerequisites have not been met Run: runai pods -n runai | grep nvidia Select one of the nvidia pods and run: kubectl logs -n runai nvidia-device-plugin-daemonset-<id> If the log contains an error, it means that NVIDIA related prerequisites have not been met. Review step 1 in NVIDIA prerequisites . Verify that: Step 1.1: NVIDIA drivers are installed Step 1.2: NVIDIA Docker is installed. A typical issue here is the installation of the NVIDIA Container Toolkit instead of NVIDIA Docker 2 . Step 1.3: Verify that NVIDIA Docker is the default docker runtime If the system has recently been installed, verify that docker has restarted by running the aforementioned pkill command Check the status of Docker by running: sudo systemctl status docker Firewall issues Add verbosity to Prometheus by editing RunaiConfig: kubectl edit runaiconfig runai -n runai Add a debug log level: prometheus-operator : prometheus : prometheusSpec : logLevel : debug Run: kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\ -n runai -f --tail 100 Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server Machine Clocks are not synced Run: date on cluster nodes and verify that date/time is correct. If not, Set the Linux time service (NTP). Restart Run:AI services. Depending on the previous time gap between servers, you may need to reinstall the Run:AI cluster Symptom: Projects are not syncing \u00b6 Create a Project on the Admin UI, then run: runai list projects . The new Project does not appear. Typical root cause: The Run:AI agent is not syncing properly. This may be due to: A dependency on the internal Run:AI database. See separate symptom below Firewall issues Run: runai pods -n runai | grep agent See if the agent is in Running state. Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server Symptom: Internal Database has not started \u00b6 Run: runai pods -n runai | grep runai-db-0 The status of the Run:AI database is not Running Typical root causes: More than one default storage class is installed Incompatible NFS version More than one default storage class is installed The Run:AI Cluster installation includes, by default, a storage class named local path provisioner which is installed as a default storage class. In some cases, your k8s cluster may already have a default storage class installed. In such cases you should disable the local path provisioner. Having two default storage classes will disable both the internal database and some of the metrics. Run: kubectl get storageclass And look for default storage classes. Run: kubectl describe pod -n runai runai-db-0 See that there is indeed a storage class error appearing To disable local path provisioner, run: kubectl edit runaiconfig -n runai Add the following lines under spec : local-path-provisioner : enabled : false Incompatible NFS version Default NFS Protocol level is currently 4. If your NFS requires an older version, you may need to add the option as follows. Run: kubectl edit runaiconfig runai -n runai Add mountOptions as follows: nfs-client-provisioner : nfs : mountOptions : [ \"nfsvers=3\" ] Symptom: Cluster Installation failed on Rancher-based Kubernetes (RKE) \u00b6 Cluster is not installed. When running kubectl get pods -n runai you see that pod init-ca has not started Resolution During initialization, Run:AI creates a Certificate Signing Request (CSR) which needs to be approved by the cluster's Certificate Authority (CA). In RKE, this is not enabled by default, and the paths to your Certificate Authority's keypair must be referenced manually by adding the following parameters inside your cluster.yml file, under kube-controller: kube-controller : extra_args : cluster-signing-cert-file : /etc/kubernetes/ssl/kube-ca.pem cluster-signing-key-file : /etc/kubernetes/ssl/kube-ca-key.pem For further information see here . Diagnostic Tools \u00b6 Adding Verbosity to Database container \u00b6 Run: kubectl edit runaiconfig runai -n runai Under spec , add: spec : postgresql : image : debug : true Then view the log by running: kubectl logs -n runai runa-db-0 Internal Networking Issues \u00b6 Run:AI is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there. You can find further information on how to debug Kubernetes DNS here . Specifically, it is useful to start a Pod with networking utilities and use it for network resolution: kubectl run -i --tty netutils --image=dersimn/netutils -- bash","title":"Troubleshooting"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#determining-cluster-health","text":"Following are a set of tests to run in order to determine cluster health:","title":"Determining Cluster Health"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#1-verify-that-the-runai-services-are-running","text":"Run: kubectl get pods -n runai Verify that all pods are in Running status. Run: kubectl get deployments -n runai kubectl get sts -n runai Check that all items (deployments and StatefulSets alike) are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:AI daemon-sets run on all nodes. Others run only on nodes which contain GPUs. Verify that for all daemon-sets the desired number is equal to current and to ready .","title":"1. Verify that the Run:AI services are running"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#2-verify-that-data-is-sent-to-the-cloud","text":"Log in to https://app.run.ai/dashboards/now Verify that all metrics in the overview dashboard are showing. Specifically the list of nodes and the numeric indicators Go to Projects and create a new Project. Find the new Project using the CLI command: runai list projects","title":"2. Verify that data is sent to the cloud"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#3-submit-a-job","text":"Submitting a Job will allow you to verify that the Run:AI scheduling service is in order. Make sure that the Project you have created has a quota of at least 1 GPU Run: runai config project <project-name> runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the Job is a Running state when running: runai list jobs Verify that the Job is showing in the Jobs area in app.run.ai/jobs","title":"3. Submit a Job"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#symptom-metrics-are-not-showing-on-overview-dashboard","text":"Some or all metrics are not showing in https://app.run.ai/dashboards/now Typical root causes: NVIDIA prerequisites have not been met. Firewall related issues. Internal clock is not synced. NVIDIA prerequisites have not been met Run: runai pods -n runai | grep nvidia Select one of the nvidia pods and run: kubectl logs -n runai nvidia-device-plugin-daemonset-<id> If the log contains an error, it means that NVIDIA related prerequisites have not been met. Review step 1 in NVIDIA prerequisites . Verify that: Step 1.1: NVIDIA drivers are installed Step 1.2: NVIDIA Docker is installed. A typical issue here is the installation of the NVIDIA Container Toolkit instead of NVIDIA Docker 2 . Step 1.3: Verify that NVIDIA Docker is the default docker runtime If the system has recently been installed, verify that docker has restarted by running the aforementioned pkill command Check the status of Docker by running: sudo systemctl status docker Firewall issues Add verbosity to Prometheus by editing RunaiConfig: kubectl edit runaiconfig runai -n runai Add a debug log level: prometheus-operator : prometheus : prometheusSpec : logLevel : debug Run: kubectl logs prometheus-runai-prometheus-operator-prometheus-0 prometheus \\ -n runai -f --tail 100 Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server Machine Clocks are not synced Run: date on cluster nodes and verify that date/time is correct. If not, Set the Linux time service (NTP). Restart Run:AI services. Depending on the previous time gap between servers, you may need to reinstall the Run:AI cluster","title":"Symptom: Metrics are not showing on Overview Dashboard"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#symptom-projects-are-not-syncing","text":"Create a Project on the Admin UI, then run: runai list projects . The new Project does not appear. Typical root cause: The Run:AI agent is not syncing properly. This may be due to: A dependency on the internal Run:AI database. See separate symptom below Firewall issues Run: runai pods -n runai | grep agent See if the agent is in Running state. Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server","title":"Symptom: Projects are not syncing"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#symptom-internal-database-has-not-started","text":"Run: runai pods -n runai | grep runai-db-0 The status of the Run:AI database is not Running Typical root causes: More than one default storage class is installed Incompatible NFS version More than one default storage class is installed The Run:AI Cluster installation includes, by default, a storage class named local path provisioner which is installed as a default storage class. In some cases, your k8s cluster may already have a default storage class installed. In such cases you should disable the local path provisioner. Having two default storage classes will disable both the internal database and some of the metrics. Run: kubectl get storageclass And look for default storage classes. Run: kubectl describe pod -n runai runai-db-0 See that there is indeed a storage class error appearing To disable local path provisioner, run: kubectl edit runaiconfig -n runai Add the following lines under spec : local-path-provisioner : enabled : false Incompatible NFS version Default NFS Protocol level is currently 4. If your NFS requires an older version, you may need to add the option as follows. Run: kubectl edit runaiconfig runai -n runai Add mountOptions as follows: nfs-client-provisioner : nfs : mountOptions : [ \"nfsvers=3\" ]","title":"Symptom: Internal Database has not started"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#symptom-cluster-installation-failed-on-rancher-based-kubernetes-rke","text":"Cluster is not installed. When running kubectl get pods -n runai you see that pod init-ca has not started Resolution During initialization, Run:AI creates a Certificate Signing Request (CSR) which needs to be approved by the cluster's Certificate Authority (CA). In RKE, this is not enabled by default, and the paths to your Certificate Authority's keypair must be referenced manually by adding the following parameters inside your cluster.yml file, under kube-controller: kube-controller : extra_args : cluster-signing-cert-file : /etc/kubernetes/ssl/kube-ca.pem cluster-signing-key-file : /etc/kubernetes/ssl/kube-ca-key.pem For further information see here .","title":"Symptom: Cluster Installation failed on Rancher-based Kubernetes (RKE)"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#diagnostic-tools","text":"","title":"Diagnostic Tools"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#adding-verbosity-to-database-container","text":"Run: kubectl edit runaiconfig runai -n runai Under spec , add: spec : postgresql : image : debug : true Then view the log by running: kubectl logs -n runai runa-db-0","title":"Adding Verbosity to Database container"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#internal-networking-issues","text":"Run:AI is based on Kubernetes. Kubernetes runs its own internal subnet with a separate DNS service. If you see in the logs that services have trouble connecting, the problem may reside there. You can find further information on how to debug Kubernetes DNS here . Specifically, it is useful to start a Pod with networking utilities and use it for network resolution: kubectl run -i --tty netutils --image=dersimn/netutils -- bash","title":"Internal Networking Issues"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/","text":"Upgrading a Cluster Installation \u00b6 To perform the tasks below you will need the Run:AI Administrator CLI. See Installing the Run:AI Administrator Command-line Interface . Find the current Run:AI cluster version \u00b6 To find the current version of the Run:AI cluster, run: runai-adm get version Upgrade \u00b6 To upgrade a Run:AI cluster, run: runai-adm upgrade -v <NEW_VERSION> Replace <NEW_VERSION> with a version number you receive from Run:AI customer support. To verify that the upgrade has succeeded run: kubectl get pods -n runai Verify that all pods are running or completed.","title":"Cluster Upgrade"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#upgrading-a-cluster-installation","text":"To perform the tasks below you will need the Run:AI Administrator CLI. See Installing the Run:AI Administrator Command-line Interface .","title":"Upgrading a Cluster Installation"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#find-the-current-runai-cluster-version","text":"To find the current version of the Run:AI cluster, run: runai-adm get version","title":"Find the current Run:AI cluster version"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#upgrade","text":"To upgrade a Run:AI cluster, run: runai-adm upgrade -v <NEW_VERSION> Replace <NEW_VERSION> with a version number you receive from Run:AI customer support. To verify that the upgrade has succeeded run: kubectl get pods -n runai Verify that all pods are running or completed.","title":"Upgrade"},{"location":"Administrator/Cluster-Setup/install-k8s/","text":"Native Kubeneretes Installation \u00b6 Kubernetes is composed of master(s) and workers. The instructions below are for creating a bare-bones installation of a single master and a number of workers. For a more complex Kubernetes installation, use tools such as Kubespray https://kubespray.io/ , or review Kubernetes documentation to learn how to customize the native installation. Prerequisites: \u00b6 All machines have Ubuntu 18.04 or Ubuntu 20.04 Run on Master Node \u00b6 If not yet installed, install docker by performing the instructions here . Specifically, you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Restart the docker service: sudo systemctl restart docker Install Kubernetes master: sudo sh -c 'cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF' sudo apt-get update && sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .19.6-00 kubeadm = 1 .19.6-00 kubectl = 1 .19.6-00 sudo swapoff -a sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --kubernetes-version = v1.19.6 The kubeadm init command above has emitted as output a kubeadm join command. Save it for joining the workers below. Copy the Kubernetes configuration files which provides access to the cluster: mkdir .kube sudo cp -i /etc/kubernetes/admin.conf .kube/config sudo chown $( id -u ) : $( id -g ) .kube/config Add Kubernetes networking: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Test that Kubernetes is up and running: kubectl get nodes Verify that the master node is ready Run on Kubernetes Workers \u00b6 If not yet installed, install docker by performing the instructions here: https://docs.docker.com/engine/install/ubuntu/. Specifically you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Restart the docker service: sudo systemctl restart docker On Worker Nodes with Kubernetes, install NVIDIA Docker and make it the default docker runtime as described here : Install Kubernetes worker: sudo sh -c 'cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF' sudo apt-get update && sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .19.6-00 kubeadm = 1 .19.6-00 sudo swapoff -a Replace the following join command with the one saved from the init command above: sudo kubeadm join 10 .0.0.3:6443 --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Note The default token expires after 24 hours. If the token has expired, go to the master node and run sudo kubeadm token create --print-join-command . This will produce an up to date join command. Return to the master node. Re-run kubectl get nodes and verify that the new node is ready. Permanently disable swap on all nodes \u00b6 Edit the file /etc/fstab Comment out any swap entry if such exists","title":"Kubernetes Install"},{"location":"Administrator/Cluster-Setup/install-k8s/#native-kubeneretes-installation","text":"Kubernetes is composed of master(s) and workers. The instructions below are for creating a bare-bones installation of a single master and a number of workers. For a more complex Kubernetes installation, use tools such as Kubespray https://kubespray.io/ , or review Kubernetes documentation to learn how to customize the native installation.","title":"Native Kubeneretes Installation"},{"location":"Administrator/Cluster-Setup/install-k8s/#prerequisites","text":"All machines have Ubuntu 18.04 or Ubuntu 20.04","title":"Prerequisites:"},{"location":"Administrator/Cluster-Setup/install-k8s/#run-on-master-node","text":"If not yet installed, install docker by performing the instructions here . Specifically, you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Restart the docker service: sudo systemctl restart docker Install Kubernetes master: sudo sh -c 'cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF' sudo apt-get update && sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .19.6-00 kubeadm = 1 .19.6-00 kubectl = 1 .19.6-00 sudo swapoff -a sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 --kubernetes-version = v1.19.6 The kubeadm init command above has emitted as output a kubeadm join command. Save it for joining the workers below. Copy the Kubernetes configuration files which provides access to the cluster: mkdir .kube sudo cp -i /etc/kubernetes/admin.conf .kube/config sudo chown $( id -u ) : $( id -g ) .kube/config Add Kubernetes networking: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Test that Kubernetes is up and running: kubectl get nodes Verify that the master node is ready","title":"Run on Master Node"},{"location":"Administrator/Cluster-Setup/install-k8s/#run-on-kubernetes-workers","text":"If not yet installed, install docker by performing the instructions here: https://docs.docker.com/engine/install/ubuntu/. Specifically you can use a convenience script provided in the document: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Restart the docker service: sudo systemctl restart docker On Worker Nodes with Kubernetes, install NVIDIA Docker and make it the default docker runtime as described here : Install Kubernetes worker: sudo sh -c 'cat <<EOF > /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF' sudo apt-get update && sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .19.6-00 kubeadm = 1 .19.6-00 sudo swapoff -a Replace the following join command with the one saved from the init command above: sudo kubeadm join 10 .0.0.3:6443 --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Note The default token expires after 24 hours. If the token has expired, go to the master node and run sudo kubeadm token create --print-join-command . This will produce an up to date join command. Return to the master node. Re-run kubectl get nodes and verify that the new node is ready.","title":"Run on Kubernetes Workers"},{"location":"Administrator/Cluster-Setup/install-k8s/#permanently-disable-swap-on-all-nodes","text":"Edit the file /etc/fstab Comment out any swap entry if such exists","title":"Permanently disable swap on all nodes"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/","text":"Safely Remove a Node \u00b6 Every now and again, you may need to take down a node. Typically for maintenance purposes. The node about to be taken down may be running Jobs. Without additional preparations, these Jobs will abruptly come to an end, and wait for the node to come up. To allow the Jobs to gracefully shut-down and be immediately re-allocated to other nodes, perform the following: To get the name of the node, run: kubectl get nodes Then run: kubectl drain <NODE-NAME> The command will tell Kubernetes to not schedule any new jobs on the node and evict all currently running Jobs. Kubernetes will attempt to immediately schedule evicted Jobs to other nodes. You can then safely shutdown the node. When the node is up again, run: kubectl uncordon <NODE-NAME> The command tells Kubernetes that the node is ready again to accept Jobs. Node Memory Management \u00b6 It is possible for Researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine). Symptoms \u00b6 The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue appears on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes: Resource Reservation \u00b6 Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup . This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved : cpu : 100m memory : 1G systemReserved : cpu : 100m memory : 1G Eviction \u00b6 Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard : memory.available : \"500Mi\" # Default value for evictionHard on kubelet nodefs.available : \"10%\" nodefs.inodesFree : \"5%\" imagefs.available : \"15%\" Please note that specifying values for evictionHard will override the default values on kubelet which are of very high importance. For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ .","title":"Kubernetes Best Practices"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#safely-remove-a-node","text":"Every now and again, you may need to take down a node. Typically for maintenance purposes. The node about to be taken down may be running Jobs. Without additional preparations, these Jobs will abruptly come to an end, and wait for the node to come up. To allow the Jobs to gracefully shut-down and be immediately re-allocated to other nodes, perform the following: To get the name of the node, run: kubectl get nodes Then run: kubectl drain <NODE-NAME> The command will tell Kubernetes to not schedule any new jobs on the node and evict all currently running Jobs. Kubernetes will attempt to immediately schedule evicted Jobs to other nodes. You can then safely shutdown the node. When the node is up again, run: kubectl uncordon <NODE-NAME> The command tells Kubernetes that the node is ready again to accept Jobs.","title":"Safely Remove a Node"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#node-memory-management","text":"It is possible for Researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine).","title":"Node Memory Management"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#symptoms","text":"The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue appears on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes:","title":"Symptoms"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#resource-reservation","text":"Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup . This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved : cpu : 100m memory : 1G systemReserved : cpu : 100m memory : 1G","title":"Resource Reservation"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#eviction","text":"Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard : memory.available : \"500Mi\" # Default value for evictionHard on kubelet nodefs.available : \"10%\" nodefs.inodesFree : \"5%\" imagefs.available : \"15%\" Please note that specifying values for evictionHard will override the default values on kubelet which are of very high importance. For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ .","title":"Eviction"},{"location":"Administrator/Cluster-Setup/node-downtime/","text":"Planned and Unplanned Node Downtime \u00b6 Introduction \u00b6 Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue. The purpose of this document is to provide a process for retaining the Run:AI service and Researcher workloads during and after the downtime. Node Types \u00b6 The document differentiates between Run:AI System Worker Nodes and GPU Worker Nodes : Worker Nodes - are where Machine Learning workloads run. Run:AI System Nodes - In a production installation Run:AI software runs on one or more Run:AI System Nodes on which the Run:AI software runs. Worker Nodes \u00b6 Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether, for planned maintenance, or an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible. Training vs. Interactive \u00b6 Run:AI differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down. Planned Maintenance \u00b6 Before stopping a Worker node, perform the following: Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution: kubectl drain <node_name> --delete-local-data --ignore-daemonsets Shutdown the node and perform the required maintenance. When done, start the node and then run: kubectl uncordon <node-name> Unplanned Downtime \u00b6 If a node has failed and has immediately restarted then all services will automatically start and there is nothing that needs doing. If a node is to remain down for some time you will want to drain the node so that workloads will migrate to another node: kubectl drain <node_name> --delete-local-data --ignore-daemonsets When the node is up again, run: kubectl uncordon <node-name> If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run: kubectl delete node <node-name> However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin . Run:AI System Nodes \u00b6 In a production installation, Run:AI software runs on one or more Run:AI system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:AI system node to complete the process below. Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above. Rejoin Node into Kubernetes Cluster \u00b6 To rejoin a node to the cluster follow the following steps: On the master node, run: kubeadm token create --print-join-command This would output a kubeadm join command. Run the command on the worker node in order for it to re-join the Kubernetes cluster. Verify that the node is joined by running: kubectl get nodes When the machine is up you will need to re-label nodes according to their role","title":"Node Downtime"},{"location":"Administrator/Cluster-Setup/node-downtime/#planned-and-unplanned-node-downtime","text":"","title":"Planned and Unplanned Node Downtime"},{"location":"Administrator/Cluster-Setup/node-downtime/#introduction","text":"Nodes (Machines) that are part of the cluster are susceptible to occasional downtime. This can be either as part of planned maintenance where we bring down the node for a specified time in an orderly fashion or an unplanned downtime where the machine abruptly stops due to a software or hardware issue. The purpose of this document is to provide a process for retaining the Run:AI service and Researcher workloads during and after the downtime.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/node-downtime/#node-types","text":"The document differentiates between Run:AI System Worker Nodes and GPU Worker Nodes : Worker Nodes - are where Machine Learning workloads run. Run:AI System Nodes - In a production installation Run:AI software runs on one or more Run:AI System Nodes on which the Run:AI software runs.","title":"Node Types"},{"location":"Administrator/Cluster-Setup/node-downtime/#worker-nodes","text":"Worker Nodes are where machine learning workloads run. Ideally, when a node is down, whether, for planned maintenance, or an abrupt downtime, these workloads should migrate to other available nodes or wait in the queue to be started when possible.","title":"Worker Nodes"},{"location":"Administrator/Cluster-Setup/node-downtime/#training-vs-interactive","text":"Run:AI differentiates between Training and Interactive workloads. The key difference at node downtime is that Training workloads will automatically move to a new node while Interactive workloads require a manual process. The manual process is recommended for Training workloads as well, as it hastens the process -- it takes time for Kubernetes to identify that a node is down.","title":"Training vs. Interactive"},{"location":"Administrator/Cluster-Setup/node-downtime/#planned-maintenance","text":"Before stopping a Worker node, perform the following: Stop the Kubernetes scheduler from starting new workloads on the node and drain node from all existing workloads. Workloads will move to other nodes or await on queue for renewed execution: kubectl drain <node_name> --delete-local-data --ignore-daemonsets Shutdown the node and perform the required maintenance. When done, start the node and then run: kubectl uncordon <node-name>","title":"Planned Maintenance"},{"location":"Administrator/Cluster-Setup/node-downtime/#unplanned-downtime","text":"If a node has failed and has immediately restarted then all services will automatically start and there is nothing that needs doing. If a node is to remain down for some time you will want to drain the node so that workloads will migrate to another node: kubectl drain <node_name> --delete-local-data --ignore-daemonsets When the node is up again, run: kubectl uncordon <node-name> If the node is to be permanently shut down, you can remove it completely from Kubernetes. Run: kubectl delete node <node-name> However, if you plan to bring back the node, you will need to rejoin the node into the cluster. See Rejoin .","title":"Unplanned Downtime"},{"location":"Administrator/Cluster-Setup/node-downtime/#runai-system-nodes","text":"In a production installation, Run:AI software runs on one or more Run:AI system nodes. As a best practice, it's best to have more than one such node so that during planned maintenance or unplanned downtime of a single node, the other node will take over. If a second node does not exist, you will have to designate an arbitrary node on the cluster as a Run:AI system node to complete the process below. Protocols for planned maintenance and unplanned downtime are identical to Worker Nodes. See the section above.","title":"Run:AI System Nodes"},{"location":"Administrator/Cluster-Setup/node-downtime/#rejoin-node-into-kubernetes-cluster","text":"To rejoin a node to the cluster follow the following steps: On the master node, run: kubeadm token create --print-join-command This would output a kubeadm join command. Run the command on the worker node in order for it to re-join the Kubernetes cluster. Verify that the node is joined by running: kubectl get nodes When the machine is up you will need to re-label nodes according to their role","title":"Rejoin Node into Kubernetes Cluster"},{"location":"Administrator/Cluster-Setup/node-roles/","text":"Designating Specific Role Nodes \u00b6 When installing a production cluster you may want to: Set one or more Run:AI system nodes. These are nodes dedicated to Run:AI software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:AI to specific nodes in the cluster. To perform these tasks you will need the Run:AI Administrator CLI. See Installing the Run:AI Administrator Command-line Interface . Dedicated Run:AI System Nodes \u00b6 Find out the names of the nodes designated for the Run:AI system by running kubectl get nodes . For each such node run: runai-adm set node-role --runai-system-worker <node-name> If you re-run kubectl get nodes you will see the node role of these nodes changed to runai-system To remove the runai-system node role run: runai-adm remove node-role --runai-system-worker <node-name> Dedicated GPU & CPU Nodes \u00b6 Separate nodes into those that: * Run GPU workloads * Run CPU workloads * Do not run Run:AI at all. these jobs will not be monitored using the Run:AI Administration User interface. Review nodes names using kubectl get nodes . For each such node run: runai-adm set node-role --gpu-worker <node-name> or runai-adm set node-role --cpu-worker <node-name> Nodes not marked as GPU worker or CPU worker will not run Run:AI at all. To set all workers not running runai-system as GPU workers run: runai-adm set node-role --all <node-name> To remove the CPU or GPU worker node role run: runai-adm remove node-role --cpu-worker <node-name> or runai-adm remove node-role --gpu-worker <node-name>","title":"Set Node Roles"},{"location":"Administrator/Cluster-Setup/node-roles/#designating-specific-role-nodes","text":"When installing a production cluster you may want to: Set one or more Run:AI system nodes. These are nodes dedicated to Run:AI software. Machine learning frequently requires jobs that require CPU but not GPU . You may want to direct these jobs to dedicated nodes that do not have GPUs, so as not to overload these machines. Limit Run:AI to specific nodes in the cluster. To perform these tasks you will need the Run:AI Administrator CLI. See Installing the Run:AI Administrator Command-line Interface .","title":"Designating Specific Role Nodes"},{"location":"Administrator/Cluster-Setup/node-roles/#dedicated-runai-system-nodes","text":"Find out the names of the nodes designated for the Run:AI system by running kubectl get nodes . For each such node run: runai-adm set node-role --runai-system-worker <node-name> If you re-run kubectl get nodes you will see the node role of these nodes changed to runai-system To remove the runai-system node role run: runai-adm remove node-role --runai-system-worker <node-name>","title":"Dedicated Run:AI System Nodes"},{"location":"Administrator/Cluster-Setup/node-roles/#dedicated-gpu-cpu-nodes","text":"Separate nodes into those that: * Run GPU workloads * Run CPU workloads * Do not run Run:AI at all. these jobs will not be monitored using the Run:AI Administration User interface. Review nodes names using kubectl get nodes . For each such node run: runai-adm set node-role --gpu-worker <node-name> or runai-adm set node-role --cpu-worker <node-name> Nodes not marked as GPU worker or CPU worker will not run Run:AI at all. To set all workers not running runai-system as GPU workers run: runai-adm set node-role --all <node-name> To remove the CPU or GPU worker node role run: runai-adm remove node-role --cpu-worker <node-name> or runai-adm remove node-role --gpu-worker <node-name>","title":"Dedicated GPU &amp; CPU Nodes"},{"location":"Administrator/Cluster-Setup/non-root-containers/","text":"Introduction \u00b6 In docker, as well as in Kubernetes, the default for running containers is running as 'root'. The implication of running as root is that processes running within the container have enough permissions to change anything on the machine itself. This gives a lot of power to containers, but does not sit well with modern security standards. Specifically enterprise security. Non-root Containers \u00b6 There are two runai submit flags which limit this behavior at the Researcher level: The flag --run-as-user starts the container without root access. The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.). For more information see Privilege Escalation . However, these flags are voluntary. They are not enforced by the system. It is possible to set these flags as a cluster-wide default for the Run:AI CLI, such that all CLI users will be limited to non-root containers. Setting a Cluster-Wide Default \u00b6 Save the following in a file (cluster-config.yaml) apiVersion : v1 data : config : | enforceRunAsUser: true enforcePreventPrivilegeEscalation: true kind : ConfigMap metadata : name : cluster-config namespace : runai labels : runai/cluster-config : \"true\" Run: kubectl apply -f cluster-config.yaml Limitation This configuration limits non-root for all Run:AI CLI users. However, it does not prevent users or malicious actors from starting containers directly via Kubernetes API (e.g. via YAML files). There are third party enterprise tools that can provide this level of security. Creating a Temporary Home Directory \u00b6 For containers to run as a specific user, the user needs to have a pre-created home directory within the image. This can be a daunting IT task. To overcome this, Run:AI provides an additional flag --create-home-dir . Adding this flag creates a temporary home directory for the user within the container. Notes Data saved in this directory will not be saved when the container exits. This flag is set by default to true when the --run-as-user flag is used, and false if not.","title":"Non-root Containers"},{"location":"Administrator/Cluster-Setup/non-root-containers/#introduction","text":"In docker, as well as in Kubernetes, the default for running containers is running as 'root'. The implication of running as root is that processes running within the container have enough permissions to change anything on the machine itself. This gives a lot of power to containers, but does not sit well with modern security standards. Specifically enterprise security.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/non-root-containers/#non-root-containers","text":"There are two runai submit flags which limit this behavior at the Researcher level: The flag --run-as-user starts the container without root access. The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.). For more information see Privilege Escalation . However, these flags are voluntary. They are not enforced by the system. It is possible to set these flags as a cluster-wide default for the Run:AI CLI, such that all CLI users will be limited to non-root containers.","title":"Non-root Containers"},{"location":"Administrator/Cluster-Setup/non-root-containers/#setting-a-cluster-wide-default","text":"Save the following in a file (cluster-config.yaml) apiVersion : v1 data : config : | enforceRunAsUser: true enforcePreventPrivilegeEscalation: true kind : ConfigMap metadata : name : cluster-config namespace : runai labels : runai/cluster-config : \"true\" Run: kubectl apply -f cluster-config.yaml Limitation This configuration limits non-root for all Run:AI CLI users. However, it does not prevent users or malicious actors from starting containers directly via Kubernetes API (e.g. via YAML files). There are third party enterprise tools that can provide this level of security.","title":"Setting a Cluster-Wide Default"},{"location":"Administrator/Cluster-Setup/non-root-containers/#creating-a-temporary-home-directory","text":"For containers to run as a specific user, the user needs to have a pre-created home directory within the image. This can be a daunting IT task. To overcome this, Run:AI provides an additional flag --create-home-dir . Adding this flag creates a temporary home directory for the user within the container. Notes Data saved in this directory will not be saved when the container exits. This flag is set by default to true when the --run-as-user flag is used, and false if not.","title":"Creating a Temporary Home Directory"},{"location":"Administrator/Cluster-Setup/proxy-server/","text":"Introduction \u00b6 Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\". In some organizations, outbound connectivity requires a proxy. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work. Run:AI Configuration \u00b6 The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. The Run:AI cluster installation is performed by accessing the Administrator User Interface at https://app.run.ai/clusters downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for httpProxy : global : ... httpProxy : enabled : false proxyUrl : http://<proxy-url>:<proxy-port> tlsCert : |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true Set the proxy URL and Port. Paste the contents of the certificate under tlsCert .","title":"Configure an HTTP Proxy Server "},{"location":"Administrator/Cluster-Setup/proxy-server/#introduction","text":"Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\". In some organizations, outbound connectivity requires a proxy. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/proxy-server/#runai-configuration","text":"The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. The Run:AI cluster installation is performed by accessing the Administrator User Interface at https://app.run.ai/clusters downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for httpProxy : global : ... httpProxy : enabled : false proxyUrl : http://<proxy-url>:<proxy-port> tlsCert : |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true Set the proxy URL and Port. Paste the contents of the certificate under tlsCert .","title":"Run:AI Configuration"},{"location":"Administrator/Cluster-Setup/researcher-authentication/","text":"Setup Project-based Researcher Access Control \u00b6 Introduction \u00b6 By default, Run:AI is configured to allow all Researchers access to all Jobs and Projects. This document provides step-by-step instructions on how to enable access-control. Run:AI access control is at the Project level. When you assign Users to Projects - only these users are allowed to submit Jobs and access Jobs details. How it works \u00b6 The Run:AI command-line interface uses a Kubernetes configuration file residing on a client machine. The configuration file contains information on how to access the Kubernetes cluster and hence the Run:AI Authentication setup works as follows: Client-side: Modify the Kubernetes configuration file to prompt for credentials. Server-side: Modify the Kubernetes cluster to validate credentials against the Run:AI Authentication authority. Assign Users to Projects using the Run:AI Administration UI. Administration User Interface Setup \u00b6 Enable Researcher Authentication \u00b6 Under app.run.ai settings: Enable the flag Researcher Authentication . Copy the values for Client ID and Realm which appear on screen. Assign Users to Projects \u00b6 Assign Researchers to Projects: Under Users add a Researcher and assign it with a Researcher role. Under Projects , edit or create a Project. Use the Users tab to assign the Researcher to the Project. Client-Side \u00b6 To control access to Run:AI (and Kubernetes) resources, you must modify the Kubernetes certificate. The certificate is distributed to users as part of the Comnand-line interface installation . When making changes to the certificate, keep a copy of the original certificate to be used for cluster administration. After making the modifications, distribute the modified certificate to Researchers. Under the ~/.kube directory edit the config file, and add the following: - name : <USER_NAME> user : auth-provider : config : auth-flow : cli realm : <REALM> client-id : <CLIENT_ID> idp-issuer-url : https://runai-prod.auth0.com/ name : oidc Where <USER_NAME> is an arbitrary name which is also referred to under contexts | context | user in the same file. You must distribute the modified certificate to Researchers. Server-Side \u00b6 Locate the Kubernetes API Server configuration file. The file's location may defer between different Kubernetes distributions. The default location is /etc/kubernetes/manifests/kube-apiserver.yaml Edit the document to add the following parameters at the end of the existing command list: spec : containers : - command : ... - --oidc-client-id=<CLIENT_ID> - --oidc-issuer-url=https://runai-prod.auth0.com/ - --oidc-username-prefix=- - --oidc-groups-claim=email Verify that the kube-apiserver-<master-node-name> pod in the kube-system namespace has been restarted and that changes have been incorporated. Run: kubectl get pods -n kube-system kube-apiserver-<master-node-name> -o yaml And search for the above oidc flags. Test \u00b6 Submit a Job. You will be redirected to a browser page that requires authentication. If you are using a machine without a browser, you will be prompted with a URL to run elsewhere and return a resulting token. If the Job was submitted with a Project for which you have no access, your access will be denied. If the Job was submitted with a Project for which you have access, your access will be granted. Existing Jobs in Projects you do not have access to, will show when you run runai job list -p <project-name> but you will not be able to view logs, get further info, bash into or delete.","title":"Authentication and Authorization"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#setup-project-based-researcher-access-control","text":"","title":"Setup Project-based Researcher Access Control"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#introduction","text":"By default, Run:AI is configured to allow all Researchers access to all Jobs and Projects. This document provides step-by-step instructions on how to enable access-control. Run:AI access control is at the Project level. When you assign Users to Projects - only these users are allowed to submit Jobs and access Jobs details.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#how-it-works","text":"The Run:AI command-line interface uses a Kubernetes configuration file residing on a client machine. The configuration file contains information on how to access the Kubernetes cluster and hence the Run:AI Authentication setup works as follows: Client-side: Modify the Kubernetes configuration file to prompt for credentials. Server-side: Modify the Kubernetes cluster to validate credentials against the Run:AI Authentication authority. Assign Users to Projects using the Run:AI Administration UI.","title":"How it works"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#administration-user-interface-setup","text":"","title":"Administration User Interface Setup"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#enable-researcher-authentication","text":"Under app.run.ai settings: Enable the flag Researcher Authentication . Copy the values for Client ID and Realm which appear on screen.","title":"Enable Researcher Authentication"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#assign-users-to-projects","text":"Assign Researchers to Projects: Under Users add a Researcher and assign it with a Researcher role. Under Projects , edit or create a Project. Use the Users tab to assign the Researcher to the Project.","title":"Assign Users to Projects"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#client-side","text":"To control access to Run:AI (and Kubernetes) resources, you must modify the Kubernetes certificate. The certificate is distributed to users as part of the Comnand-line interface installation . When making changes to the certificate, keep a copy of the original certificate to be used for cluster administration. After making the modifications, distribute the modified certificate to Researchers. Under the ~/.kube directory edit the config file, and add the following: - name : <USER_NAME> user : auth-provider : config : auth-flow : cli realm : <REALM> client-id : <CLIENT_ID> idp-issuer-url : https://runai-prod.auth0.com/ name : oidc Where <USER_NAME> is an arbitrary name which is also referred to under contexts | context | user in the same file. You must distribute the modified certificate to Researchers.","title":"Client-Side"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#server-side","text":"Locate the Kubernetes API Server configuration file. The file's location may defer between different Kubernetes distributions. The default location is /etc/kubernetes/manifests/kube-apiserver.yaml Edit the document to add the following parameters at the end of the existing command list: spec : containers : - command : ... - --oidc-client-id=<CLIENT_ID> - --oidc-issuer-url=https://runai-prod.auth0.com/ - --oidc-username-prefix=- - --oidc-groups-claim=email Verify that the kube-apiserver-<master-node-name> pod in the kube-system namespace has been restarted and that changes have been incorporated. Run: kubectl get pods -n kube-system kube-apiserver-<master-node-name> -o yaml And search for the above oidc flags.","title":"Server-Side"},{"location":"Administrator/Cluster-Setup/researcher-authentication/#test","text":"Submit a Job. You will be redirected to a browser page that requires authentication. If you are using a machine without a browser, you will be prompted with a URL to run elsewhere and return a resulting token. If the Job was submitted with a Project for which you have no access, your access will be denied. If the Job was submitted with a Project for which you have access, your access will be granted. Existing Jobs in Projects you do not have access to, will show when you run runai job list -p <project-name> but you will not be able to view logs, get further info, bash into or delete.","title":"Test"},{"location":"Administrator/Cluster-Setup/single-node-install/","text":"Quick Install of Run:AI on a Single Node \u00b6 Below are instructions on how to install Run:AI cluster on a single node. This process is good for learning Run:AI or using a Run:AI cluster on a single node . Multiple nodes are not supported with this installation. To install a cluster with multiple nodes use Cluster Installation . The installation process below is comprised of a single script and includes the installation of a built-in Kubernetes using minikube . Prerequisites \u00b6 The installation below assumes: A single node, with at least one GPU. Running Ubuntu 18.04 or Ubuntu 20.04. sudo access to the node. An email and a password provided by Run:AI customer support. Outbound internet connectivity If NVIDIA Drivers are not installed, the script will install the latest NVIDIA Drivers. Installation steps \u00b6 Log in to app.run.ai for the first time to change your password. Do not create a new Cluster. Get the script: wget https://raw.githubusercontent.com/run-ai/docs/master/install/single-node-install.sh && chmod +x single-node-install.sh Run the script: sudo ./single-node-install.sh <email> '<password>' (note that the password may have special characters, hence the need for surrounding quotes) If the NVIDIA Drivers have not been pre-installed, they will be installed by the script. The script will then ask for a reboot, after which, re-run the command above. Node Shutdown and Restart \u00b6 To shut down your node, you must first perform an orderly shutdown of Kubernetes by running: sudo minikube stop When you restart your node, Kubernetes must be restarted as well, using the following command: sudo minikube start --driver=none --apiserver-ips 127.0.0.1 --apiserver-name localhost The Run:AI cluster will automatically start following Kubernetes. Deleting Run:AI \u00b6 Run: sudo minikube delete --all Next Steps \u00b6 Set up at least one Project Working with Projects . Review Quickstart Guides to run workloads.","title":"Quick Single Node Install"},{"location":"Administrator/Cluster-Setup/single-node-install/#quick-install-of-runai-on-a-single-node","text":"Below are instructions on how to install Run:AI cluster on a single node. This process is good for learning Run:AI or using a Run:AI cluster on a single node . Multiple nodes are not supported with this installation. To install a cluster with multiple nodes use Cluster Installation . The installation process below is comprised of a single script and includes the installation of a built-in Kubernetes using minikube .","title":"Quick Install of Run:AI on a Single Node"},{"location":"Administrator/Cluster-Setup/single-node-install/#prerequisites","text":"The installation below assumes: A single node, with at least one GPU. Running Ubuntu 18.04 or Ubuntu 20.04. sudo access to the node. An email and a password provided by Run:AI customer support. Outbound internet connectivity If NVIDIA Drivers are not installed, the script will install the latest NVIDIA Drivers.","title":"Prerequisites"},{"location":"Administrator/Cluster-Setup/single-node-install/#installation-steps","text":"Log in to app.run.ai for the first time to change your password. Do not create a new Cluster. Get the script: wget https://raw.githubusercontent.com/run-ai/docs/master/install/single-node-install.sh && chmod +x single-node-install.sh Run the script: sudo ./single-node-install.sh <email> '<password>' (note that the password may have special characters, hence the need for surrounding quotes) If the NVIDIA Drivers have not been pre-installed, they will be installed by the script. The script will then ask for a reboot, after which, re-run the command above.","title":"Installation steps"},{"location":"Administrator/Cluster-Setup/single-node-install/#node-shutdown-and-restart","text":"To shut down your node, you must first perform an orderly shutdown of Kubernetes by running: sudo minikube stop When you restart your node, Kubernetes must be restarted as well, using the following command: sudo minikube start --driver=none --apiserver-ips 127.0.0.1 --apiserver-name localhost The Run:AI cluster will automatically start following Kubernetes.","title":"Node Shutdown and Restart"},{"location":"Administrator/Cluster-Setup/single-node-install/#deleting-runai","text":"Run: sudo minikube delete --all","title":"Deleting Run:AI"},{"location":"Administrator/Cluster-Setup/single-node-install/#next-steps","text":"Set up at least one Project Working with Projects . Review Quickstart Guides to run workloads.","title":"Next Steps"},{"location":"Administrator/Presentations/Administrator-Onboarding-Presentation/","text":"","title":"Administrator Onboarding"},{"location":"Administrator/Researcher-Setup/cli-install/","text":"Install the Run:AI Command-line Interface \u00b6 The Run:AI Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI. Prerequisites \u00b6 Run:AI CLI runs on Mac and Linux. When installing the command-line interface, its worth considering future upgrades: Install the CLI on a dedicated Jumpbox machine. Researches will connect to the Jumpbox from which they can submit Run:AI commands Install the CLI on a shared directory that is mounted on Researchers' machines. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ . Helm . See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from the Kubernetes cluster installation. Researcher Authentication \u00b6 When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control . Use the modified Kubernetes configuration file described in the article. Setup \u00b6 Kubernetes Configuration \u00b6 On the Researcher's root folder, create a directory .kube . Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults. If you choose to locate the file at a different location than ~/.kube/config , you must create a shell variable to point to the configuration file as follows: export KUBECONFIG=<Kubernetes-config-file> Test the connection by running: kubectl get nodes Run:AI CLI Installation \u00b6 Download the latest release from the Run:AI releases page Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai list jobs Troubleshooting the CLI Installation \u00b6 See Troubleshooting a CLI installation Updating the Run:AI CLI \u00b6 To update the CLI to the latest version run: sudo runai update","title":"Install the CLI"},{"location":"Administrator/Researcher-Setup/cli-install/#install-the-runai-command-line-interface","text":"The Run:AI Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI.","title":"Install the Run:AI Command-line Interface"},{"location":"Administrator/Researcher-Setup/cli-install/#prerequisites","text":"Run:AI CLI runs on Mac and Linux. When installing the command-line interface, its worth considering future upgrades: Install the CLI on a dedicated Jumpbox machine. Researches will connect to the Jumpbox from which they can submit Run:AI commands Install the CLI on a shared directory that is mounted on Researchers' machines. Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ . Helm . See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from the Kubernetes cluster installation.","title":"Prerequisites"},{"location":"Administrator/Researcher-Setup/cli-install/#researcher-authentication","text":"When enabled, Researcher authentication requires additional setup when installing the CLI. To configure authentication see Setup Project-based Researcher Access Control . Use the modified Kubernetes configuration file described in the article.","title":"Researcher Authentication"},{"location":"Administrator/Researcher-Setup/cli-install/#setup","text":"","title":"Setup"},{"location":"Administrator/Researcher-Setup/cli-install/#kubernetes-configuration","text":"On the Researcher's root folder, create a directory .kube . Copy the Kubernetes configuration file into the directory. Each Researcher should have a separate copy of the configuration file. The Researcher should have write access to the configuration file as it stores user defaults. If you choose to locate the file at a different location than ~/.kube/config , you must create a shell variable to point to the configuration file as follows: export KUBECONFIG=<Kubernetes-config-file> Test the connection by running: kubectl get nodes","title":"Kubernetes Configuration"},{"location":"Administrator/Researcher-Setup/cli-install/#runai-cli-installation","text":"Download the latest release from the Run:AI releases page Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai list jobs","title":"Run:AI CLI Installation"},{"location":"Administrator/Researcher-Setup/cli-install/#troubleshooting-the-cli-installation","text":"See Troubleshooting a CLI installation","title":"Troubleshooting the CLI Installation"},{"location":"Administrator/Researcher-Setup/cli-install/#updating-the-runai-cli","text":"To update the CLI to the latest version run: sudo runai update","title":"Updating the Run:AI CLI"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/","text":"When running the CLI you get an error an invalid configuration error \u00b6 When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided Solution \u00b6 Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory that contains a configuration file pointing to the Kubernetes cluster. When running the CLI you get an error: open .../.kube/config.lock: permission denied \u00b6 When running any CLI command you get a permission denied error. Solution \u00b6 The user running the CLI does not have read permissions to the .kube directory. When running 'runai logs', the logs are delayed \u00b6 By default, Python buffers stdout and stderr, which is not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered. Solution \u00b6 Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. python -u main.py . Runai list jobs command works but runai submit does not \u00b6 Solution \u00b6 Helm utility is not installed. See Run:AI CLI Installation documentation.","title":"Troubleshooting"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#when-running-the-cli-you-get-an-error-an-invalid-configuration-error","text":"When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided","title":"When running the CLI you get an error an invalid configuration error"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution","text":"Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory that contains a configuration file pointing to the Kubernetes cluster.","title":"Solution"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#when-running-the-cli-you-get-an-error-open-kubeconfiglock-permission-denied","text":"When running any CLI command you get a permission denied error.","title":"When running the CLI you get an error: open .../.kube/config.lock: permission denied"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution_1","text":"The user running the CLI does not have read permissions to the .kube directory.","title":"Solution"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#when-running-runai-logs-the-logs-are-delayed","text":"By default, Python buffers stdout and stderr, which is not flushed in real-time. This may cause logs to appear sometimes minutes after being buffered.","title":"When running 'runai logs', the logs are delayed"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution_2","text":"Set the env var PYTHONUNBUFFERED to any non-empty string or pass -u to Python. e.g. python -u main.py .","title":"Solution"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#runai-list-jobs-command-works-but-runai-submit-does-not","text":"","title":"Runai list jobs command works but runai submit does not"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution_3","text":"Helm utility is not installed. See Run:AI CLI Installation documentation.","title":"Solution"},{"location":"Administrator/Researcher-Setup/docker-registry-config/","text":"Using a Docker Registry with Credentials \u00b6 Why? \u00b6 Some Docker images are stored in private docker registries. In order for the Researcher to access the images, we will need to provide credentials for the registry. How? \u00b6 For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret_name may be any arbitrary string user_name and password are the repository user and password Note : the secret may take up to a minute to update in the system. Google Cloud Registry \u00b6 Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, login to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr-secret namespace : runai labels : runai/cluster-wide : \"true\" data : .dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes.io/dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Use a Private Docker Registry"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#using-a-docker-registry-with-credentials","text":"","title":"Using a Docker Registry with Credentials"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#why","text":"Some Docker images are stored in private docker registries. In order for the Researcher to access the images, we will need to provide credentials for the registry.","title":"Why?"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#how","text":"For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret_name may be any arbitrary string user_name and password are the repository user and password Note : the secret may take up to a minute to update in the system.","title":"How?"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#google-cloud-registry","text":"Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, login to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr-secret namespace : runai labels : runai/cluster-wide : \"true\" data : .dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes.io/dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Google Cloud Registry"},{"location":"Administrator/Researcher-Setup/docker-to-runai/","text":"Dockers, Images, and Kubernetes \u00b6 Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows. Image Repository \u00b6 If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web. Data \u00b6 Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command). Working with Containers \u00b6 Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference . Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for Researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline Researchers' work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation","title":"Switch from Docker to Run:AI "},{"location":"Administrator/Researcher-Setup/docker-to-runai/#dockers-images-and-kubernetes","text":"Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the Researcher's workflow as follows.","title":"Dockers, Images, and Kubernetes"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#image-repository","text":"If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the Researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web.","title":"Image Repository"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#data","text":"Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).","title":"Data"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#working-with-containers","text":"Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference .","title":"Working with Containers"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for Researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline Researchers' work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/","text":"Why? \u00b6 In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation Configuring Node Groups \u00b6 To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value Using Node Groups via the CLI \u00b6 Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information Assigning Node Groups to a Project \u00b6 To assign specific node groups to a Project see working with Projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.","title":"Limit Run:AI to Specific Nodes"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#why","text":"In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, Hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation","title":"Why?"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#configuring-node-groups","text":"To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value","title":"Configuring Node Groups"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#using-node-groups-via-the-cli","text":"Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information","title":"Using Node Groups via the CLI"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#assigning-node-groups-to-a-project","text":"To assign specific node groups to a Project see working with Projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the Project.","title":"Assigning Node Groups to a Project"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/","text":"Following is a step by step guide for getting a new Researcher up to speed with Run:AI and Kubernetes. Change of Paradigms: from Docker to Kubernetes \u00b6 As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI. Setup the Run:AI Command-Line Interface \u00b6 Run:AI CLI needs to be installed on the Researcher machine. This document provides step by step instructions. Provide the Researcher with a GPU Quota \u00b6 To submit workloads with Run:AI, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota. Provide access to the Run:AI Administration UI \u00b6 Some organizations would want to provide Researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI . See this document for further information on how to provide access. Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for Researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation .","title":"Introduction"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","text":"As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the Researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI.","title":"Change of Paradigms: from Docker to Kubernetes"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#setup-the-runai-command-line-interface","text":"Run:AI CLI needs to be installed on the Researcher machine. This document provides step by step instructions.","title":"Setup the Run:AI Command-Line Interface"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","text":"To submit workloads with Run:AI, the Researcher must be provided with a Project that contains a GPU quota. Please see Working with Projects document on how to create Projects and set a quota.","title":"Provide the Researcher with a GPU Quota"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#provide-access-to-the-runai-administration-ui","text":"Some organizations would want to provide Researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI . See this document for further information on how to provide access.","title":"Provide access to the Run:AI Administration UI"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for Researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the Researchers' work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation .","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/researcher-ui-setup/","text":"Researcher User Interface \u00b6 The Run:AI Researcher User Interface allows Researchers which are less experienced with a command-line interface to view Projects, as well as submit and view Jobs. Important The Run:AI Researcher User Interface is currently in beta . The Run:AI Researcher User Interface does not work when the system is configured to authenticate Researchers . We are working to add this functionality. Connecting to the Researcher UI \u00b6 To find the URL for the Researcher UI, use the method here .","title":"Researcher UI"},{"location":"Administrator/Researcher-Setup/researcher-ui-setup/#researcher-user-interface","text":"The Run:AI Researcher User Interface allows Researchers which are less experienced with a command-line interface to view Projects, as well as submit and view Jobs. Important The Run:AI Researcher User Interface is currently in beta . The Run:AI Researcher User Interface does not work when the system is configured to authenticate Researchers . We are working to add this functionality.","title":"Researcher User Interface"},{"location":"Administrator/Researcher-Setup/researcher-ui-setup/#connecting-to-the-researcher-ui","text":"To find the URL for the Researcher UI, use the method here .","title":"Connecting to the Researcher UI"},{"location":"Administrator/Researcher-Setup/template-config/","text":"Configure Command-Line Interface Templates \u00b6 What are Templates? \u00b6 Templates are a way to reduce the number of flags required when using the Command-Line Interface to start workloads. The Researcher can: Use a template by running runai submit --template <template-name> Review list of templates by running runai list template Review the contents of a specific template by running runai describe template <template-name> The purpose of this document is to provide the Administrator with guidelines on how to create & maintain templates. Template and Kubernetes \u00b6 CLI Templates are implemented as Kubernetes ConfigMaps . A Kubernetes ConfigMap is the standard way to save cluster-wide settings. Creating a Template \u00b6 To create a template, create a file (e.g. my-template.yaml ) with: apiVersion : v1 kind : ConfigMap data : name : template-1 description : \"my first template\" values : | gpu: required: true image: value: tensorflow/tensorflow:1.14.0-gpu-py3 environments: - LEARNING_RATE=0.2 - MYUSER=$USER - MYPASSWORD=SECRET:my-secret,cred-pass metadata : name : template-1 namespace : runai labels : runai/template : \"true\" To store this template run: kubectl apply -f my-template.yaml Notes The template above sets the following: That --gpu (or -g) is a required field when using this template The default image file will be tensorflow/tensorflow:1.14.0-gpu-py3 . The user can override this value and use a different image by setting the --image (-i) flag. There are two environment variables set LEARNING_RATE and MYUSER . Note that MYUSER will be set at runtime according to the value of $USER . The user can add environment variables, and override existing ones. MYPASSWORD is set from a Kubernetes secret. For further information see Setting secrets in Jobs The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the runai template list command. See additional information below on flag syntax. To see this template in the template list run: runai list template To show the properties of the created template run: runai describe template template-1 Use the template when submitting a workload runai submit my-job1 .... --template template-1 Syntax \u00b6 When specifying a single-value flag, use the full name of the flag. For example, for setting --gpu use gpu . For a list of flags, see the runai-submit reference document . When specifying a multi-value flag, use the plural of the flag name. For example: for setting the --environment flag use environments . For setting the --volume flag. Use volumes When specifying a single value flag, use the syntax: single-value-flag : required : true/false value : string When specifying a multi-value flag, use the syntax: multi-value-flag : - value1 - value2 - ... The Default Template \u00b6 The Administrator can also set a template that is always active: apiVersion : v1 kind : ConfigMap data : name : template-admin description : \"my first template\" values : | job-name-prefix: value: acme volumes: - /mnt/nfs-share/john:/workspace/john metadata : name : template-admin namespace : runai labels : runai/template : \"true\" Notes The template is denoted as the admin template with the name template-admin Override rules \u00b6 The User, when running runai submit always overrides the default template and a template specified with --template The default template overrides any specified template. Deleting a Template \u00b6 to delete a template, run: kubectl delete cm -n runai <template-name> See Also \u00b6 For a list of runai submit flags, see the Run:AI command-line reference","title":"Configure Command-Line Interface Templates"},{"location":"Administrator/Researcher-Setup/template-config/#configure-command-line-interface-templates","text":"","title":"Configure Command-Line Interface Templates"},{"location":"Administrator/Researcher-Setup/template-config/#what-are-templates","text":"Templates are a way to reduce the number of flags required when using the Command-Line Interface to start workloads. The Researcher can: Use a template by running runai submit --template <template-name> Review list of templates by running runai list template Review the contents of a specific template by running runai describe template <template-name> The purpose of this document is to provide the Administrator with guidelines on how to create & maintain templates.","title":"What are Templates?"},{"location":"Administrator/Researcher-Setup/template-config/#template-and-kubernetes","text":"CLI Templates are implemented as Kubernetes ConfigMaps . A Kubernetes ConfigMap is the standard way to save cluster-wide settings.","title":"Template and Kubernetes"},{"location":"Administrator/Researcher-Setup/template-config/#creating-a-template","text":"To create a template, create a file (e.g. my-template.yaml ) with: apiVersion : v1 kind : ConfigMap data : name : template-1 description : \"my first template\" values : | gpu: required: true image: value: tensorflow/tensorflow:1.14.0-gpu-py3 environments: - LEARNING_RATE=0.2 - MYUSER=$USER - MYPASSWORD=SECRET:my-secret,cred-pass metadata : name : template-1 namespace : runai labels : runai/template : \"true\" To store this template run: kubectl apply -f my-template.yaml Notes The template above sets the following: That --gpu (or -g) is a required field when using this template The default image file will be tensorflow/tensorflow:1.14.0-gpu-py3 . The user can override this value and use a different image by setting the --image (-i) flag. There are two environment variables set LEARNING_RATE and MYUSER . Note that MYUSER will be set at runtime according to the value of $USER . The user can add environment variables, and override existing ones. MYPASSWORD is set from a Kubernetes secret. For further information see Setting secrets in Jobs The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the runai template list command. See additional information below on flag syntax. To see this template in the template list run: runai list template To show the properties of the created template run: runai describe template template-1 Use the template when submitting a workload runai submit my-job1 .... --template template-1","title":"Creating a Template"},{"location":"Administrator/Researcher-Setup/template-config/#syntax","text":"When specifying a single-value flag, use the full name of the flag. For example, for setting --gpu use gpu . For a list of flags, see the runai-submit reference document . When specifying a multi-value flag, use the plural of the flag name. For example: for setting the --environment flag use environments . For setting the --volume flag. Use volumes When specifying a single value flag, use the syntax: single-value-flag : required : true/false value : string When specifying a multi-value flag, use the syntax: multi-value-flag : - value1 - value2 - ...","title":"Syntax"},{"location":"Administrator/Researcher-Setup/template-config/#the-default-template","text":"The Administrator can also set a template that is always active: apiVersion : v1 kind : ConfigMap data : name : template-admin description : \"my first template\" values : | job-name-prefix: value: acme volumes: - /mnt/nfs-share/john:/workspace/john metadata : name : template-admin namespace : runai labels : runai/template : \"true\" Notes The template is denoted as the admin template with the name template-admin","title":"The Default Template"},{"location":"Administrator/Researcher-Setup/template-config/#override-rules","text":"The User, when running runai submit always overrides the default template and a template specified with --template The default template overrides any specified template.","title":"Override rules"},{"location":"Administrator/Researcher-Setup/template-config/#deleting-a-template","text":"to delete a template, run: kubectl delete cm -n runai <template-name>","title":"Deleting a Template"},{"location":"Administrator/Researcher-Setup/template-config/#see-also","text":"For a list of runai submit flags, see the Run:AI command-line reference","title":"See Also"},{"location":"Administrator/Researcher-Setup/use-secrets/","text":"Secrets in Jobs \u00b6 Kubernetes Secrets \u00b6 Sometimes you want to use sensitive information within your code. Examples are: passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets . Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration. A Kubernetes secret may hold multiple key - value pairs . Using Secrets in Run:AI Jobs \u00b6 Our goal is to provide Run:AI Jobs with secrets as input in a secure way. Using the Run:AI command-line, you will be able to pass a reference to a secret that already exists in Kubernetes. Creating a secret \u00b6 For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/ . Here is an example: apiVersion : v1 kind : Secret metadata : name : my-secret namespace : runai-<project-name> data : username : am9obgo= password : bXktcGFzc3dvcmQK Then run: kubectl apply -f <file-name> Notes Secrets are base64 encoded Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:AI Project name above. Run:AI provides the ability to propagate secrets throughout all Run:AI Projects. See below. Attaching a secret to a Job on Submit \u00b6 When you submit a new Job, you will want to connect the secret to the new Job. To do that, run: runai submit -e <ENV-VARIABLE>=SECRET:<secret-name>,<secret-key> .... For example: runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username Secrets and Projects \u00b6 As per the note above, secrets are namespace-specific. If your secret relates to all Run:AI Projects, do the following to propagate the secret to all Projects: Create a secret within the runai namespace. Run the following once to allow Run:AI to propagate the secret to all Run:AI Projects: Reminder The Run:AI Administrator CLI can be obtained here . To delete a secret from all Run:AI Projects, run: runai-adm remove secret <secret name> --cluster-wide Secrets and Templates \u00b6 A Secret can be set at the template level. For additional information see template configuration","title":"Use Secrets in Jobs"},{"location":"Administrator/Researcher-Setup/use-secrets/#secrets-in-jobs","text":"","title":"Secrets in Jobs"},{"location":"Administrator/Researcher-Setup/use-secrets/#kubernetes-secrets","text":"Sometimes you want to use sensitive information within your code. Examples are: passwords, OAuth tokens, or ssh keys. The best practice for saving such information in Kubernetes is via Kubernetes Secrets . Kubernetes Secrets let you store and manage sensitive information. Access to secrets is limited via configuration. A Kubernetes secret may hold multiple key - value pairs .","title":"Kubernetes Secrets"},{"location":"Administrator/Researcher-Setup/use-secrets/#using-secrets-in-runai-jobs","text":"Our goal is to provide Run:AI Jobs with secrets as input in a secure way. Using the Run:AI command-line, you will be able to pass a reference to a secret that already exists in Kubernetes.","title":"Using Secrets in Run:AI Jobs"},{"location":"Administrator/Researcher-Setup/use-secrets/#creating-a-secret","text":"For details on how to create a Kubernetes secret see: https://kubernetes.io/docs/concepts/configuration/secret/ . Here is an example: apiVersion : v1 kind : Secret metadata : name : my-secret namespace : runai-<project-name> data : username : am9obgo= password : bXktcGFzc3dvcmQK Then run: kubectl apply -f <file-name> Notes Secrets are base64 encoded Secrets are stored in the scope of a namespace and will not be accessible from other namespaces. Hence the reference to the Run:AI Project name above. Run:AI provides the ability to propagate secrets throughout all Run:AI Projects. See below.","title":"Creating a secret"},{"location":"Administrator/Researcher-Setup/use-secrets/#attaching-a-secret-to-a-job-on-submit","text":"When you submit a new Job, you will want to connect the secret to the new Job. To do that, run: runai submit -e <ENV-VARIABLE>=SECRET:<secret-name>,<secret-key> .... For example: runai submit -i ubuntu -e MYUSERNAME=SECRET:my-secret,username","title":"Attaching a secret to a Job on Submit"},{"location":"Administrator/Researcher-Setup/use-secrets/#secrets-and-projects","text":"As per the note above, secrets are namespace-specific. If your secret relates to all Run:AI Projects, do the following to propagate the secret to all Projects: Create a secret within the runai namespace. Run the following once to allow Run:AI to propagate the secret to all Run:AI Projects: Reminder The Run:AI Administrator CLI can be obtained here . To delete a secret from all Run:AI Projects, run: runai-adm remove secret <secret name> --cluster-wide","title":"Secrets and Projects"},{"location":"Administrator/Researcher-Setup/use-secrets/#secrets-and-templates","text":"A Secret can be set at the template level. For additional information see template configuration","title":"Secrets and Templates"},{"location":"Administrator/admin-ui-setup/admin-ui-users/","text":"Adding, Updating, and Deleting Users \u00b6 Introduction \u00b6 The Admin User Interface allows the creation of Run:AI Users. Run:AI Users can receive varying levels of access to the Administration UI and to submitting Jobs on the Cluster. Notes: It is possible to connect the Admin UI Users module to the organization's LDAP directory. For further information please contact Run:AI customer support. Working with Users \u00b6 Create User \u00b6 Note To be able to manipulate Users, you must have Administrator access. if you do not have such access, please contact an Administrator. The list of Administrators is shown on the Users page (see below) Login to the Users area of the Run:AI Administration User interface at https://app.run.ai/users . On the top right, select \"Add New Users\". Choose a User name and email. Leave password as blank, it will be set by the User Select Roles. Note -- more than one role can be selected. The available roles are: Administrator : Can manage Users and install Clusters. Editor : Can manage Projects and Departments. Viewer : View-only access to Admin UI. Researcher : Can run ML workloads using the Run:AI command-line interface, The Researcher user interface or similar. This setting is relevant only if Researcher Authentication is enabled (subject to the adding . Select a Cluster. This determines what Clusters are accessible to this User Press \"Save\" The User will receive a join mail and will be able to set a password. Update a User \u00b6 Select an existing User. Right-click and press \"Edit\". Update the values and press \"Save\". Delete an existing User \u00b6 Select an existing User. Right-click and press \"Delete\".","title":"Users"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#adding-updating-and-deleting-users","text":"","title":"Adding, Updating, and Deleting Users"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#introduction","text":"The Admin User Interface allows the creation of Run:AI Users. Run:AI Users can receive varying levels of access to the Administration UI and to submitting Jobs on the Cluster. Notes: It is possible to connect the Admin UI Users module to the organization's LDAP directory. For further information please contact Run:AI customer support.","title":"Introduction"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#working-with-users","text":"","title":"Working with Users"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#create-user","text":"Note To be able to manipulate Users, you must have Administrator access. if you do not have such access, please contact an Administrator. The list of Administrators is shown on the Users page (see below) Login to the Users area of the Run:AI Administration User interface at https://app.run.ai/users . On the top right, select \"Add New Users\". Choose a User name and email. Leave password as blank, it will be set by the User Select Roles. Note -- more than one role can be selected. The available roles are: Administrator : Can manage Users and install Clusters. Editor : Can manage Projects and Departments. Viewer : View-only access to Admin UI. Researcher : Can run ML workloads using the Run:AI command-line interface, The Researcher user interface or similar. This setting is relevant only if Researcher Authentication is enabled (subject to the adding . Select a Cluster. This determines what Clusters are accessible to this User Press \"Save\" The User will receive a join mail and will be able to set a password.","title":"Create User"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#update-a-user","text":"Select an existing User. Right-click and press \"Edit\". Update the values and press \"Save\".","title":"Update a User"},{"location":"Administrator/admin-ui-setup/admin-ui-users/#delete-an-existing-user","text":"Select an existing User. Right-click and press \"Delete\".","title":"Delete an existing User"},{"location":"Administrator/admin-ui-setup/department-setup/","text":"Introduction \u00b6 Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create priorities, Run:AI introduced the concept of Projects . Projects are quota entities that associate a Project name with GPU allocation and preferences. A Researcher submitting a workload needs to associate a Project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Administrators manage Projects as detailed here . At some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached with a quota. There are organizational quotas at a higher level. Departments \u00b6 Departments create a second hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. A Department quota supersedes a Project quota. Overquota behavior \u00b6 Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 DGXs with 80 GPUs, totaling a cluster of 160 GPUs. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need it. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:AI scheduler as over-quota. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Department allocation should be equal to the number of GPUs in the cluster. Creating and Managing Departments \u00b6 Enable Departments \u00b6 Departments are disabled by default. To start working with Departments: Go to Settings | General Enable Departments Once Departments are enabled, the menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All Projects created before the Department feature was enabled will belong to the default Department. Adding Departments \u00b6 You can add new Departments by pressing the Add New Department at the top right of the Department view. Add Department name and quota allocation. Assigning Projects to Departments \u00b6 Under Projects edit an existing Project, you will see a new Department drop down with which you can associate a Project with a Department.","title":"Departments"},{"location":"Administrator/admin-ui-setup/department-setup/#introduction","text":"Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create priorities, Run:AI introduced the concept of Projects . Projects are quota entities that associate a Project name with GPU allocation and preferences. A Researcher submitting a workload needs to associate a Project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Administrators manage Projects as detailed here . At some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached with a quota. There are organizational quotas at a higher level.","title":"Introduction"},{"location":"Administrator/admin-ui-setup/department-setup/#departments","text":"Departments create a second hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. A Department quota supersedes a Project quota.","title":"Departments"},{"location":"Administrator/admin-ui-setup/department-setup/#overquota-behavior","text":"Consider an example from an academic use case: the Computer Science Department and the GeoPhysics Department have each purchased 10 DGXs with 80 GPUs, totaling a cluster of 160 GPUs. The two Departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need it. As such, there could be many Projects in the GeoPhysics Department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:AI scheduler as over-quota. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Department allocation should be equal to the number of GPUs in the cluster.","title":"Overquota behavior"},{"location":"Administrator/admin-ui-setup/department-setup/#creating-and-managing-departments","text":"","title":"Creating and Managing Departments"},{"location":"Administrator/admin-ui-setup/department-setup/#enable-departments","text":"Departments are disabled by default. To start working with Departments: Go to Settings | General Enable Departments Once Departments are enabled, the menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All Projects created before the Department feature was enabled will belong to the default Department.","title":"Enable Departments"},{"location":"Administrator/admin-ui-setup/department-setup/#adding-departments","text":"You can add new Departments by pressing the Add New Department at the top right of the Department view. Add Department name and quota allocation.","title":"Adding Departments"},{"location":"Administrator/admin-ui-setup/department-setup/#assigning-projects-to-departments","text":"Under Projects edit an existing Project, you will see a new Department drop down with which you can associate a Project with a Department.","title":"Assigning Projects to Departments"},{"location":"Administrator/admin-ui-setup/project-setup/","text":"Introduction \u00b6 Researchers are submitting Jobs via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and prioritize work, Run:AI introduces the concept of Projects . Projects are quota entities that associate a Project name with GPU allocation and allocation preferences. A Researcher submitting a Job needs to associate a Project name with the request. The Run:AI scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation. Modeling Projects \u00b6 As an Admin, you need to determine how to model Projects. You can: Set a Project per user. Set a Project per team of users. Set a Project per a real organizational Project. Project Quotas \u00b6 Each Project is associated with a quota of GPUs that can be allocated for this Project at the same time. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this Project can receive an over-quota . As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Project allocation should be equal to the number of GPUs in the cluster. Working with Projects \u00b6 Create a new Project \u00b6 Note In order to be able to manipulate Projects, you must have Editor access. See the \"Users\" Area Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects On the top right, select \"Add New Project\" Choose a Project name and a Project quota Press \"Save\" Update an existing Project \u00b6 Select an existing Project. Right-click and press \"Edit\". Update the values and press \"Save\". Delete an existing Project \u00b6 Select an existing Project. Right-click and press \"Delete\". Limit Jobs to run on Specific Node Groups \u00b6 A frequent use case is to assign specific Projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes. While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization. Grouping Nodes \u00b6 To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects. Each node can only be annotated with a single name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 You can annotate multiple nodes with the same label Setting Affinity for a Specific Project \u00b6 To mandate training Jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog. Further Affinity Refinement by the Researcher \u00b6 The Researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific Project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information runai submit Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive Jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab and set a limit (day, hour, minute). The setting only takes effect for Jobs that have started after the duration has been changed. Assign Users to Project \u00b6 When Researcher Authentication is enabled, the Project form will contain an additional Users tab. The tab will allow you to assign Researchers to their Projects.","title":"Projects"},{"location":"Administrator/admin-ui-setup/project-setup/#introduction","text":"Researchers are submitting Jobs via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and prioritize work, Run:AI introduces the concept of Projects . Projects are quota entities that associate a Project name with GPU allocation and allocation preferences. A Researcher submitting a Job needs to associate a Project name with the request. The Run:AI scheduler will compare the request against the current allocations and the Project and determine whether the workload can be allocated resources or whether it should remain in the queue for future allocation.","title":"Introduction"},{"location":"Administrator/admin-ui-setup/project-setup/#modeling-projects","text":"As an Admin, you need to determine how to model Projects. You can: Set a Project per user. Set a Project per team of users. Set a Project per a real organizational Project.","title":"Modeling Projects"},{"location":"Administrator/admin-ui-setup/project-setup/#project-quotas","text":"Each Project is associated with a quota of GPUs that can be allocated for this Project at the same time. This is guaranteed quota in the sense that Researchers using this Project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this Project can receive an over-quota . As long as GPUs are unused, a Researcher using this Project can get more GPUs. However, these GPUs can be taken away at a moment's notice. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the Project allocation should be equal to the number of GPUs in the cluster.","title":"Project Quotas"},{"location":"Administrator/admin-ui-setup/project-setup/#working-with-projects","text":"","title":"Working with Projects"},{"location":"Administrator/admin-ui-setup/project-setup/#create-a-new-project","text":"Note In order to be able to manipulate Projects, you must have Editor access. See the \"Users\" Area Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects On the top right, select \"Add New Project\" Choose a Project name and a Project quota Press \"Save\"","title":"Create a new Project"},{"location":"Administrator/admin-ui-setup/project-setup/#update-an-existing-project","text":"Select an existing Project. Right-click and press \"Edit\". Update the values and press \"Save\".","title":"Update an existing Project"},{"location":"Administrator/admin-ui-setup/project-setup/#delete-an-existing-project","text":"Select an existing Project. Right-click and press \"Delete\".","title":"Delete an existing Project"},{"location":"Administrator/admin-ui-setup/project-setup/#limit-jobs-to-run-on-specific-node-groups","text":"A frequent use case is to assign specific Projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes. While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization.","title":"Limit Jobs to run on Specific Node Groups"},{"location":"Administrator/admin-ui-setup/project-setup/#grouping-nodes","text":"To set node affinities, you must first annotate nodes with labels. These labels will later be associated with Projects. Each node can only be annotated with a single name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 You can annotate multiple nodes with the same label","title":"Grouping Nodes"},{"location":"Administrator/admin-ui-setup/project-setup/#setting-affinity-for-a-specific-project","text":"To mandate training Jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive Jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the Project dialog.","title":"Setting Affinity for a Specific Project"},{"location":"Administrator/admin-ui-setup/project-setup/#further-affinity-refinement-by-the-researcher","text":"The Researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific Project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information runai submit","title":"Further Affinity Refinement by the Researcher"},{"location":"Administrator/admin-ui-setup/project-setup/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive Jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive Jobs: Create a Project or edit an existing Project. Go to the Time Limit tab and set a limit (day, hour, minute). The setting only takes effect for Jobs that have started after the duration has been changed.","title":"Limit Duration of Interactive Jobs"},{"location":"Administrator/admin-ui-setup/project-setup/#assign-users-to-project","text":"When Researcher Authentication is enabled, the Project form will contain an additional Users tab. The tab will allow you to assign Researchers to their Projects.","title":"Assign Users to Project"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/","text":"Rancher is a software that manages Kubernetes clusters. Some customers provide Rancher to data scientists in order to launch workloads. This guide provides step by step instructions on how to launch workloads via Rancher. It assumes the reader has some familiarity with Rancher itself. There are other ways for data scientists to launch Workloads such as the Run:AI CLI or Kubeflow . The advantage of Rancher is the usage of a user interface. The disadvantage is that it exposes the data scientist to Kubernetes/Docker terminology that would otherwise remain hidden Types of Workloads \u00b6 We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Training session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports Terminology \u00b6 Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the Job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI Projects\" (not to be confused with Rancher Projects). Each Project contains a GPU quota. Each workload must be annotated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads Using Rancher to Launch Workloads \u00b6 Using your browser, navigate to Rancher Login to Rancher with your user name and password Click on the top left menu, go to the company's assigned cluster and default Rancher Project (not to be confused with a Run:AI Project) Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud a environment this would typically be a Layer-4 load balancer. On-premise environments depend on how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. * Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" * On the bottom and select show advanced options . Expand labels and labels and add 2 labels, adding the name of the user and the name of the Project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Submit workloads via Rancher"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#types-of-workloads","text":"We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Training session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports","title":"Types of Workloads"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#terminology","text":"Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the Job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI Projects\" (not to be confused with Rancher Projects). Each Project contains a GPU quota. Each workload must be annotated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads","title":"Terminology"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#using-rancher-to-launch-workloads","text":"Using your browser, navigate to Rancher Login to Rancher with your user name and password Click on the top left menu, go to the company's assigned cluster and default Rancher Project (not to be confused with a Run:AI Project) Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud a environment this would typically be a Layer-4 load balancer. On-premise environments depend on how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. * Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" * On the bottom and select show advanced options . Expand labels and labels and add 2 labels, adding the name of the user and the name of the Project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Using Rancher to Launch Workloads"},{"location":"Researcher/overview-researcher/","text":"Overview: Researcher Documentation \u00b6 Researchers use Run:AI to submit jobs. As part of the Researcher documentation you will find: Quickstart Guides which provide step-by-step guides to Run:AI technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:AI. Information about the Run:AI Scheduler . The Run:AI Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Introductory Presentations .","title":"Overview"},{"location":"Researcher/overview-researcher/#overview-researcher-documentation","text":"Researchers use Run:AI to submit jobs. As part of the Researcher documentation you will find: Quickstart Guides which provide step-by-step guides to Run:AI technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:AI. Information about the Run:AI Scheduler . The Run:AI Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Introductory Presentations .","title":"Overview: Researcher Documentation"},{"location":"Researcher/Presentations/Researcher-Onboarding-Presentation/","text":"","title":"Researcher Onboarding"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/","text":"Introduction \u00b6 When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs. Requesting CPU & Memory \u00b6 When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit CPU over allocation \u00b6 The number of CPUs your Job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively. If the flag --cpu is not specified, it will be taken from the cluster default (see the section below) Memory over allocation \u00b6 The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out of memory exception and terminate. CPU and Memory limits \u00b6 You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your Job will never be allocated with more than the amount stated in the --cpu-limit flag If your Job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. The limit (for both CPU and memory) overrides the cluster default described in the section below For further details on these flags see: runai submit Flag Defaults \u00b6 Defaults for --cpu flag \u00b6 If your Job has not specified --cpu , the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your Job has only specified --gpu 2 and has not specified --cpu , then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change the ratio see below. Defaults for --memory flag \u00b6 If your Job has not specified --memory , the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change the ratio see below. Defaults for --cpu-limit flag \u00b6 If your Job has not specified --cpu-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio. Defaults for --memory-limit flag \u00b6 If your Job has not specified --memory-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio. Changing the ratios \u00b6 To change the cluster wide-ratio use the following command. The command below sets: a CPU request with a default ratio of 2:1 CPUs to GPUs. a Memory request with a default ratio of 200MB per GPU. a CPU limit with a default ratio of 4:1 CPU to GPU. a Memory limit with a default ratio of 2GB per GPU. kubectl patch runaiconfig runai -n runai --type = 'json' \\ -p = '[{\"op\": \"add\", \"path\": \"/spec/limitRange\", \"value\": {\"cpuDefaultRequestGpuFactor\": \"2\", \"memoryDefaultRequestGpuFactor\": \"200Mi\", \"cpuDefaultLimitGpuFactor\" : \"4\", \"memoryDefaultLimitGpuFactor\" : \"2Gi\" }}]' You must specify all 4 values. Validating CPU & Memory Allocations \u00b6 To review CPU & Memory allocations you need to look into Kubernetes. A Run:AI Job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the Job by running: runai describe job <JOB_NAME> the pod will appear under the PODS category. Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia.com/gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia.com/gpu : 2","title":"Allocation of CPU and Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#introduction","text":"When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs.","title":"Introduction"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#requesting-cpu-memory","text":"When submitting a Job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the Job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit","title":"Requesting CPU &amp; Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-over-allocation","text":"The number of CPUs your Job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively. If the flag --cpu is not specified, it will be taken from the cluster default (see the section below)","title":"CPU over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#memory-over-allocation","text":"The amount of Memory your Job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your Job may receive an out of memory exception and terminate.","title":"Memory over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-and-memory-limits","text":"You can limit your Job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your Job will never be allocated with more than the amount stated in the --cpu-limit flag If your Job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. The limit (for both CPU and memory) overrides the cluster default described in the section below For further details on these flags see: runai submit","title":"CPU and Memory limits"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#flag-defaults","text":"","title":"Flag Defaults"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-cpu-flag","text":"If your Job has not specified --cpu , the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your Job has only specified --gpu 2 and has not specified --cpu , then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change the ratio see below.","title":"Defaults for --cpu flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-memory-flag","text":"If your Job has not specified --memory , the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change the ratio see below.","title":"Defaults for --memory flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-cpu-limit-flag","text":"If your Job has not specified --cpu-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to CPUs. See below on how to change the ratio.","title":"Defaults for --cpu-limit flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-memory-limit-flag","text":"If your Job has not specified --memory-limit , then by default, the system will not set a limit. You can set a cluster-wide limit as a ratio of GPUs to Memory. See below on how to change the ratio.","title":"Defaults for --memory-limit flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#changing-the-ratios","text":"To change the cluster wide-ratio use the following command. The command below sets: a CPU request with a default ratio of 2:1 CPUs to GPUs. a Memory request with a default ratio of 200MB per GPU. a CPU limit with a default ratio of 4:1 CPU to GPU. a Memory limit with a default ratio of 2GB per GPU. kubectl patch runaiconfig runai -n runai --type = 'json' \\ -p = '[{\"op\": \"add\", \"path\": \"/spec/limitRange\", \"value\": {\"cpuDefaultRequestGpuFactor\": \"2\", \"memoryDefaultRequestGpuFactor\": \"200Mi\", \"cpuDefaultLimitGpuFactor\" : \"4\", \"memoryDefaultLimitGpuFactor\" : \"2Gi\" }}]' You must specify all 4 values.","title":"Changing the ratios"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#validating-cpu-memory-allocations","text":"To review CPU & Memory allocations you need to look into Kubernetes. A Run:AI Job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the Job by running: runai describe job <JOB_NAME> the pod will appear under the PODS category. Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia.com/gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia.com/gpu : 2","title":"Validating CPU &amp; Memory Allocations"},{"location":"Researcher/Scheduling/Job-Statuses/","text":"Introduction \u00b6 The runai submit function and its sibling the runai submit-mpi function submit Run:AI Jobs for execution. A Job has a status . Once a Job is submitted it goes through a number of statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:AI-specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job. Successful Flow \u00b6 A regular, training Job which has no errors and executes without preemption would go through the following statuses: Pending - the Job is waiting to be scheduled. ContainerCreating - the Job has been scheduled, the Job docker image is now downloading. Running - the Job is now executing. Succeeded - the Job has finished with exit code 0 (success). The Job can be preempted, in which case it can go through other statuses: Terminating - the Job is now being preempted. Pending - the Job is waiting in queue again to receive resources. An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see the table below. Error flow \u00b6 A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number or retires the Job will enter a final status of Fail An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image which cannot be downloaded. There are special statuses for such Jobs. See table below Status Table \u00b6 Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Administrator User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive Job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible Job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:AI Scheduler wasn't running when the Job has finished. How to get more information \u00b6 The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run: runai describe job <job-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run: runai logs <job-name> Distributed Training (mpi) Jobs \u00b6 A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed Jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Job Statuses"},{"location":"Researcher/Scheduling/Job-Statuses/#introduction","text":"The runai submit function and its sibling the runai submit-mpi function submit Run:AI Jobs for execution. A Job has a status . Once a Job is submitted it goes through a number of statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:AI-specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job.","title":"Introduction"},{"location":"Researcher/Scheduling/Job-Statuses/#successful-flow","text":"A regular, training Job which has no errors and executes without preemption would go through the following statuses: Pending - the Job is waiting to be scheduled. ContainerCreating - the Job has been scheduled, the Job docker image is now downloading. Running - the Job is now executing. Succeeded - the Job has finished with exit code 0 (success). The Job can be preempted, in which case it can go through other statuses: Terminating - the Job is now being preempted. Pending - the Job is waiting in queue again to receive resources. An interactive Job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see the table below.","title":"Successful Flow"},{"location":"Researcher/Scheduling/Job-Statuses/#error-flow","text":"A regular, training Job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The Job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number or retires the Job will enter a final status of Fail An interactive Job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image which cannot be downloaded. There are special statuses for such Jobs. See table below","title":"Error flow"},{"location":"Researcher/Scheduling/Job-Statuses/#status-table","text":"Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Administrator User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive Job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible Job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The Job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:AI Scheduler wasn't running when the Job has finished.","title":"Status Table"},{"location":"Researcher/Scheduling/Job-Statuses/#how-to-get-more-information","text":"The system stores various events during the Job's lifecycle. These events can be helpful in diagnosing issues around Job scheduling. To view these events run: runai describe job <job-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, Jobs that have exited with an exit code different than zero may write an exit reason in this log. To see Job logs run: runai logs <job-name>","title":"How to get more information"},{"location":"Researcher/Scheduling/Job-Statuses/#distributed-training-mpi-jobs","text":"A distributed (mpi) Job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed Jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Distributed Training (mpi) Jobs"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/","text":"Introduction \u00b6 At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works. Terminology \u00b6 Workload Types \u00b6 Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads.Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint. Projects \u00b6 Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences. A Researcher submitting a workload must associate a Project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on Projects and how to configure them, see: Working with Projects Departments \u00b6 A Department is a second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Project quota. For further information on Departments and how to configure them, see: Working with Departments Pods \u00b6 Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory. Basic Scheduling Concepts \u00b6 Interactive vs. Unattended \u00b6 The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted. Guaranteed Quota and Over-Quota \u00b6 Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a Project does not end up with over-quota made fully of interactive workloads. For additional details see below Scheduler Details \u00b6 Allocation & Preemption \u00b6 The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived Project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads Reclaim \u00b6 During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects. Fairness \u00b6 The Run:AI scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both Projects go over quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends. This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this equivalence Bin-packing & Consolidation \u00b6 Part of an efficient scheduler is the ability to eliminate defragmentation: The first step in avoiding defragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to defragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload. Elasticity \u00b6 Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of consolidation helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink Jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload. Advanced \u00b6 GPU Fractions \u00b6 Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB or memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made in fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Elasticity is not supported with fractions. Hyperparameter Optimization supports fractions. Distributed Training \u00b6 Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:AI spawns an additional launcher process which manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what is typically multiple nodes, before the Job can start. If one pod is preempted, the others are also immediately preempted. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:AI system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:AI see here Hyperparameter Optimization \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With HPO, the Researcher provides a single script which is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods are exactly as described in the Scheduler Details section above for Jobs. For more information on Hyperparameter Optimization in Run:AI see here","title":"The Run:AI Scheduler"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#introduction","text":"At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works.","title":"Introduction"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#terminology","text":"","title":"Terminology"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#workload-types","text":"Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads.Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the Researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the Researcher is to save checkpoints and allow the code to restore from the last checkpoint.","title":"Workload Types"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#projects","text":"Projects are quota entities that associate a Project name with a deserved GPU quota as well as other preferences. A Researcher submitting a workload must associate a Project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the Project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on Projects and how to configure them, see: Working with Projects","title":"Projects"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#departments","text":"A Department is a second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Project quota. For further information on Departments and how to configure them, see: Working with Departments","title":"Departments"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#pods","text":"Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparameter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.","title":"Pods"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#basic-scheduling-concepts","text":"","title":"Basic Scheduling Concepts"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#interactive-vs-unattended","text":"The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted.","title":"Interactive vs. Unattended"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#guaranteed-quota-and-over-quota","text":"Every new workload is associated with a Project. The Project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the Project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the Project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a Project does not end up with over-quota made fully of interactive workloads. For additional details see below","title":"Guaranteed Quota and Over-Quota"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#scheduler-details","text":"","title":"Scheduler Details"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#allocation-preemption","text":"The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived Project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the Project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' Project and continues with the same flow until it finishes attempting to schedule all workloads","title":"Allocation &amp; Preemption"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#reclaim","text":"During the above process, there may be a pending workload whose Project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another Project which has gone over-quota while preserving fairness between Projects.","title":"Reclaim"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#fairness","text":"The Run:AI scheduler determines fairness between multiple over-quota Projects according to their GPU quota. Consider for example two Projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different Projects regardless of the time they entered the system. The fairness works according to the relative portion of the GPU quota for each Project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both Projects go over quota, Project A will receive 75% (=3/(1+3)) of the idle GPUs and Project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new Job is submitted to the system or an existing Job ends. This fairness equivalence will also be maintained amongst running Jobs. The scheduler will preempt training sessions to maintain this equivalence","title":"Fairness"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#bin-packing-consolidation","text":"Part of an efficient scheduler is the ability to eliminate defragmentation: The first step in avoiding defragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate Jobs on demand. If a workload cannot be allocated due to defragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload.","title":"Bin-packing &amp; Consolidation"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#elasticity","text":"Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of consolidation helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink Jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload.","title":"Elasticity"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#advanced","text":"","title":"Advanced"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#gpu-fractions","text":"Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between Jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB or memory, then the Job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memory exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made in fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Elasticity is not supported with fractions. Hyperparameter Optimization supports fractions.","title":"GPU Fractions"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#distributed-training","text":"Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:AI spawns an additional launcher process which manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what is typically multiple nodes, before the Job can start. If one pod is preempted, the others are also immediately preempted. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same Job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:AI system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:AI see here","title":"Distributed Training"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#hyperparameter-optimization","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With HPO, the Researcher provides a single script which is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods are exactly as described in the Scheduler Details section above for Jobs. For more information on Hyperparameter Optimization in Run:AI see here","title":"Hyperparameter Optimization"},{"location":"Researcher/Scheduling/auto-delete-jobs/","text":"Introduction \u00b6 Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When Jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old Jobs by running the command: runai list jobs You can delete the Job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a Job for automatic deletion some time after its finish. Important Deleting a Job, deletes the container behind it, and with it all related information such as Job logs. Data that was saved by the Researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface Enable Automatic Deletion in Cluster (Admin only) \u00b6 In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray . Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to/etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube-apiserver ..... - --feature-gates=TTLAfterFinished=true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube-controller-manager ..... - --feature-gates=TTLAfterFinished=true Automatic Deletion \u00b6 When starting the Job, add the flag --ttl-after-finish duration . duration is the duration, post Job finish, after which the Job is automatically deleted. Example durations are: 5s, 2m, 3h, 4d etc. For example, the following call will delete the Job 2 hours after the Job finishes: runai submit myjob1 --ttl-after-finish 2h Using Templates to set Automatic Deletion as Default \u00b6 You can use Run:AI templates to set auto-delete to be the default. See template configuration for more information on how to make this flag a part of the default template.","title":"Automatically Delete Finished Workloads"},{"location":"Researcher/Scheduling/auto-delete-jobs/#introduction","text":"Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When Jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old Jobs by running the command: runai list jobs You can delete the Job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a Job for automatic deletion some time after its finish. Important Deleting a Job, deletes the container behind it, and with it all related information such as Job logs. Data that was saved by the Researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface","title":"Introduction"},{"location":"Researcher/Scheduling/auto-delete-jobs/#enable-automatic-deletion-in-cluster-admin-only","text":"In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray . Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to/etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube-apiserver ..... - --feature-gates=TTLAfterFinished=true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube-controller-manager ..... - --feature-gates=TTLAfterFinished=true","title":"Enable Automatic Deletion in Cluster (Admin only)"},{"location":"Researcher/Scheduling/auto-delete-jobs/#automatic-deletion","text":"When starting the Job, add the flag --ttl-after-finish duration . duration is the duration, post Job finish, after which the Job is automatically deleted. Example durations are: 5s, 2m, 3h, 4d etc. For example, the following call will delete the Job 2 hours after the Job finishes: runai submit myjob1 --ttl-after-finish 2h","title":"Automatic Deletion"},{"location":"Researcher/Scheduling/auto-delete-jobs/#using-templates-to-set-automatic-deletion-as-default","text":"You can use Run:AI templates to set auto-delete to be the default. See template configuration for more information on how to make this flag a part of the default template.","title":"Using Templates to set Automatic Deletion as Default"},{"location":"Researcher/Walkthroughs/Run-AI-Walkthroughs/","text":"Below are a set of Quickstart documents. The purpose of these documents is to get you acquainted with an aspect of Run:AI in the simplest possible form. Follow the Quickstart documents below to learn more: Unattended training sessions Interactive build sessions Interactive build sessions with externalized services Using GPU Fractions Distributed Training Hyperparameter Optimization Over-Quota, Basic Fairness & Bin Packing Fairness Elasticity","title":"Run:AI Quickstart Guides"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","text":"Quickstart: Launch Interactive Build Workloads with Connected Ports \u00b6 Introduction \u00b6 This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads When starting a container with the Run:AI Command-Line Interface (CLI), it is possible to expose internal ports to the container user. Exposing a Container Port \u00b6 There are a number of ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides in. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<port> The document below provides examples for Port Forwarding and Ingress . Contact your Administrator to see which methods are available in your cluster Note The step below use a Jupyter Notebook as an example for how to expose Ports. There is also a special shortcut for starting a Jupyter Notebook detailed here . Port Forwarding, Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project Run Workload \u00b6 At the command-line run: runai config project team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 --interactive \\ --service-type = portforward --port 8888 :8888 --command -- start-notebook.sh --NotebookApp.base_url = jupyter1 The Job is based on a generic Jupyter notebook docker image jupyter/base-notebook We named the Job jupyter1 . Note that in this Jupyter implementation, the name of the Job should also be copied to the Notebook base URL. Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job. The Job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the runai submit command is not stopped Open the Jupyter notebook \u00b6 Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook. To get the full URL with the notebook token, run the following in another shell: runai logs jupyter1 -p team-a Ingress, Step by Step Walkthrough \u00b6 Note: Ingress must be set up by your Administrator prior to usage. For more information see: Exposing Ports from Researcher Containers Using Ingress . Setup \u00b6 Perform the setup steps for port forwarding above. Run Workload \u00b6 At the command-line run: runai config project team-a runai submit test-ingress -i jupyter/base-notebook -g 1 --interactive \\ --service-type = ingress --port 8888 --command -- start-notebook.sh --NotebookApp.base_url = team-a-test-ingress An ingress service URL will be created, run: runai list jobs You will see the service URL with which to access the Jupyter notebook Important With ingress, Run:AI creates an access URL whose domain is uniform (and is IP which serves as the access point to the cluster). The rest of the path is unique and is build as: <project-name>-<job-name> . Thus, with the example above, we must set the Jupyter notebook base URL to respond to the service at team-a-test-ingress See Also \u00b6 Develop on Run:AI using Visual Studio Code Develop on Run:AI using PyCharm","title":"Build with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#quickstart-launch-interactive-build-workloads-with-connected-ports","text":"","title":"Quickstart: Launch Interactive Build Workloads with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","text":"This Quickstart is an extension of the Quickstart document: Start and Use Interactive Build Workloads When starting a container with the Run:AI Command-Line Interface (CLI), it is possible to expose internal ports to the container user.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","text":"There are a number of ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides in. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<port> The document below provides examples for Port Forwarding and Ingress . Contact your Administrator to see which methods are available in your cluster Note The step below use a Jupyter Notebook as an example for how to expose Ports. There is also a special shortcut for starting a Jupyter Notebook detailed here .","title":"Exposing a Container Port"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walkthrough","text":"","title":"Port Forwarding, Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","text":"At the command-line run: runai config project team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 --interactive \\ --service-type = portforward --port 8888 :8888 --command -- start-notebook.sh --NotebookApp.base_url = jupyter1 The Job is based on a generic Jupyter notebook docker image jupyter/base-notebook We named the Job jupyter1 . Note that in this Jupyter implementation, the name of the Job should also be copied to the Notebook base URL. Note the interactive flag which means the Job will not have a start or end. It is the Researcher's responsibility to close the Job. The Job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the runai submit command is not stopped","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#open-the-jupyter-notebook","text":"Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook. To get the full URL with the notebook token, run the following in another shell: runai logs jupyter1 -p team-a","title":"Open the Jupyter notebook"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#ingress-step-by-step-walkthrough","text":"Note: Ingress must be set up by your Administrator prior to usage. For more information see: Exposing Ports from Researcher Containers Using Ingress .","title":"Ingress, Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup_1","text":"Perform the setup steps for port forwarding above.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload_1","text":"At the command-line run: runai config project team-a runai submit test-ingress -i jupyter/base-notebook -g 1 --interactive \\ --service-type = ingress --port 8888 --command -- start-notebook.sh --NotebookApp.base_url = team-a-test-ingress An ingress service URL will be created, run: runai list jobs You will see the service URL with which to access the Jupyter notebook Important With ingress, Run:AI creates an access URL whose domain is uniform (and is IP which serves as the access point to the cluster). The rest of the path is unique and is build as: <project-name>-<job-name> . Thus, with the example above, we must set the Jupyter notebook base URL to respond to the service at team-a-test-ingress","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","text":"Develop on Run:AI using Visual Studio Code Develop on Run:AI using PyCharm","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-build/","text":"Quickstart: Launch Interactive Build Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Quickstart \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project Run Workload \u00b6 At the command-line run: runai config project team-a runai submit build1 -i ubuntu -g 1 --interactive --command -- sleep infinity The job is based on a sample docker image python We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is sleep infinity . You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with --attach to attach immediately to a session. Follow up on the job's status by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai describe job build1 Get a Shell to the container \u00b6 Run: runai bash build1 This should provide a direct shell into the computer View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai/jobs Under \"Jobs\" you can view the new Workload: Stop Workload \u00b6 Run the following: runai delete build1 This would stop the training workload. You can verify this by running runai list jobs again. Next Steps \u00b6 Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports . Follow the Quickstart document: Launch Unattended Training Workloads .","title":"Build"},{"location":"Researcher/Walkthroughs/walkthrough-build/#quickstart-launch-interactive-build-workloads","text":"","title":"Quickstart: Launch Interactive Build Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-quickstart","text":"","title":"Step by Step Quickstart"},{"location":"Researcher/Walkthroughs/walkthrough-build/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build/#run-workload","text":"At the command-line run: runai config project team-a runai submit build1 -i ubuntu -g 1 --interactive --command -- sleep infinity The job is based on a sample docker image python We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the Researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is sleep infinity . You must provide a command or the container will start and then exit immediately. Alternatively, replace these flags with --attach to attach immediately to a session. Follow up on the job's status by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai describe job build1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","text":"Run: runai bash build1 This should provide a direct shell into the computer","title":"Get a Shell to the container"},{"location":"Researcher/Walkthroughs/walkthrough-build/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai/jobs Under \"Jobs\" you can view the new Workload:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","text":"Run the following: runai delete build1 This would stop the training workload. You can verify this by running runai list jobs again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#next-steps","text":"Expose internal ports to your interactive build workload: Quickstart Launch an Interactive Build Workload with Connected Ports . Follow the Quickstart document: Launch Unattended Training Workloads .","title":"Next Steps"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","text":"Quickstart: Launch Distributed Training Workloads \u00b6 Introduction \u00b6 Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project Run Training Distributed Workload \u00b6 At the command-line run: runai config project team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed We named the Job dist The Job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed . The image contains a startup script that runs a deep learning Horovod-based workload. Follow up on the Job's status by running: runai list jobs The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai describe job dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist Run an Interactive Distributed Workload \u00b6 It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed --interactive \\ --command -- sh -c sleep infinity When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run: horovodrun -np $RUNAI_MPI_NUM_WORKERS \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod For Horovod version 0.17.0 or later, add the -hostfile flag as follows: horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod The environment variable RUNAI_MPI_NUM_WORKERS is passed by Run:AI and contains the number of worker processes provided to the runai submit-mpi command (in the above example the value is 2). See Also \u00b6 The source code of the image used in this Quickstart document is in Github For a full list of the submit-mpi options see runai submit-mpi","title":"Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#quickstart-launch-distributed-training-workloads","text":"","title":"Quickstart: Launch Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","text":"Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a Quickstart document for such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-training-distributed-workload","text":"At the command-line run: runai config project team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed We named the Job dist The Job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed . The image contains a startup script that runs a deep learning Horovod-based workload. Follow up on the Job's status by running: runai list jobs The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai describe job dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist","title":"Run Training Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-an-interactive-distributed-workload","text":"It is also possible to run a distributed training Job as \"interactive\". This is useful if you want to test your distributed training Job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed --interactive \\ --command -- sh -c sleep infinity When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed workload. For Horovod version smaller than 0.17.0 run: horovodrun -np $RUNAI_MPI_NUM_WORKERS \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod For Horovod version 0.17.0 or later, add the -hostfile flag as follows: horovodrun -np $RUNAI_MPI_NUM_WORKERS -hostfile /etc/mpi/hostfile \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod The environment variable RUNAI_MPI_NUM_WORKERS is passed by Run:AI and contains the number of worker processes provided to the runai submit-mpi command (in the above example the value is 2).","title":"Run an Interactive Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#see-also","text":"The source code of the image used in this Quickstart document is in Github For a full list of the submit-mpi options see runai submit-mpi","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/","text":"Quickstart: Elasticity, Dynamically Stretch or Compress Workload's GPU Allocation \u00b6 Introduction \u00b6 Elasticity allows unattended, train-based workloads to shrink or expand based on the cluster's availability. Shrinking a training Job allows your workload to run on a smaller number of GPUs than the Researcher code was originally written for. Expanding a training Job allows your workload to run on more GPUs than the Researcher code was originally written for. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Run:AI Python Researcher Library installed on a docker image Step by Step Walkthrough \u00b6 Setup \u00b6 A GPU cluster with a single node of 2 GPUs. If the cluster contains more than one node, use Node affinity to simulate a single node or use more filler Jobs as described below. If the cluster nodes contain more than 2 GPUs, you can create an interactive Job on a different project to consume the remaining GPUs. Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project Expansion \u00b6 At the command-line run: runai config project team-a runai submit elastic1 -i gcr.io/run-ai-demo/quickstart -g 1 --elastic This would start an unattended training Job for team-a The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job elastic1 and have requested 1 GPU for the Job The flag --elastic enables the Elasticity feature Follow up on the Job's progress by running: runai list jobs The result: Discussion The Job has requested 1 GPU, but has been allocated with 2, as 2 are available right now. The code needs to be ready to accept more GPUs than it requested, otherwise, the GPUs will not be utilized. The Run:AI Elasticity library helps with expanding the Job effectively. Add a filler class: runai submit filler1 -i ubuntu -g 1 --interactive --command -- sleep infinity runai list jobs The result: Discussion An interactive Job (filler1) needs to be scheduled. The elastic Job is now reduced to the originally requested single-GPU. Finally, delete the Jobs: runai delete elastic1 filler1 Shrinking \u00b6 At the command-line run: runai submit filler2 -i ubuntu -g 1 --interactive --command -- sleep infinity runai submit elastic2 -i gcr.io/run-ai-demo/quickstart -g 2 --elastic This would start a filler Job on 1 GPU and attempt to start another unattended Job with 2 GPUs Follow up on the Job's progress by running: runai list jobs The result: Discussion Since only a single GPU remains unallocated, under normal circumstances, the Job should not start. However, the --elastic flag tells the system to allocate a single GPU instead. Delete the filler Job and list the Jobs again: runai delete filler2 runai list jobs The result: Discussion With the filler Job gone, the elastic Job has more room to expand, which it does. Finally, delete the Job: runai delete elastic2 See Also \u00b6 For more information on the elasticity module of the Researcher python library, see Researcher library : Elasticity Keras Sample code in Github Pytorch Sample code in Github","title":"Elasticity"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#quickstart-elasticity-dynamically-stretch-or-compress-workloads-gpu-allocation","text":"","title":"Quickstart: Elasticity, Dynamically Stretch or Compress Workload's GPU Allocation"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#introduction","text":"Elasticity allows unattended, train-based workloads to shrink or expand based on the cluster's availability. Shrinking a training Job allows your workload to run on a smaller number of GPUs than the Researcher code was originally written for. Expanding a training Job allows your workload to run on more GPUs than the Researcher code was originally written for.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Run:AI Python Researcher Library installed on a docker image","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#setup","text":"A GPU cluster with a single node of 2 GPUs. If the cluster contains more than one node, use Node affinity to simulate a single node or use more filler Jobs as described below. If the cluster nodes contain more than 2 GPUs, you can create an interactive Job on a different project to consume the remaining GPUs. Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#expansion","text":"At the command-line run: runai config project team-a runai submit elastic1 -i gcr.io/run-ai-demo/quickstart -g 1 --elastic This would start an unattended training Job for team-a The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job elastic1 and have requested 1 GPU for the Job The flag --elastic enables the Elasticity feature Follow up on the Job's progress by running: runai list jobs The result: Discussion The Job has requested 1 GPU, but has been allocated with 2, as 2 are available right now. The code needs to be ready to accept more GPUs than it requested, otherwise, the GPUs will not be utilized. The Run:AI Elasticity library helps with expanding the Job effectively. Add a filler class: runai submit filler1 -i ubuntu -g 1 --interactive --command -- sleep infinity runai list jobs The result: Discussion An interactive Job (filler1) needs to be scheduled. The elastic Job is now reduced to the originally requested single-GPU. Finally, delete the Jobs: runai delete elastic1 filler1","title":"Expansion"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#shrinking","text":"At the command-line run: runai submit filler2 -i ubuntu -g 1 --interactive --command -- sleep infinity runai submit elastic2 -i gcr.io/run-ai-demo/quickstart -g 2 --elastic This would start a filler Job on 1 GPU and attempt to start another unattended Job with 2 GPUs Follow up on the Job's progress by running: runai list jobs The result: Discussion Since only a single GPU remains unallocated, under normal circumstances, the Job should not start. However, the --elastic flag tells the system to allocate a single GPU instead. Delete the filler Job and list the Jobs again: runai delete filler2 runai list jobs The result: Discussion With the filler Job gone, the elastic Job has more room to expand, which it does. Finally, delete the Job: runai delete elastic2","title":"Shrinking"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#see-also","text":"For more information on the elasticity module of the Researcher python library, see Researcher library : Elasticity Keras Sample code in Github Pytorch Sample code in Github","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","text":"Quickstart: Launch Workloads with GPU Fractions \u00b6 Introduction \u00b6 Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 1 GPU to the Project Run Workload \u00b6 At the command-line run: runai config project team - a runai submit frac05 - i gcr . io / run - ai - demo / quickstart - g 0 . 5 --interactive runai submit frac03 - i gcr . io / run - ai - demo / quickstart - g 0 . 3 The Jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the Jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. The Jobs are assigned to team-a with an allocation of a single GPU. Follow up on the Job's status by running: runai list jobs The result: Note that both Jobs were allocated to the same node. When both Jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception Use Exact GPU Memory \u00b6 Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example: runai submit -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G Which will provide 5GB of GPU memory.","title":"GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#quickstart-launch-workloads-with-gpu-fractions","text":"","title":"Quickstart: Launch Workloads with GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","text":"Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 Jobs running on the same GPU, meaning you could do eight times the work with the same hardware.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 1 GPU to the Project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","text":"At the command-line run: runai config project team - a runai submit frac05 - i gcr . io / run - ai - demo / quickstart - g 0 . 5 --interactive runai submit frac03 - i gcr . io / run - ai - demo / quickstart - g 0 . 3 The Jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the Jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the Job will not automatically finish. Rather, it is the Researcher's responsibility to delete the Job. Fractions support both Interactive and non-interactive Jobs. The Jobs are assigned to team-a with an allocation of a single GPU. Follow up on the Job's status by running: runai list jobs The result: Note that both Jobs were allocated to the same node. When both Jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#use-exact-gpu-memory","text":"Instead of requesting a fraction of the GPU, you can ask for specific GPU memory requirements. For example: runai submit -i gcr.io/run-ai-demo/quickstart --gpu-memory 5G Which will provide 5GB of GPU memory.","title":"Use Exact GPU Memory"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/","text":"Quickstart: Hyperparameter Optimization \u00b6 Introduction \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are a number of strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the later does an exhaustive search from a list of pre-selected values. Run:AI provides the ability to run, manage, and view HPO runs. The following is a Quickstart of such a scenario. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project On shared storage create a library to store HPO results. E.g. /nfs/john/hpo Pods \u00b6 With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory. HPO Sample Code \u00b6 The Quickstart code can be found in github.com/run-ai/docs . The code uses the Run:AI Researcher python library . Below are some highlights of the code: # import Run:AI HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:AI HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/nfs\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ]) Run an HPO Workload \u00b6 At the command-line run: runai config project team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/nfs We named the Job hpo1 The Job is assigned to team-a The Job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /nfs . The running pods will use the directory to sync hyperparameters and save results. Follow up on the Job's status by running: runai list jobs The result: Follow up on the Job's pods by running: runai describe job hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit Jobs on another Project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO Job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai describe job hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001} Examine the Results \u00b6 The Run:AI HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is a snapshot of the file for two experiments with two epochs each: creationTime: 24/08/2020 08:50:06 experiments: - config: batch_size: 32 lr: 1 id: 1 modificationTime: 24/08/2020 08:50:06 reports: - epoch: 0 metrics: acc: 0.09814 loss: 2.310984723968506 val_acc: 0.1 val_loss: 2.3098626373291014 reportTime: 24/08/2020 08:52:11 - epoch: 1 metrics: acc: 0.09914 loss: 2.30994320602417 val_acc: 0.1 val_loss: 2.3110838134765626 reportTime: 24/08/2020 08:54:10 - config: batch_size: 32 lr: 0.1 id: 2 modificationTime: 24/08/2020 08:50:36 reports: - epoch: 0 metrics: acc: 0.11012 loss: 2.2979678358459474 val_acc: 0.1667 val_loss: 2.268467852783203 reportTime: 24/08/2020 08:52:44 - epoch: 1 metrics: acc: 0.2047 loss: 2.0894255745697023 val_acc: 0.2833 val_loss: 1.8615504817962647 reportTime: 24/08/2020 08:54:45 Finally, you can delete the HPO Job by running: runai delete hpo1 See Also \u00b6 For further information on the Run:AI HPO support library see: The Run:AI HPO Support Library Sample code in Github","title":"Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#quickstart-hyperparameter-optimization","text":"","title":"Quickstart: Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#introduction","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are a number of strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the later does an exhaustive search from a list of pre-selected values. Run:AI provides the ability to run, manage, and view HPO runs. The following is a Quickstart of such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\" Allocate 2 GPUs to the Project On shared storage create a library to store HPO results. E.g. /nfs/john/hpo","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#pods","text":"With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.","title":"Pods"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#hpo-sample-code","text":"The Quickstart code can be found in github.com/run-ai/docs . The code uses the Run:AI Researcher python library . Below are some highlights of the code: # import Run:AI HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:AI HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/nfs\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ])","title":"HPO Sample Code"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#run-an-hpo-workload","text":"At the command-line run: runai config project team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/nfs We named the Job hpo1 The Job is assigned to team-a The Job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /nfs . The running pods will use the directory to sync hyperparameters and save results. Follow up on the Job's status by running: runai list jobs The result: Follow up on the Job's pods by running: runai describe job hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit Jobs on another Project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO Job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai describe job hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001}","title":"Run an HPO Workload"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#examine-the-results","text":"The Run:AI HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is a snapshot of the file for two experiments with two epochs each: creationTime: 24/08/2020 08:50:06 experiments: - config: batch_size: 32 lr: 1 id: 1 modificationTime: 24/08/2020 08:50:06 reports: - epoch: 0 metrics: acc: 0.09814 loss: 2.310984723968506 val_acc: 0.1 val_loss: 2.3098626373291014 reportTime: 24/08/2020 08:52:11 - epoch: 1 metrics: acc: 0.09914 loss: 2.30994320602417 val_acc: 0.1 val_loss: 2.3110838134765626 reportTime: 24/08/2020 08:54:10 - config: batch_size: 32 lr: 0.1 id: 2 modificationTime: 24/08/2020 08:50:36 reports: - epoch: 0 metrics: acc: 0.11012 loss: 2.2979678358459474 val_acc: 0.1667 val_loss: 2.268467852783203 reportTime: 24/08/2020 08:52:44 - epoch: 1 metrics: acc: 0.2047 loss: 2.0894255745697023 val_acc: 0.2833 val_loss: 1.8615504817962647 reportTime: 24/08/2020 08:54:45 Finally, you can delete the HPO Job by running: runai delete hpo1","title":"Examine the Results"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#see-also","text":"For further information on the Run:AI HPO support library see: The Run:AI HPO Support Library Sample code in Github","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","text":"Quickstart: Over-Quota and Bin Packing \u00b6 Goals \u00b6 The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:AI canonical image gcr.io/run-ai-demo/quickstart Part I: Over-quota \u00b6 Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized. Part 2: Basic Fairness via Preemption \u00b6 Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one Job, must be preempted : moved out to allow team-b to grow. Run:AI scheduler chooses to preempt Job a1 . It is important that unattended Jobs will save checkpoints . This will ensure that whenever Job a1 resume, it will do so from where it left off. Part 3: Bin Packing \u00b6 Run the following command: runai delete a2 -p team-a a1 is now going to start running again. Run: runai list jobs -A You have two Jobs that are running on the first node and one Job that is running alone the second node. Choose one of the two Job from the full node and delete it: runai delete <job-name> -p <project> The status now is: Now, run a 2 GPU Job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that Job a1 has been preempted and then restarted on the second node, in order to clear space fo the new a2 Job. This is bin-packing or consolidation","title":"Over-Quota, Basic Fairness & Bin-Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#quickstart-over-quota-and-bin-packing","text":"","title":"Quickstart: Over-Quota and Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","text":"The goal of this Quickstart is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.","title":"Goals"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:AI canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","text":"Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized.","title":"Part I: Over-quota"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","text":"Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one Job, must be preempted : moved out to allow team-b to grow. Run:AI scheduler chooses to preempt Job a1 . It is important that unattended Jobs will save checkpoints . This will ensure that whenever Job a1 resume, it will do so from where it left off.","title":"Part 2: Basic Fairness via Preemption"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","text":"Run the following command: runai delete a2 -p team-a a1 is now going to start running again. Run: runai list jobs -A You have two Jobs that are running on the first node and one Job that is running alone the second node. Choose one of the two Job from the full node and delete it: runai delete <job-name> -p <project> The status now is: Now, run a 2 GPU Job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that Job a1 has been preempted and then restarted on the second node, in order to clear space fo the new a2 Job. This is bin-packing or consolidation","title":"Part 3: Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","text":"Quickstart: Queue Fairness \u00b6 Goal \u00b6 The goal of this Quickstart is to explain fairness . The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources. This Quickstart is about queue fairness . It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:AI canonical image gcr.io/run-ai-demo/quickstart Part I: Immediate Displacement of Over-Quota \u00b6 Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b Jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects. Part 2: Queue Fairness \u00b6 Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete. runai delete b2 -p team-b Discussion As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted.","title":"Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#quickstart-queue-fairness","text":"","title":"Quickstart: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","text":"The goal of this Quickstart is to explain fairness . The over-quota Quickstart shows basic fairness where allocated GPUs per Project are adhered to such that if a Project is in over-quota, its Job will be preempted once another Project requires its resources. This Quickstart is about queue fairness . It shows that Jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in Project A has submitted 50 Jobs and soon after that, a person in Project B has submitted 25 Jobs, the Jobs in the queue will be processed fairly.","title":"Goal"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:AI canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","text":"Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b Jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the Projects.","title":"Part I: Immediate Displacement of Over-Quota"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","text":"Now lets start deleting Jobs. Alternatively, you can wait for Jobs to complete. runai delete b2 -p team-b Discussion As the quotas are equal (1 for each Project, the remaining pending Jobs will get scheduled one by one alternating between Projects, regardless of the time in which they were submitted.","title":"Part 2: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-train/","text":"Quickstart: Launch Unattended Training Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload. View training status and resource consumption using the Run:AI user interface and the Run:AI CLI. View training logs. Stop the training. Prerequisites \u00b6 To complete this Quickstart you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\". Allocate 2 GPUs to the Project. Run Workload \u00b6 At the command-line run: runai config project team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job train1 Follow up on the Job's progress by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the Job is waiting to be scheduled Running - the Job is running Succeeded - the Job has ended A full list of Job statuses can be found here To get additional status on your Job run: runai describe job train1 View Logs \u00b6 Run the following: runai logs train1 You should see a log of a running deep learning session: View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai/jobs Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization: Stop Workload \u00b6 Run the following: runai delete train1 This would stop the training workload. You can verify this by running runai list jobs again. Next Steps \u00b6 Follow the Quickstart document: Launch Interactive Workloads Use your container to run an unattended training workload","title":"Training"},{"location":"Researcher/Walkthroughs/walkthrough-train/#quickstart-launch-unattended-training-workloads","text":"","title":"Quickstart: Launch Unattended Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Quickstart you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload. View training status and resource consumption using the Run:AI user interface and the Run:AI CLI. View training logs. Stop the training.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","text":"To complete this Quickstart you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/walkthrough-train/#setup","text":"Login to the Projects area of the Run:AI Administration user interface at https://app.run.ai/projects Add a Project named \"team-a\". Allocate 2 GPUs to the Project.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","text":"At the command-line run: runai config project team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training Job for team-a with an allocation of a single GPU. The Job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the Job train1 Follow up on the Job's progress by running: runai list jobs The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the Job is waiting to be scheduled Running - the Job is running Succeeded - the Job has ended A full list of Job statuses can be found here To get additional status on your Job run: runai describe job train1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","text":"Run the following: runai logs train1 You should see a log of a running deep learning session:","title":"View Logs"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai/jobs Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning Job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","text":"Run the following: runai delete train1 This would stop the training workload. You can verify this by running runai list jobs again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","text":"Follow the Quickstart document: Launch Interactive Workloads Use your container to run an unattended training workload","title":"Next Steps"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/","text":"Best Practice: From Bare Metal to Docker Images \u00b6 Introduction \u00b6 Some Researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More Researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task. Why Use Docker Images? \u00b6 Docker images and 'containerization' in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of 'setting up an environment'. The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image. Moving a Data Science Environment to Docker \u00b6 A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run Training data \u00b6 Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine. Machine Learning Code and Inputs \u00b6 As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system. Code Dependencies \u00b6 Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically. ML Lifecycle: Build and Train \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running. Build Workloads \u00b6 With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service). Training Workloads \u00b6 For training workloads you can use a well-known image (e.g. the nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command-line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:AI see Converting your Workload to use Unattended Training Execution .","title":"Bare-Metal to Docker Images"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#best-practice-from-bare-metal-to-docker-images","text":"","title":"Best Practice: From Bare Metal to Docker Images"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#introduction","text":"Some Researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More Researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task.","title":"Introduction"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#why-use-docker-images","text":"Docker images and 'containerization' in general provide a level of abstraction which, by large, frees developers and Researchers from the mundane tasks of 'setting up an environment'. The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image.","title":"Why Use Docker Images?"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#moving-a-data-science-environment-to-docker","text":"A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run","title":"Moving a Data Science Environment to Docker"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#training-data","text":"Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the Researcher is currently using, allowing the Researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine.","title":"Training data"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#machine-learning-code-and-inputs","text":"As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.","title":"Machine Learning Code and Inputs"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#code-dependencies","text":"Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically.","title":"Code Dependencies"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#ml-lifecycle-build-and-train","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running.","title":"ML Lifecycle: Build and Train"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#build-workloads","text":"With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).","title":"Build Workloads"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#training-workloads","text":"For training workloads you can use a well-known image (e.g. the nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command-line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training Job. For further information on how to set up and parameterize a training workload via docker or Run:AI see Converting your Workload to use Unattended Training Execution .","title":"Training Workloads"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/","text":"Best Practice: Save Deep-Learning Checkpoints \u00b6 Introduction \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs). How to Save Checkpoints \u00b6 TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). This document uses Keras as an example. The code itself can be found here Where to Save Checkpoints \u00b6 It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example: runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh The command saves the checkpoints in an NFS checkpoints folder /mnt/nfs_share/john When to Save Checkpoints \u00b6 Save Periodically \u00b6 It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows: checkpoints_file = \"weights.best.hdf5\" checkpoint = ModelCheckpoint ( checkpoints_file , monitor = 'val_acc' , verbose = 1 , save_best_only = True , mode = 'max' ) Save on Exit Signal \u00b6 If periodic checkpoints are not enough, you can use a signal-hook provided by Run:AI (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the Job to return later. exit ( 1 ) signal . signal ( signal . SIGTERM , graceful_exit_handler ) By default, you will have 30 seconds to save your checkpoints. Important For the signal to be captured, it must be propagated from the startup script to the python child process. See code here Resuming using Saved Checkpoints \u00b6 A Run:AI unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist (see above) If saved checkpoints exist, load them and start the run using these checkpoints import os checkpoints_file = \"weights.best.hdf5\" if os . path . isfile ( checkpoints_file ): print ( \"loading checkpoint file: \" + checkpoints_file ) model . load_weights ( checkpoints_file )","title":"Save Deep Learning Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#best-practice-save-deep-learning-checkpoints","text":"","title":"Best Practice: Save Deep-Learning Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#introduction","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).","title":"Introduction"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#how-to-save-checkpoints","text":"TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). This document uses Keras as an example. The code itself can be found here","title":"How to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#where-to-save-checkpoints","text":"It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. Example: runai submit train-with-checkpoints -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh The command saves the checkpoints in an NFS checkpoints folder /mnt/nfs_share/john","title":"Where to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#when-to-save-checkpoints","text":"","title":"When to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#save-periodically","text":"It is a best practice to save checkpoints at intervals. For example, every epoch as the Keras code below shows: checkpoints_file = \"weights.best.hdf5\" checkpoint = ModelCheckpoint ( checkpoints_file , monitor = 'val_acc' , verbose = 1 , save_best_only = True , mode = 'max' )","title":"Save Periodically"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#save-on-exit-signal","text":"If periodic checkpoints are not enough, you can use a signal-hook provided by Run:AI (via Kubernetes). The hook is python code that is called before your Job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the Job to return later. exit ( 1 ) signal . signal ( signal . SIGTERM , graceful_exit_handler ) By default, you will have 30 seconds to save your checkpoints. Important For the signal to be captured, it must be propagated from the startup script to the python child process. See code here","title":"Save on Exit Signal"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#resuming-using-saved-checkpoints","text":"A Run:AI unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist (see above) If saved checkpoints exist, load them and start the run using these checkpoints import os checkpoints_file = \"weights.best.hdf5\" if os . path . isfile ( checkpoints_file ): print ( \"loading checkpoint file: \" + checkpoints_file ) model . load_weights ( checkpoints_file )","title":"Resuming using Saved Checkpoints"},{"location":"Researcher/best-practices/convert-to-unattended/","text":"Best Practice: Convert your Workload to Run Unattended \u00b6 Motivation \u00b6 Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyperparameter optimization runs. Best Practices \u00b6 Docker Image \u00b6 A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow . Others, use generic images as the base image to a more customized image using Dockerfiles . Realizing that Researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the Researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice: Create a Startup Script \u00b6 All the commands you run inside the interactive Job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the Researcher to add/remove libraries without needing changes to the image itself. Support Variance Between Different Runs \u00b6 Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case. Checkpoints \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: Saving Deep Learning Checkpoints . Running the Job \u00b6 Using runai submit , drop the flag --interactive . For submitting a Job using the script created above, please use --command flag to specify a command, use the -- syntax to pass arguments, and pass environment variables using the flag --environment . Example with Environment variables: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -e 'EPOCHS=30' -e 'LEARNING_RATE=0.02' --command -- ./startup.sh Example with Command-line arguments: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ --command -- ./startup.sh batch-size=64 number-of-epochs=3 Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:AI CLI. Use CLI Templates \u00b6 Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to template those configurations and use pre-configured configuration when submitting a Job. Please refer to Configure Command-Line Interface Templates . Attached Files \u00b6 The 3 relevant files mentioned in this document can be downloaded from Github See Also \u00b6 See the unattended training Quickstart: Launch Unattended Training Workloads","title":"Convert a Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practice-convert-your-workload-to-run-unattended","text":"","title":"Best Practice: Convert your Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","text":"Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires Researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyperparameter optimization runs.","title":"Motivation"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","text":"","title":"Best Practices"},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","text":"A docker container is based on a docker image. Some Researchers use generic images such as ones provided by Nvidia, for example: NVIDIA NGC TensorFlow . Others, use generic images as the base image to a more customized image using Dockerfiles . Realizing that Researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the Researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice:","title":"Docker Image"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","text":"All the commands you run inside the interactive Job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the Researcher to add/remove libraries without needing changes to the image itself.","title":"Create a Startup Script"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","text":"Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case.","title":"Support Variance Between Different Runs"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: Saving Deep Learning Checkpoints .","title":"Checkpoints"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","text":"Using runai submit , drop the flag --interactive . For submitting a Job using the script created above, please use --command flag to specify a command, use the -- syntax to pass arguments, and pass environment variables using the flag --environment . Example with Environment variables: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ -e 'EPOCHS=30' -e 'LEARNING_RATE=0.02' --command -- ./startup.sh Example with Command-line arguments: runai submit train1 -i tensorflow/tensorflow:1.14.0-gpu-py3 -v /nfs/john:/mydir -g 1 --working-dir /mydir/ --command -- ./startup.sh batch-size=64 number-of-epochs=3 Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:AI CLI.","title":"Running the Job"},{"location":"Researcher/best-practices/convert-to-unattended/#use-cli-templates","text":"Different run configurations may vary significantly and can be tedious to be written each time on the command-line. To make life easier, our CLI offers a way to template those configurations and use pre-configured configuration when submitting a Job. Please refer to Configure Command-Line Interface Templates .","title":"Use CLI Templates"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","text":"The 3 relevant files mentioned in this document can be downloaded from Github","title":"Attached Files"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","text":"See the unattended training Quickstart: Launch Unattended Training Workloads","title":"See Also"},{"location":"Researcher/best-practices/env-variables/","text":"Best Practice: Identifying your Job from within the Container \u00b6 Motivation \u00b6 There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name. Run:AI provides environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again. Identifying a Job \u00b6 Run:AI provides the following environment variables: JOB_NAME - the name of the Job. JOB_UUID - a unique identifier for the Job. Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same. Identifying a Pod \u00b6 With Hyperparameter Optimization , experiments are run as Pods within the Job. Run:AI provides the following environment variables to identify the Pod. POD_INDEX - An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. POD_UUID - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change. Usage Example in Python \u00b6 import os jobName = os . environ [ 'JOB_NAME' ] jobUUID = os . environ [ 'JOB_UUID' ]","title":"Identify a Job from within Container"},{"location":"Researcher/best-practices/env-variables/#best-practice-identifying-your-job-from-within-the-container","text":"","title":"Best Practice: Identifying your Job from within the Container"},{"location":"Researcher/best-practices/env-variables/#motivation","text":"There may be use cases where your container may need to uniquely identify the Job it is currently running in. A typical use case is for saving Job artifacts under a unique name. Run:AI provides environment variables you can use. These variables are guaranteed to be unique even if the Job is preempted or evicted and then runs again.","title":"Motivation"},{"location":"Researcher/best-practices/env-variables/#identifying-a-job","text":"Run:AI provides the following environment variables: JOB_NAME - the name of the Job. JOB_UUID - a unique identifier for the Job. Note that the Job can be deleted and then recreated with the same name. A Job UUID will be different even if the Job names are the same.","title":"Identifying a Job"},{"location":"Researcher/best-practices/env-variables/#identifying-a-pod","text":"With Hyperparameter Optimization , experiments are run as Pods within the Job. Run:AI provides the following environment variables to identify the Pod. POD_INDEX - An index number (0, 1, 2, 3....) for a specific Pod within the Job. This is useful for Hyperparameter Optimization to allow easy mapping to individual experiments. The Pod index will remain the same if restarted (due to a failure or preemption). Therefore, it can be used by the Researcher to identify experiments. POD_UUID - a unique identifier for the Pod. if the Pod is restarted, the Pod UUID will change.","title":"Identifying a Pod"},{"location":"Researcher/best-practices/env-variables/#usage-example-in-python","text":"import os jobName = os . environ [ 'JOB_NAME' ] jobUUID = os . environ [ 'JOB_UUID' ]","title":"Usage Example in Python"},{"location":"Researcher/cli-reference/Introduction/","text":"The Run:AI Command-line Interface (CLI) is one of the ways for a Researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. To install and configure the Run:AI CLI see Researcher Setup - Start Here","title":"Introduction"},{"location":"Researcher/cli-reference/runai-attach/","text":"Description \u00b6 Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set. Synopsis \u00b6 runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --no-stdin Do not attach STDIN. --no-tty Do not allocate a pseudo-TTY --pod string Attach to a specific pod within the Job. To find the list of pods run runai describe job <job-name> and then use the pod name with the --pod flag. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. Output \u00b6 None","title":"runai attach"},{"location":"Researcher/cli-reference/runai-attach/#description","text":"Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the Job has multiple pods the job will attach to the first pod unless otherwise set.","title":"Description"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","text":"runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-attach/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. --no-stdin Do not attach STDIN. --no-tty Do not allocate a pseudo-TTY --pod string Attach to a specific pod within the Job. To find the list of pods run runai describe job <job-name> and then use the pod name with the --pod flag.","title":"Options"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-attach/#output","text":"None","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/","text":"Description \u00b6 Get a bash session inside a running Job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command. Synopsis \u00b6 runai bash <job-name> [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --pod string Specify a pod of a running Job. To get a list of the pods of a specific Job, run runai describe job <job-name> command Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text Output \u00b6 The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in a running state yet. See also \u00b6 Build Workloads. See Quickstart document: Launch Interactive Build Workloads .","title":"runai bash"},{"location":"Researcher/cli-reference/runai-bash/#description","text":"Get a bash session inside a running Job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command.","title":"Description"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","text":"runai bash <job-name> [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-bash/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. --pod string Specify a pod of a running Job. To get a list of the pods of a specific Job, run runai describe job <job-name> command","title":"Options"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-bash/#output","text":"The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in a running state yet.","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/#see-also","text":"Build Workloads. See Quickstart document: Launch Interactive Build Workloads .","title":"See also"},{"location":"Researcher/cli-reference/runai-cluster/","text":"Attention runai cluster list has been replaced by runai list clusters . runai cluster set has been replaced by runai config cluster .","title":"runai cluster"},{"location":"Researcher/cli-reference/runai-config/","text":"Description \u00b6 Set a default Project or Cluster Synopsis \u00b6 runai config project <project-name> [ --loglevel value ] [ --help | -h ] runai config cluster <project-name> [ --loglevel value ] [ --help | -h ] Options \u00b6 <project-name> - The name of the Project you want to set as default. Mandatory. <cluster-name> - The name of the Cluster you want to set as the current Cluster. Mandatory. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. Output \u00b6 None","title":"runai config"},{"location":"Researcher/cli-reference/runai-config/#description","text":"Set a default Project or Cluster","title":"Description"},{"location":"Researcher/cli-reference/runai-config/#synopsis","text":"runai config project <project-name> [ --loglevel value ] [ --help | -h ] runai config cluster <project-name> [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-config/#options","text":"<project-name> - The name of the Project you want to set as default. Mandatory. <cluster-name> - The name of the Cluster you want to set as the current Cluster. Mandatory.","title":"Options"},{"location":"Researcher/cli-reference/runai-config/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-config/#output","text":"None","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/","text":"Description \u00b6 Delete a Job and its associated Pods. Note that once you delete a Job, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able access logs. Any data saved on the container and not stored on a shared location will be lost. Synopsis \u00b6 runai delete <job-name> [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 The Job will be deleted and not available via the command runai list jobs . The Job will not be deleted from the Run:AI user interface Job list. See Also \u00b6 Build Workloads. See Quickstart document: Launch Interactive Build Workloads . Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"runai delete"},{"location":"Researcher/cli-reference/runai-delete/#description","text":"Delete a Job and its associated Pods. Note that once you delete a Job, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able access logs. Any data saved on the container and not stored on a shared location will be lost.","title":"Description"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","text":"runai delete <job-name> [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-delete/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text.","title":"Options"},{"location":"Researcher/cli-reference/runai-delete/#output","text":"The Job will be deleted and not available via the command runai list jobs . The Job will not be deleted from the Run:AI user interface Job list.","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/#see-also","text":"Build Workloads. See Quickstart document: Launch Interactive Build Workloads . Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-describe/","text":"Attention runai get has been replaced by runai describe job . Description \u00b6 Display details of a Job or Node. Synopsis \u00b6 runai describe job <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] [ --output string | -o string ] runai describe node [ node-name ] [ --loglevel value ] [ --help | -h ] runai describe template <template-name> [ --loglevel value ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. <node-name> - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown. <template-name> - The name of the Template to run the command on. Mandatory. Templates are a way to reduce the number of flags required when running the command runai submit and set various defaults. Templates are added by the Administrator. -o | --output Output format. One of: json|yaml|wide. Default is 'wide' Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: runai config project <project-name> . --help | -h Show help text Output \u00b6 The runai describe job command will show Job properties and status as well as lifecycle events and the list of related pods. The runai describe node command will show Node properties. The runai describe template command will show Template properties. See Also \u00b6 See: Configure Command-Line Interface Templates on how to configure Templates.","title":"runai describe"},{"location":"Researcher/cli-reference/runai-describe/#description","text":"Display details of a Job or Node.","title":"Description"},{"location":"Researcher/cli-reference/runai-describe/#synopsis","text":"runai describe job <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] [ --output string | -o string ] runai describe node [ node-name ] [ --loglevel value ] [ --help | -h ] runai describe template <template-name> [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-describe/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. <node-name> - The name of the Node to run the command with. If a Node name is not specified, a description of all Nodes is shown. <template-name> - The name of the Template to run the command on. Mandatory. Templates are a way to reduce the number of flags required when running the command runai submit and set various defaults. Templates are added by the Administrator. -o | --output Output format. One of: json|yaml|wide. Default is 'wide'","title":"Options"},{"location":"Researcher/cli-reference/runai-describe/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project, use: runai config project <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-describe/#output","text":"The runai describe job command will show Job properties and status as well as lifecycle events and the list of related pods. The runai describe node command will show Node properties. The runai describe template command will show Template properties.","title":"Output"},{"location":"Researcher/cli-reference/runai-describe/#see-also","text":"See: Configure Command-Line Interface Templates on how to configure Templates.","title":"See Also"},{"location":"Researcher/cli-reference/runai-exec/","text":"Description \u00b6 Execute a command inside a running Job Note: to execute a bash command, you can also use the shorthand runai bash Synopsis \u00b6 runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. <command> the command itself (e.g. bash ). --stdin | -i Keep STDIN open even if not attached. --tty | -t Allocate a pseudo-TTY. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 The command will run in the context of the container. See Also \u00b6","title":"runai exec"},{"location":"Researcher/cli-reference/runai-exec/#description","text":"Execute a command inside a running Job Note: to execute a bash command, you can also use the shorthand runai bash","title":"Description"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","text":"runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-exec/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. <command> the command itself (e.g. bash ). --stdin | -i Keep STDIN open even if not attached. --tty | -t Allocate a pseudo-TTY.","title":"Options"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-exec/#output","text":"The command will run in the context of the container.","title":"Output"},{"location":"Researcher/cli-reference/runai-exec/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-list/","text":"Attention runai list has been replaced by runai list jobs . runai clusters list has been replaced by runai list clusters . runai project list has been replaced by runai list projects . runai template list has been replaced by runai list templates . Description \u00b6 Show lists of Jobs, Projects, Clusters or Nodes. Synopsis \u00b6 runai list jobs [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] runai list projects [ --loglevel value ] [ --help | -h ] runai list clusters [ --loglevel value ] [ --help | -h ] runai list nodes [ node-name ] [ --loglevel value ] [ --help | -h ] runai list templates [ --loglevel value ] [ --help | -h ] Options \u00b6 node-name - Name of a specific node to list (optional). --all-projects | -A Show Jobs from all Projects. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 A list of Jobs, Nodes, Projects, Clusters, or Templates. To filter 'runai list nodes' for a specific Node, add the Node name. See Also \u00b6 To show details for a specific Job, Node or Template see runai describe .","title":"runai list"},{"location":"Researcher/cli-reference/runai-list/#description","text":"Show lists of Jobs, Projects, Clusters or Nodes.","title":"Description"},{"location":"Researcher/cli-reference/runai-list/#synopsis","text":"runai list jobs [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] runai list projects [ --loglevel value ] [ --help | -h ] runai list clusters [ --loglevel value ] [ --help | -h ] runai list nodes [ node-name ] [ --loglevel value ] [ --help | -h ] runai list templates [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-list/#options","text":"node-name - Name of a specific node to list (optional). --all-projects | -A Show Jobs from all Projects.","title":"Options"},{"location":"Researcher/cli-reference/runai-list/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-list/#output","text":"A list of Jobs, Nodes, Projects, Clusters, or Templates. To filter 'runai list nodes' for a specific Node, add the Node name.","title":"Output"},{"location":"Researcher/cli-reference/runai-list/#see-also","text":"To show details for a specific Job, Node or Template see runai describe .","title":"See Also"},{"location":"Researcher/cli-reference/runai-logs/","text":"Description \u00b6 Show the logs of a Job. Synopsis \u00b6 runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> - The name of the Job to run the command with. Mandatory. --follow | -f Stream the logs. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai describe job <job-name> . --instance (string) | -i (string) Show logs for a specific instance in cases where a Job contains multiple pods. --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together. --since-time (date-time) Return logs after specified date. Date format should be RFC3339 , example: 2020-01-26T15:00:00Z . --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything. See Also \u00b6 Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"runai logs"},{"location":"Researcher/cli-reference/runai-logs/#description","text":"Show the logs of a Job.","title":"Description"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","text":"runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-logs/#options","text":"<job-name> - The name of the Job to run the command with. Mandatory. --follow | -f Stream the logs. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai describe job <job-name> . --instance (string) | -i (string) Show logs for a specific instance in cases where a Job contains multiple pods. --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together. --since-time (date-time) Return logs after specified date. Date format should be RFC3339 , example: 2020-01-26T15:00:00Z . --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output.","title":"Options"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai config project <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-logs/#output","text":"The command will show the logs of the first process in the container. For training Jobs, this would be the command run at startup. For interactive Jobs, the command may not show anything.","title":"Output"},{"location":"Researcher/cli-reference/runai-logs/#see-also","text":"Training Workloads. See Quickstart document: Launch Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-project/","text":"Attention runai project list has been replaced by runai list projects . runai project set has been replaced by runai config project .","title":"runai project"},{"location":"Researcher/cli-reference/runai-submit-mpi/","text":"Description \u00b6 Submit a Distributed Training (MPI) Run:AI Job for execution. Synopsis \u00b6 runai submit-mpi [ --always-pull-image ] [ --attach ] [ --backoffLimit int ] [ --command ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --name string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --stdin ] [ --template string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image: runai submit-mpi --name dist1 --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed (see: distributed training Quickstart ). Options \u00b6 Aliases and Shortcuts \u00b6 --name The name of the Job. --interactive Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --template string Provide the name of a template. A template can provide default and mandatory values. The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:AI will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix. Container Related \u00b6 --always-pull-image stringArray (deprecated) Deprecated. Please use image-pull-policy=always instead. When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a penalty of performance degradation on Job start. --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command If set, overrides the image's entry point with the command supplied after '--' Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ). --git-sync string Clone a git repository into the container running the Job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source=REPOSITORY is the only mandatory field --image string | -i string Image to use when creating the container for this Job --image-pull-policy string Pulling policy of the image When starting a container. Options are: always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. ifNotPresent : the image is pulled only if it is not already present locally. never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY --working-dir string Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double Limitations on the number of CPU consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocated for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --gpu-memory GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. Network \u00b6 --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference documentation. --host-network Use the host's network stack inside the container. For further information see docker run reference documentation. Job Lifecycle \u00b6 --backoffLimit int The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --processes int Number of distributed training processes. The default is 1. Access Control \u00b6 --create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. For more information see non root containers . --prevent-privilege-escalation Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. For more information see non root containers . Scheduling \u00b6 --node-type string Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects . Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) Specify the Project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 The command will attempt to submit an mpi Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . See Also \u00b6 See Quickstart document Running Distributed Training .","title":"runai submit-mpi"},{"location":"Researcher/cli-reference/runai-submit-mpi/#description","text":"Submit a Distributed Training (MPI) Run:AI Job for execution.","title":"Description"},{"location":"Researcher/cli-reference/runai-submit-mpi/#synopsis","text":"runai submit-mpi [ --always-pull-image ] [ --attach ] [ --backoffLimit int ] [ --command ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --name string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --stdin ] [ --template string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Options with a value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit-mpi/#examples","text":"start an unattended mpi training Job of name dist1, based on Project team-a using a quickstart-distributed image: runai submit-mpi --name dist1 --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed (see: distributed training Quickstart ).","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit-mpi/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-submit-mpi/#aliases-and-shortcuts","text":"--name The name of the Job. --interactive Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --template string Provide the name of a template. A template can provide default and mandatory values. The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:AI will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix.","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit-mpi/#container-related","text":"--always-pull-image stringArray (deprecated) Deprecated. Please use image-pull-policy=always instead. When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a penalty of performance degradation on Job start. --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command If set, overrides the image's entry point with the command supplied after '--' Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ). --git-sync string Clone a git repository into the container running the Job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source=REPOSITORY is the only mandatory field --image string | -i string Image to use when creating the container for this Job --image-pull-policy string Pulling policy of the image When starting a container. Options are: always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. ifNotPresent : the image is pulled only if it is not already present locally. never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY --working-dir string Starts the container with the specified directory as the current directory.","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit-mpi/#resource-allocation","text":"--cpu double CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double Limitations on the number of CPU consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocated for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --gpu-memory GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit-mpi/#storage","text":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively.","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit-mpi/#network","text":"--host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference documentation. --host-network Use the host's network stack inside the container. For further information see docker run reference documentation.","title":"Network"},{"location":"Researcher/cli-reference/runai-submit-mpi/#job-lifecycle","text":"--backoffLimit int The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --processes int Number of distributed training processes. The default is 1.","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit-mpi/#access-control","text":"--create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. For more information see non root containers . --prevent-privilege-escalation Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. For more information see non root containers .","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit-mpi/#scheduling","text":"--node-type string Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects .","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit-mpi/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\") --project | -p (string) Specify the Project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit-mpi/#output","text":"The command will attempt to submit an mpi Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> .","title":"Output"},{"location":"Researcher/cli-reference/runai-submit-mpi/#see-also","text":"See Quickstart document Running Distributed Training .","title":"See Also"},{"location":"Researcher/cli-reference/runai-submit/","text":"Description \u00b6 Submit a Run:AI Job for execution. Synopsis \u00b6 runai submit [ --always-pull-image ] [ --attach ] [ --backoffLimit int ] [ --command ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --elastic ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --imagePullPolicy string ] [ --interactive ] [ --jupyter ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --name string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --service-type string | -s string ] [ --stdin ] [ --template string ] [ --ttl-after-finish duration ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 All examples assume a Run:AI Project has been set using runai config project <project-name> . Start an interactive Job: runai submit --name build1 -i ubuntu -g 1 --interactive --command -- sleep infinity (see: build Quickstart ). Externalize ports: runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --service-type=nodeport --port 30022:22 --command -- /usr/sbin/sshd -D (see: build with ports Quickstart ). Start a Training Job runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training Quickstart ). Use GPU Fractions runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions Quickstart ). Hyperparameter Optimization runai submit --name hpo1 -i gcr.io/run-ai-demo/walkthrough-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization Quickstart ). Submit a Job without a name (automatically generates a name) runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Submit a Job without a name with a pre-defined prefix and an incremental index suffix runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1 Options \u00b6 Aliases and Shortcuts \u00b6 --name The name of the Job. --interactive Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --jupyter Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit --name jup1 --jupyter -g 0.5 --service-type=ingress will start an interactive session named jup1 and use an ingress load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list jobs to find the URL for the notebook. --template string Provide the name of a template. A template can provide default and mandatory values. --job-name-prefix The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:AI will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix Container Related \u00b6 --always-pull-image stringArray (deprecated) Deprecated. Please use image-pull-policy=always instead. When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a penalty of performance degradation on Job start. --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command If set, overrides the image's entry point with the command supplied after '--' Example: --command -- python script.py 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ) --git-sync string Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source=REPOSITORY is the only mandatory field --image string | -i string Image to use when creating the container for this Job --image-pull-policy string Pulling policy of the image When starting a container. Options are: always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. ifNotPresent : the image is pulled only if it is not already present locally. never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY. --working-dir string Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double Limitations on the number of CPU consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocated for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --gpu-memory GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. Network \u00b6 --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference . --host-network Use the host's network stack inside the container. For further information see docker run reference . --port stringArray Expose ports from the Job container. Used together with --service-type . Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --service-type string | -s string Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport , ingress. Use the command runai list to obtain the endpoint to use the service when the Job is running. Different service methods have different endpoint structure. Job Lifecycle \u00b6 --backoffLimit int The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --completions int The number of successful pods required for this Job to be completed. Used for Hyperparameter optimization . Use together with --parallelism . --parallelism int The number of pods this Job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions . --ttl-after-finish duration Define the duration, post Job finish, after which the Job is automatically deleted (5s, 2m, 3h, etc). Note: This setting must first be enabled at the cluster level. See Automatically Delete Jobs After Job Finish . Access Control \u00b6 --create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. For more information see non root containers . --prevent-privilege-escalation Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. For more information see [non root containers(../../Administrator/Cluster-Setup/non-root-containers.md). Scheduling \u00b6 --elastic Mark the Job as elastic. For further information on Elasticity see Elasticity Dynamically Stretch Compress Jobs According to GPU Availability . --node-type string Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects . --preemptible Mark an interactive Job as preemptible. Preemptible Jobs can be scheduled above guaranteed quota but may be reclaimed at any time. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text. Output \u00b6 The command will attempt to submit a Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . Note that the submit call may use templates to provide defaults to any of the above flags. See Also \u00b6 See any of the Quickstart documents here: . See template configuration for a description on how templates work.","title":"runai submit"},{"location":"Researcher/cli-reference/runai-submit/#description","text":"Submit a Run:AI Job for execution.","title":"Description"},{"location":"Researcher/cli-reference/runai-submit/#synopsis","text":"runai submit [ --always-pull-image ] [ --attach ] [ --backoffLimit int ] [ --command ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --elastic ] [ --environment stringArray | -e stringArray ] [ --git-sync string ] [ --gpu double | -g double ] [ --gpu-memory string ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --imagePullPolicy string ] [ --interactive ] [ --jupyter ] [ --job-name-prefix string ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --name string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --service-type string | -s string ] [ --stdin ] [ --template string ] [ --ttl-after-finish duration ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] -- [ COMMAND ] [ ARGS... ] [ options ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit/#examples","text":"All examples assume a Run:AI Project has been set using runai config project <project-name> . Start an interactive Job: runai submit --name build1 -i ubuntu -g 1 --interactive --command -- sleep infinity (see: build Quickstart ). Externalize ports: runai submit --name build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --service-type=nodeport --port 30022:22 --command -- /usr/sbin/sshd -D (see: build with ports Quickstart ). Start a Training Job runai submit --name train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training Quickstart ). Use GPU Fractions runai submit --name frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions Quickstart ). Hyperparameter Optimization runai submit --name hpo1 -i gcr.io/run-ai-demo/walkthrough-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization Quickstart ). Submit a Job without a name (automatically generates a name) runai submit -i gcr.io/run-ai-demo/quickstart -g 1 Submit a Job without a name with a pre-defined prefix and an incremental index suffix runai submit --job-name-prefix -i gcr.io/run-ai-demo/quickstart -g 1","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-submit/#aliases-and-shortcuts","text":"--name The name of the Job. --interactive Mark this Job as Interactive. Interactive Jobs are not terminated automatically by the system. --jupyter Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit --name jup1 --jupyter -g 0.5 --service-type=ingress will start an interactive session named jup1 and use an ingress load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list jobs to find the URL for the notebook. --template string Provide the name of a template. A template can provide default and mandatory values. --job-name-prefix The prefix to use to automatically generate a Job name with an incremental index. When a Job name is omitted Run:AI will generate a Job name. The optional --job-name-prefix flag creates Job names with the provided prefix","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit/#container-related","text":"--always-pull-image stringArray (deprecated) Deprecated. Please use image-pull-policy=always instead. When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a penalty of performance degradation on Job start. --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command If set, overrides the image's entry point with the command supplied after '--' Example: --command -- python script.py 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ) --git-sync string Clone a git repository into the container running the job. The parameter should follow the syntax: source=REPOSITORY,branch=BRANCH_NAME,rev=REVISION,username=USERNAME,password=PASSWORD,target=TARGET_DIRECTORY_TO_CLONE . Note that source=REPOSITORY is the only mandatory field --image string | -i string Image to use when creating the container for this Job --image-pull-policy string Pulling policy of the image When starting a container. Options are: always (default): force image pulling to check whether local image already exists. If the image already exists locally and has the same digest, then the image will not be downloaded. ifNotPresent : the image is pulled only if it is not already present locally. never : the image is assumed to exist locally. No attempt is made to pull the image. For more information see Kubernetes documentation . --local-image (deprecated) Deprecated. Please use image-pull-policy=never instead. Use a local image for this Job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY. --working-dir string Starts the container with the specified directory as the current directory.","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","text":"--cpu double CPU units to allocate for the Job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the Job. --cpu-limit double Limitations on the number of CPU consumed by the Job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocated for the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --gpu-memory GPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of GPU memory to the Job. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this Job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the Job. --memory-limit string CPU memory to allocate for this Job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit/#storage","text":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively.","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit/#network","text":"--host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference . --host-network Use the host's network stack inside the container. For further information see docker run reference . --port stringArray Expose ports from the Job container. Used together with --service-type . Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --service-type string | -s string Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport , ingress. Use the command runai list to obtain the endpoint to use the service when the Job is running. Different service methods have different endpoint structure.","title":"Network"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","text":"--backoffLimit int The number of times the Job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --completions int The number of successful pods required for this Job to be completed. Used for Hyperparameter optimization . Use together with --parallelism . --parallelism int The number of pods this Job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions . --ttl-after-finish duration Define the duration, post Job finish, after which the Job is automatically deleted (5s, 2m, 3h, etc). Note: This setting must first be enabled at the cluster level. See Automatically Delete Jobs After Job Finish .","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit/#access-control","text":"--create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. For more information see non root containers . --prevent-privilege-escalation Prevent the Job\u2019s container and all launched processes from gaining additional privileges after the Job starts. Default is false . For more information see non root containers . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a Job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. For more information see [non root containers(../../Administrator/Cluster-Setup/non-root-containers.md).","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit/#scheduling","text":"--elastic Mark the Job as elastic. For further information on Elasticity see Elasticity Dynamically Stretch Compress Jobs According to GPU Availability . --node-type string Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your Administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the Project. For more information see: Working with Projects . --preemptible Mark an interactive Job as preemptible. Preemptible Jobs can be scheduled above guaranteed quota but may be reclaimed at any time.","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --project | -p (string) Specify the Project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default Project. To change the default Project use runai config project <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit/#output","text":"The command will attempt to submit a Job. You can follow up on the Job by running runai list jobs or runai describe job <job-name> . Note that the submit call may use templates to provide defaults to any of the above flags.","title":"Output"},{"location":"Researcher/cli-reference/runai-submit/#see-also","text":"See any of the Quickstart documents here: . See template configuration for a description on how templates work.","title":"See Also"},{"location":"Researcher/cli-reference/runai-template/","text":"Attention runai template list has been replaced by runai list templates . runai template get has been replaced by runai describe template .","title":"runai template"},{"location":"Researcher/cli-reference/runai-top-node/","text":"Description \u00b6 Show list of Nodes (machines), their capacity and utilization. Synopsis \u00b6 runai top node [--help | -h] [--details | -d] Options \u00b6 Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. --details | -d Show additional details. Output \u00b6 Shows a list of Nodes their capacity and utilization. See Also \u00b6","title":"runai top node"},{"location":"Researcher/cli-reference/runai-top-node/#description","text":"Show list of Nodes (machines), their capacity and utilization.","title":"Description"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","text":"runai top node [--help | -h] [--details | -d]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-top-node/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. --details | -d Show additional details.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-top-node/#output","text":"Shows a list of Nodes their capacity and utilization.","title":"Output"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-update/","text":"Description \u00b6 Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions. sudo runai update Synopsis \u00b6 runai update [ --loglevel value ] [ --help | -h ] Options \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. Output \u00b6 Update of the Run:AI command-line interface. See Also \u00b6","title":"runai update"},{"location":"Researcher/cli-reference/runai-update/#description","text":"Find and install the latest version of the runai command-line utility. The command must be run with sudo permissions. sudo runai update","title":"Description"},{"location":"Researcher/cli-reference/runai-update/#synopsis","text":"runai update [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-update/#options","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text.","title":"Options"},{"location":"Researcher/cli-reference/runai-update/#output","text":"Update of the Run:AI command-line interface.","title":"Output"},{"location":"Researcher/cli-reference/runai-update/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-version/","text":"Description \u00b6 Show the version of this utility. Synopsis \u00b6 runai version [ --loglevel value ] [ --help | -h ] Options \u00b6 --loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text. Output \u00b6 The version of the Run:AI command-line interface. See Also \u00b6","title":"runai version"},{"location":"Researcher/cli-reference/runai-version/#description","text":"Show the version of this utility.","title":"Description"},{"location":"Researcher/cli-reference/runai-version/#synopsis","text":"runai version [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-version/#options","text":"--loglevel (string) Set the logging level. One of: debug | info | warn | error (default \"info\"). --help | -h Show help text.","title":"Options"},{"location":"Researcher/cli-reference/runai-version/#output","text":"The version of the Run:AI command-line interface.","title":"Output"},{"location":"Researcher/cli-reference/runai-version/#see-also","text":"","title":"See Also"},{"location":"Researcher/researcher-library/researcher-library-overview/","text":"Overview: The Run:AI Researcher Library \u00b6 Introduction \u00b6 Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability. Support for experiment management when performing hyperparameter optimization The library is open-source and can be reviewed here . Installing the Run:AI Researcher Library \u00b6 In your command-line run: pip install runai Run:AI Researcher Library Modules \u00b6 To review details on the specific Run:AI Researcher Library modules see: Reporting via the Run:AI Researcher Library Elasticity, Dynamically Stretch/Compress Jobs According to GPU Availability Hyperparameter optimization support","title":"Overview"},{"location":"Researcher/researcher-library/researcher-library-overview/#overview-the-runai-researcher-library","text":"","title":"Overview: The Run:AI Researcher Library"},{"location":"Researcher/researcher-library/researcher-library-overview/#introduction","text":"Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability. Support for experiment management when performing hyperparameter optimization The library is open-source and can be reviewed here .","title":"Introduction"},{"location":"Researcher/researcher-library/researcher-library-overview/#installing-the-runai-researcher-library","text":"In your command-line run: pip install runai","title":"Installing the Run:AI Researcher Library"},{"location":"Researcher/researcher-library/researcher-library-overview/#runai-researcher-library-modules","text":"To review details on the specific Run:AI Researcher Library modules see: Reporting via the Run:AI Researcher Library Elasticity, Dynamically Stretch/Compress Jobs According to GPU Availability Hyperparameter optimization support","title":"Run:AI Researcher Library Modules"},{"location":"Researcher/researcher-library/rl-elasticity/","text":"Researcher Library: Dynamically Stretch or Compress Workload's GPU Allocation \u00b6 Introduction \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability. Expanding a Workload \u00b6 Expanding a training Job allows your workload to run on more GPUs than the Researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing workloads to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs. Shrinking a Workload \u00b6 Shrinking a training Job allows your workload to run on a smaller number of GPUs than the Researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a Researcher to run a workload, albeit slower than intended, and let it automatically expand when GPUs become available at a later time. Shrinking a training Job uses an algorithm called Gradient Accumulation . For more information about the algorithm see: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa Installation \u00b6 Install the Run:AI Python library using the following command: pip install runai Code \u00b6 In your python code, if using Keras, add: import runai.elastic.keras If using PyTorch, add: import runai.elastic.torch Initizalization \u00b6 To initialize the module, you need two parameters: Maximum GPU batch size - The maximum batch size that your Job can use on a single GPU (in terms of GPU memory). Without Elasticity, running with batch sizes larger than this number will cause a memory overflow. This number will be used by the Run:AI elasticity module for determining whether to use Gradient Accumulation or not. Global batch size - The desired batch size. Of course, if this number is larger than the Maximum GPU batch size defined above, the model will not fit into a single GPU. The elasticity module will then use Gradient Accumulation and multiple GPUs to run your code. Call the init() method from the imported module and pass these two arguments. For example, if you are using PyTorch, use the following line: runai.elastic.torch.init(256, 64) Where 256 is the global batch size and 64 is the maximum GPU batch size . Usage \u00b6 Keras \u00b6 Create a Keras model: model = runai.elastic.keras.models.Model(model) PyTorch \u00b6 For PyTorch models, you'll need to wrap your Optimizer with Gradient Accumulation: optimizer = runai.ga.torch.optim.Optimizer(optimizer, runai.elastic.steps) Where runai.elastic.steps is the value of accumulated steps which is calculated when calling init above. In addition, you will need to data-parallelise your model. We recommend using the built-in nn.DataParallel() method: model = torch.nn.DataParallel(model) Running a Training Workload \u00b6 Run the training workload by using the \"elastic\" flag: When launching the Job with the runai submit command use --elastic When launching a Job via YAML code, use the label \"elastic\" with the value \"true\" For additional information on how to run elastic training workloads, see the following quickstart . Limitations \u00b6 Elasticity currently works with Keras-based or PyTorch-based deep learning code only. Any training Job using Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Take care to save checkpoints in your code and have your code resume from the latest checkpoint rather than start from the beginning. See Also \u00b6 For additional documentation as well as Python examples see our GitHub repository Keras sample code in Github Pytorch Sample code in Github","title":"Elasticity"},{"location":"Researcher/researcher-library/rl-elasticity/#researcher-library-dynamically-stretch-or-compress-workloads-gpu-allocation","text":"","title":"Researcher Library: Dynamically Stretch or Compress Workload's GPU Allocation"},{"location":"Researcher/researcher-library/rl-elasticity/#introduction","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability.","title":"Introduction"},{"location":"Researcher/researcher-library/rl-elasticity/#expanding-a-workload","text":"Expanding a training Job allows your workload to run on more GPUs than the Researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing workloads to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs.","title":"Expanding a Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#shrinking-a-workload","text":"Shrinking a training Job allows your workload to run on a smaller number of GPUs than the Researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a Researcher to run a workload, albeit slower than intended, and let it automatically expand when GPUs become available at a later time. Shrinking a training Job uses an algorithm called Gradient Accumulation . For more information about the algorithm see: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa","title":"Shrinking a Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#installation","text":"Install the Run:AI Python library using the following command: pip install runai","title":"Installation"},{"location":"Researcher/researcher-library/rl-elasticity/#code","text":"In your python code, if using Keras, add: import runai.elastic.keras If using PyTorch, add: import runai.elastic.torch","title":"Code"},{"location":"Researcher/researcher-library/rl-elasticity/#initizalization","text":"To initialize the module, you need two parameters: Maximum GPU batch size - The maximum batch size that your Job can use on a single GPU (in terms of GPU memory). Without Elasticity, running with batch sizes larger than this number will cause a memory overflow. This number will be used by the Run:AI elasticity module for determining whether to use Gradient Accumulation or not. Global batch size - The desired batch size. Of course, if this number is larger than the Maximum GPU batch size defined above, the model will not fit into a single GPU. The elasticity module will then use Gradient Accumulation and multiple GPUs to run your code. Call the init() method from the imported module and pass these two arguments. For example, if you are using PyTorch, use the following line: runai.elastic.torch.init(256, 64) Where 256 is the global batch size and 64 is the maximum GPU batch size .","title":"Initizalization"},{"location":"Researcher/researcher-library/rl-elasticity/#usage","text":"","title":"Usage"},{"location":"Researcher/researcher-library/rl-elasticity/#keras","text":"Create a Keras model: model = runai.elastic.keras.models.Model(model)","title":"Keras"},{"location":"Researcher/researcher-library/rl-elasticity/#pytorch","text":"For PyTorch models, you'll need to wrap your Optimizer with Gradient Accumulation: optimizer = runai.ga.torch.optim.Optimizer(optimizer, runai.elastic.steps) Where runai.elastic.steps is the value of accumulated steps which is calculated when calling init above. In addition, you will need to data-parallelise your model. We recommend using the built-in nn.DataParallel() method: model = torch.nn.DataParallel(model)","title":"PyTorch"},{"location":"Researcher/researcher-library/rl-elasticity/#running-a-training-workload","text":"Run the training workload by using the \"elastic\" flag: When launching the Job with the runai submit command use --elastic When launching a Job via YAML code, use the label \"elastic\" with the value \"true\" For additional information on how to run elastic training workloads, see the following quickstart .","title":"Running a Training Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#limitations","text":"Elasticity currently works with Keras-based or PyTorch-based deep learning code only. Any training Job using Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Take care to save checkpoints in your code and have your code resume from the latest checkpoint rather than start from the beginning.","title":"Limitations"},{"location":"Researcher/researcher-library/rl-elasticity/#see-also","text":"For additional documentation as well as Python examples see our GitHub repository Keras sample code in Github Pytorch Sample code in Github","title":"See Also"},{"location":"Researcher/researcher-library/rl-hpo-support/","text":"Researcher Library: Hyperparameter Optimization Support \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is a helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing. Getting Started \u00b6 Prerequisites \u00b6 Run:AI HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml Installing \u00b6 Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3) Usage \u00b6 Import the runai.hpo package. import runai.hpo Initialize the Run:AI HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. runai . hpo . init ( '/path/to/nfs' , 'model-abcd-hpo' ) Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai . hpo . Strategy . GridSearch Call the Run:AI HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai . hpo . report ( epoch = 5 , metrics = { 'accuracy' : 0.87 }) See Also \u00b6 See hyperparameter Optimization Quickstart Sample code in Github","title":"HPO"},{"location":"Researcher/researcher-library/rl-hpo-support/#researcher-library-hyperparameter-optimization-support","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is a helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing.","title":"Researcher Library: Hyperparameter Optimization Support"},{"location":"Researcher/researcher-library/rl-hpo-support/#getting-started","text":"","title":"Getting Started"},{"location":"Researcher/researcher-library/rl-hpo-support/#prerequisites","text":"Run:AI HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml","title":"Prerequisites"},{"location":"Researcher/researcher-library/rl-hpo-support/#installing","text":"Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3)","title":"Installing"},{"location":"Researcher/researcher-library/rl-hpo-support/#usage","text":"Import the runai.hpo package. import runai.hpo Initialize the Run:AI HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. runai . hpo . init ( '/path/to/nfs' , 'model-abcd-hpo' ) Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai . hpo . Strategy . GridSearch Call the Run:AI HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai . hpo . report ( epoch = 5 , metrics = { 'accuracy' : 0.87 })","title":"Usage"},{"location":"Researcher/researcher-library/rl-hpo-support/#see-also","text":"See hyperparameter Optimization Quickstart Sample code in Github","title":"See Also"},{"location":"Researcher/researcher-library/rl-reporting/","text":"Researcher Library: Extended Reporting on Workload Progress \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The reporting module in the library will externalize information about the run which can then be available for users of the Run:AI user interface ( https://app.run.ai ) With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing. Sending Metrics \u00b6 Python Deep-Learning Code \u00b6 In your command-line run: pip install runai In your python code add: import runai.reporter Create a Reporter object as a Python context manager (i.e. with a with statement): with runai . reporter . Reporter () as reporter : pass Then use reporter to send metrics and parameters. To send a number-based metric report, write: reporter.reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reporter . reportMetric ( \"accuracy\" , 0.34 ) To send a text-based metric report, write: reporter.reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reporter . reportParameter ( \"state\" , \"Training Model\" ) Recommended Metrics to send \u00b6 For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric Each Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the Job progress bar is computed by dividing these parameters. Automatic Sending of Metrics for Keras-Based Scripts \u00b6 For Keras based deep learning runs, there is support to automate the task of sending metrics. First, import runai.reporter.keras instead of runai.reporter . Second, use runai.reporter.keras.Reporter instead of runai.reporter.Reporter . The Keras reporter supports automatic logging. This could be done in two ways: 1. Passing autolog=True upon creation 2. Calling the method autolog after creation After enabling automatic logging, the above metrics will automatically be sent going forward. For example: with runai . reporter . keras . Reporter ( autolog = True ) as reporter : pass Adding the Metrics to the User interface \u00b6 The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send metrics via the reporter library Run the workload once to send initial data. Go to Jobs list: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Reporting"},{"location":"Researcher/researcher-library/rl-reporting/#researcher-library-extended-reporting-on-workload-progress","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The reporting module in the library will externalize information about the run which can then be available for users of the Run:AI user interface ( https://app.run.ai ) With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing.","title":"Researcher Library: Extended Reporting on Workload Progress"},{"location":"Researcher/researcher-library/rl-reporting/#sending-metrics","text":"","title":"Sending Metrics"},{"location":"Researcher/researcher-library/rl-reporting/#python-deep-learning-code","text":"In your command-line run: pip install runai In your python code add: import runai.reporter Create a Reporter object as a Python context manager (i.e. with a with statement): with runai . reporter . Reporter () as reporter : pass Then use reporter to send metrics and parameters. To send a number-based metric report, write: reporter.reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reporter . reportMetric ( \"accuracy\" , 0.34 ) To send a text-based metric report, write: reporter.reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reporter . reportParameter ( \"state\" , \"Training Model\" )","title":"Python Deep-Learning Code"},{"location":"Researcher/researcher-library/rl-reporting/#recommended-metrics-to-send","text":"For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric Each Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the Job progress bar is computed by dividing these parameters.","title":"Recommended Metrics to send"},{"location":"Researcher/researcher-library/rl-reporting/#automatic-sending-of-metrics-for-keras-based-scripts","text":"For Keras based deep learning runs, there is support to automate the task of sending metrics. First, import runai.reporter.keras instead of runai.reporter . Second, use runai.reporter.keras.Reporter instead of runai.reporter.Reporter . The Keras reporter supports automatic logging. This could be done in two ways: 1. Passing autolog=True upon creation 2. Calling the method autolog after creation After enabling automatic logging, the above metrics will automatically be sent going forward. For example: with runai . reporter . keras . Reporter ( autolog = True ) as reporter : pass","title":"Automatic Sending of Metrics for Keras-Based Scripts"},{"location":"Researcher/researcher-library/rl-reporting/#adding-the-metrics-to-the-user-interface","text":"The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send metrics via the reporter library Run the workload once to send initial data. Go to Jobs list: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Adding the Metrics to the User interface"},{"location":"Researcher/tools/dev-jupyter/","text":"Use a Jupyter Notebook with a Run:AI Job \u00b6 Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI via such a notebook. Submit a Workload \u00b6 Run the following command to connect to the Jupyter Notebook container as if it were running local: runai submit build-jupyter --jupyter -g 1 The terminal will show the following: ~> runai submit build-jupyter --jupyter -g 1 INFO [ 0001 ] Exposing default jupyter notebook port 8888 INFO [ 0001 ] Using default jupyter notebook image \"jupyter/scipy-notebook\" INFO [ 0001 ] Using default jupyter notebook service type portforward The job 'build-jupyter' has been submitted successfully You can run ` runai describe job build-jupyter -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0081 ] Job started Jupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045 Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 The Job starts a Jupyter notebook container. The connection is redirected to the local machine (127.0.0.1) on port 8888 Browse to http://localhost:8888 . Use the token in the output to log into the notebook. Alternatives \u00b6 The above flag --jupyter is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports .","title":"Jupyter Noteook"},{"location":"Researcher/tools/dev-jupyter/#use-a-jupyter-notebook-with-a-runai-job","text":"Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI via such a notebook.","title":"Use a Jupyter Notebook with a Run:AI Job"},{"location":"Researcher/tools/dev-jupyter/#submit-a-workload","text":"Run the following command to connect to the Jupyter Notebook container as if it were running local: runai submit build-jupyter --jupyter -g 1 The terminal will show the following: ~> runai submit build-jupyter --jupyter -g 1 INFO [ 0001 ] Exposing default jupyter notebook port 8888 INFO [ 0001 ] Using default jupyter notebook image \"jupyter/scipy-notebook\" INFO [ 0001 ] Using default jupyter notebook service type portforward The job 'build-jupyter' has been submitted successfully You can run ` runai describe job build-jupyter -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0081 ] Job started Jupyter notebook token: 428dc561a5431bd383eff17714460de478d673deec57c045 Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 The Job starts a Jupyter notebook container. The connection is redirected to the local machine (127.0.0.1) on port 8888 Browse to http://localhost:8888 . Use the token in the output to log into the notebook.","title":"Submit a Workload"},{"location":"Researcher/tools/dev-jupyter/#alternatives","text":"The above flag --jupyter is a shortcut with a predefined image. If you want to run your own notebook, use the quickstart on running a build workload with connected ports .","title":"Alternatives"},{"location":"Researcher/tools/dev-pycharm/","text":"Use PyCharm with a Run:AI Job \u00b6 Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from JetBrain's PyCharm . Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 PyCharm \u00b6 Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use the IP address above. Change the port to the above and use the Username root You will be prompted for a password. Enter root Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely.","title":"PyCharm"},{"location":"Researcher/tools/dev-pycharm/#use-pycharm-with-a-runai-job","text":"Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from JetBrain's PyCharm .","title":"Use PyCharm with a Run:AI Job"},{"location":"Researcher/tools/dev-pycharm/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-pycharm/#pycharm","text":"Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use the IP address above. Change the port to the above and use the Username root You will be prompted for a password. Enter root Apply settings and run the code via this interpreter. You will see your project uploaded to the container and running remotely.","title":"PyCharm"},{"location":"Researcher/tools/dev-tensorboard/","text":"Connecting to TensorBoard \u00b6 Once you launch a Deep Learning workload using Run:AI, you may want to view its progress. A popular tool for viewing progress is TensorBoard . The document below explains how to use TensorBoard to view the progress or a Run:AI Job. Submit a Workload \u00b6 When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:AI sample code here . The code shows: A reference to a log directory: log_dir = \"logs/fit/\" + datetime . datetime . now () . strftime ( \"%Y%m %d -%H%M%S\" ) A registered Keras callback for TensorBoard: tensorboard_callback = TensorBoard ( log_dir = log_dir , histogram_freq = 1 ) model . fit ( x_train , y_train , .... callbacks = [ ... , tensorboard_callback ]) The logs directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows: runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh Note the volume flag ( -v ) and working directory flag ( --working-dir ). The logs directory will be created on /mnt/nfs_share/john/logs/fit . Submit a TensorBoard Job \u00b6 Run the following: runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888 --working-dir /mydir -v /mnt/nfs_share/john:/mydir --command -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0 The terminal will show the following: The job 'tb' has been submitted successfully You can run ` runai describe job tb -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start INFO [ 0014 ] Job started Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 Browse to http://localhost:8888/ to view TensorBoard. Note A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs.","title":"TensorBoard"},{"location":"Researcher/tools/dev-tensorboard/#connecting-to-tensorboard","text":"Once you launch a Deep Learning workload using Run:AI, you may want to view its progress. A popular tool for viewing progress is TensorBoard . The document below explains how to use TensorBoard to view the progress or a Run:AI Job.","title":"Connecting to TensorBoard"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-workload","text":"When you submit a workload, your workload must save TensorBoard logs which can later be viewed. Follow this document on how to do this. You can also view the Run:AI sample code here . The code shows: A reference to a log directory: log_dir = \"logs/fit/\" + datetime . datetime . now () . strftime ( \"%Y%m %d -%H%M%S\" ) A registered Keras callback for TensorBoard: tensorboard_callback = TensorBoard ( log_dir = log_dir , histogram_freq = 1 ) model . fit ( x_train , y_train , .... callbacks = [ ... , tensorboard_callback ]) The logs directory must be saved on a Network File Server such that it can be accessed by the TensorBoard Job. For example, by running the Job as follows: runai submit train-with-logs -i tensorflow/tensorflow:1.14.0-gpu-py3 \\ -v /mnt/nfs_share/john:/mydir -g 1 --working-dir /mydir --command -- ./startup.sh Note the volume flag ( -v ) and working directory flag ( --working-dir ). The logs directory will be created on /mnt/nfs_share/john/logs/fit .","title":"Submit a Workload"},{"location":"Researcher/tools/dev-tensorboard/#submit-a-tensorboard-job","text":"Run the following: runai submit tb -i tensorflow/tensorflow:latest --interactive --service-type=portforward --port 8888:8888 --working-dir /mydir -v /mnt/nfs_share/john:/mydir --command -- tensorboard --logdir logs/fit --port 8888 --host 0.0.0.0 The terminal will show the following: The job 'tb' has been submitted successfully You can run ` runai describe job tb -p team-a ` to check the job status INFO [ 0006 ] Waiting for job to start Waiting for job to start INFO [ 0014 ] Job started Open access point ( s ) to service from localhost:8888 Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 Browse to http://localhost:8888/ to view TensorBoard. Note A single TensorBoard Job can be used to view multiple deep learning Jobs, provided it has access to the logs directory for these Jobs.","title":"Submit a TensorBoard Job"},{"location":"Researcher/tools/dev-vscode/","text":"Use Visual Studio Code with a Run:AI Job \u00b6 Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from Visual Studio Code . Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222 Visual Studio Code \u00b6 Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Visual Studio Code"},{"location":"Researcher/tools/dev-vscode/#use-visual-studio-code-with-a-runai-job","text":"Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from Visual Studio Code .","title":"Use Visual Studio Code with a Run:AI Job"},{"location":"Researcher/tools/dev-vscode/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/pycharm-demo . The image runs both python and ssh. Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'build-remote' has been submitted successfully You can run ` runai describe job build-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Note It is possible to connect to the container using a remote IP address. However, this would be less convinient as you will need to maintain port numbers manually and change them when remote accessing using the development tool. As an example, run: runai submit build-remote -i gcr.io/run-ai-demo/pycharm-demo -g 1 --interactive --service-type=nodeport --port 30022:22 The Job starts an sshd server on port 22. The Job redirects the external port 30022 to port 22 and uses a Node Port service type. Run: runai list jobs Next to the Job, under the \"Service URL\" column you will find the IP address and port. The port is 30222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-vscode/#visual-studio-code","text":"Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022 or ssh root@127.0.0.1 -p 2222). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Visual Studio Code"},{"location":"Researcher/tools/dev-x11forward-pycharm/","text":"Use PyCharm with X11 Forwarding and Run:AI \u00b6 X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection. This section is about setting up X11 forwarding from a Run:AI-based container to a PyCharm IDE on a remote machine. Submit a Workload \u00b6 You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/quickstart-x-forwarding . The image runs: Python SSH Daemon configured for X11Forwarding OpenCV python library for image handling Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'xforward-remote' has been submitted successfully You can run ` runai describe job xforward-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222 Setup the X11 Forwarding Tunnel \u00b6 Connect to the new Job by running: ssh -X root@127.0.0.1 -p 2222 Note the -X flag. Run: echo $DISPLAY Copy the value. It will be used as a PyCharm environment variable. Important The ssh terminal should remain active throughout the session. PyCharm \u00b6 Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use localhost . Change the port to the above ( 2222 ) and use the Username root . You will be prompted for a password. Enter root . Make sure to set the correct path of the Python binary. In our case it's /usr/local/bin/python . Apply your settings. Under PyCharm configuration set the following environment variables: DISPLAY - set environment variable you copied before HOME - In our case it's /root . This is required for the X11 authentication to work. Run your code. You can use our sample code here .","title":"X11 & PyCharm"},{"location":"Researcher/tools/dev-x11forward-pycharm/#use-pycharm-with-x11-forwarding-and-runai","text":"X11 is a window system for the Unix operating systems. X11 forwarding allows executing a program remotely through an SSH connection. Meaning, the executable file itself is hosted on a different machine than where the graphical interface is being displayed. The graphical windows are forwarded to your local machine through the SSH connection. This section is about setting up X11 forwarding from a Run:AI-based container to a PyCharm IDE on a remote machine.","title":"Use PyCharm with X11 Forwarding and Run:AI"},{"location":"Researcher/tools/dev-x11forward-pycharm/#submit-a-workload","text":"You will need your image to run an SSH server (e.g OpenSSH ). For the purposes of this document, we have created an image named gcr.io/run-ai-demo/quickstart-x-forwarding . The image runs: Python SSH Daemon configured for X11Forwarding OpenCV python library for image handling Details on how to create the image are here . The image is configured to use the root user and password for SSH. Run the following command to connect to the container as if it were running locally: runai submit xforward-remote -i gcr.io/run-ai-demo/quickstart-x-forwarding --interactive \\ --service-type=portforward --port 2222:22 The terminal will show the connection: The job 'xforward-remote' has been submitted successfully You can run ` runai describe job xforward-remote -p team-a ` to check the job status INFO [ 0007 ] Waiting for job to start Waiting for job to start Waiting for job to start Waiting for job to start INFO [ 0045 ] Job started Open access point ( s ) to service from localhost:2222 Forwarding from [ ::1 ] :2222 -> 22 The Job starts an sshd server on port 22. The connection is redirected to the local machine (127.0.0.1) on port 2222","title":"Submit a Workload"},{"location":"Researcher/tools/dev-x11forward-pycharm/#setup-the-x11-forwarding-tunnel","text":"Connect to the new Job by running: ssh -X root@127.0.0.1 -p 2222 Note the -X flag. Run: echo $DISPLAY Copy the value. It will be used as a PyCharm environment variable. Important The ssh terminal should remain active throughout the session.","title":"Setup the X11 Forwarding Tunnel"},{"location":"Researcher/tools/dev-x11forward-pycharm/#pycharm","text":"Under PyCharm | Preferences go to: Project | Python Interpreter Add a new SSH Interpreter. As Host, use localhost . Change the port to the above ( 2222 ) and use the Username root . You will be prompted for a password. Enter root . Make sure to set the correct path of the Python binary. In our case it's /usr/local/bin/python . Apply your settings. Under PyCharm configuration set the following environment variables: DISPLAY - set environment variable you copied before HOME - In our case it's /root . This is required for the X11 authentication to work. Run your code. You can use our sample code here .","title":"PyCharm"},{"location":"developer/overview-developer/","text":"Overview: Developer Documentation \u00b6 Developers can access Run:AI through programmatic interfaces. As part of the Developer documentation you will find: Researcher REST API . A set of REST services to add, delete, and list Jobs. Kubernetes API . An alternative method of submitting a Job by directly accessing Kubernetes API. Integrations with various Machine Learning Operation (MLOps) tools .","title":"Overview"},{"location":"developer/overview-developer/#overview-developer-documentation","text":"Developers can access Run:AI through programmatic interfaces. As part of the Developer documentation you will find: Researcher REST API . A set of REST services to add, delete, and list Jobs. Kubernetes API . An alternative method of submitting a Job by directly accessing Kubernetes API. Integrations with various Machine Learning Operation (MLOps) tools .","title":"Overview: Developer Documentation"},{"location":"developer/admin-rest-api/clusters/","text":"\u00b6 const ui = SwaggerUIBundle({ url: 'clusters.yaml', dom_id: '#swagger-ui', })","title":"Clusters"},{"location":"developer/admin-rest-api/clusters/#_1","text":"const ui = SwaggerUIBundle({ url: 'clusters.yaml', dom_id: '#swagger-ui', })","title":""},{"location":"developer/admin-rest-api/departments/","text":"\u00b6 const ui = SwaggerUIBundle({ url: 'departments.yaml', dom_id: '#swagger-ui', })","title":"Departments"},{"location":"developer/admin-rest-api/departments/#_1","text":"const ui = SwaggerUIBundle({ url: 'departments.yaml', dom_id: '#swagger-ui', })","title":""},{"location":"developer/admin-rest-api/node-types/","text":"\u00b6 const ui = SwaggerUIBundle({ url: 'node-types.yaml', dom_id: '#swagger-ui', })","title":"Node Types"},{"location":"developer/admin-rest-api/node-types/#_1","text":"const ui = SwaggerUIBundle({ url: 'node-types.yaml', dom_id: '#swagger-ui', })","title":""},{"location":"developer/admin-rest-api/overview/","text":"Administrator REST API \u00b6 The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks. Document conventions \u00b6 The rest of this documentation is using the Open API specification to describe the API. You can test the API within the document after logging in. Security and authentication \u00b6 You must be a verified user to make API requests. You can authorize against the API using a dedicated user and password provided by Run:AI Customer Support. Note that the regular user will not suffice for API access. Once authenticated, you will receive a token ( bearer ) which you should use in all further API calls. Subsequent API calls would be made with the Header: authorization: Bearer <token>","title":"Overview"},{"location":"developer/admin-rest-api/overview/#administrator-rest-api","text":"The purpose of the Administrator REST API is to provide an easy-to-use programming interface for administrative tasks.","title":"Administrator REST API"},{"location":"developer/admin-rest-api/overview/#document-conventions","text":"The rest of this documentation is using the Open API specification to describe the API. You can test the API within the document after logging in.","title":"Document conventions"},{"location":"developer/admin-rest-api/overview/#security-and-authentication","text":"You must be a verified user to make API requests. You can authorize against the API using a dedicated user and password provided by Run:AI Customer Support. Note that the regular user will not suffice for API access. Once authenticated, you will receive a token ( bearer ) which you should use in all further API calls. Subsequent API calls would be made with the Header: authorization: Bearer <token>","title":"Security and authentication"},{"location":"developer/admin-rest-api/projects/","text":"\u00b6 const ui = SwaggerUIBundle({ url: 'projects.yaml', dom_id: '#swagger-ui', })","title":"Projects"},{"location":"developer/admin-rest-api/projects/#_1","text":"const ui = SwaggerUIBundle({ url: 'projects.yaml', dom_id: '#swagger-ui', })","title":""},{"location":"developer/admin-rest-api/users/","text":"\u00b6 const ui = SwaggerUIBundle({ url: 'users.yaml', dom_id: '#swagger-ui', })","title":"Users"},{"location":"developer/admin-rest-api/users/#_1","text":"const ui = SwaggerUIBundle({ url: 'users.yaml', dom_id: '#swagger-ui', })","title":""},{"location":"developer/integrations/airflow-integration/","text":"Integrate Run:AI with Apache Airflow \u00b6 Airflow is a platform to programmatically author, schedule, and monitor workflows. Specifically, it is used in Machine Learning to create pipelines. Airflow DAG \u00b6 In Airflow, a DAG \u2013 or a Directed Acyclic Graph \u2013 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code. For example, a simple DAG could consist of three tasks: A, B, and C. It could say that A has to run successfully before B can run, but C can run anytime. It could say that task A times out after 5 minutes, and B can be restarted up to 5 times in case it fails. It might also say that the workflow will run every night at 10pm, but shouldn\u2019t start until a certain date. Airflow tasks are sent for execution. Specifically, the Airflow - Kubernetes integration allows Airflow tasks to be scheduled on a Kubernetes cluster. Run:AI - Airflow Integration \u00b6 DAGs are defined in Python. Airflow tasks based on Kubernetes are defined via the KubernetesPodOperator class. To run an Airflow task with Run:AI you must provide additional Run:AI-related properties to dag = DAG ( ... ) resources = { \"limit_gpu\" : < number - of - GPUs > } job = KubernetesPodOperator ( namespace = 'runai-<project-name>' , image = '<image-name>' , labels = { \"project\" : '<project-name>' }, name = '<task-name>' , task_id = '<task-name>' , get_logs = True , schedulername = 'runai-scheduler' , resources = resources , dag = dag ) The code: Specifies the runai-scheduler which directs the task to be scheduled with the Run:AI scheduler Specifies a Run:AI Project . A Project in Run:AI specifies guaranteed GPU & CPU quota. Once you run the DAG, you can see Airflow tasks showing in the Run:AI UI.","title":"Airflow integration"},{"location":"developer/integrations/airflow-integration/#integrate-runai-with-apache-airflow","text":"Airflow is a platform to programmatically author, schedule, and monitor workflows. Specifically, it is used in Machine Learning to create pipelines.","title":"Integrate Run:AI with Apache Airflow"},{"location":"developer/integrations/airflow-integration/#airflow-dag","text":"In Airflow, a DAG \u2013 or a Directed Acyclic Graph \u2013 is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code. For example, a simple DAG could consist of three tasks: A, B, and C. It could say that A has to run successfully before B can run, but C can run anytime. It could say that task A times out after 5 minutes, and B can be restarted up to 5 times in case it fails. It might also say that the workflow will run every night at 10pm, but shouldn\u2019t start until a certain date. Airflow tasks are sent for execution. Specifically, the Airflow - Kubernetes integration allows Airflow tasks to be scheduled on a Kubernetes cluster.","title":"Airflow DAG"},{"location":"developer/integrations/airflow-integration/#runai-airflow-integration","text":"DAGs are defined in Python. Airflow tasks based on Kubernetes are defined via the KubernetesPodOperator class. To run an Airflow task with Run:AI you must provide additional Run:AI-related properties to dag = DAG ( ... ) resources = { \"limit_gpu\" : < number - of - GPUs > } job = KubernetesPodOperator ( namespace = 'runai-<project-name>' , image = '<image-name>' , labels = { \"project\" : '<project-name>' }, name = '<task-name>' , task_id = '<task-name>' , get_logs = True , schedulername = 'runai-scheduler' , resources = resources , dag = dag ) The code: Specifies the runai-scheduler which directs the task to be scheduled with the Run:AI scheduler Specifies a Run:AI Project . A Project in Run:AI specifies guaranteed GPU & CPU quota. Once you run the DAG, you can see Airflow tasks showing in the Run:AI UI.","title":"Run:AI - Airflow Integration"},{"location":"developer/k8s-api/launch-job-via-kubernetes-api/","text":"Submit a Run:AI Job via Kubernetes API \u00b6 The easiest way to submit jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases where you want to forgo the CLI and use direct API calls. A frequent scenario for using the Kubernetes YAML syntax to submit jobs is integrations . Researchers may already be working with an existing system that submits jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough. There are a couple of alternatives to achieving this: * Use the Researcher REST API to submit, list and delete Jobs. * Launch a Job via YAML * This article is a complementary article to the article Launching jobs via YAML . It shows how to use Kubernetes API to submit jobs. The article uses Python, though Kubernetes API is available several other programming languages. Submit a Run:AI Job \u00b6 The following code builds the Job YAML from the article on launching jobs via YAML and sends it via Kubernetes API. from __future__ import print_function import kubernetes from kubernetes import client , config from pprint import pprint import json config . load_kube_config () with client . ApiClient () as api_client : namespace = 'runai-team-a' # Run:AI project name is prefixed by runai- jobname = 'my-job' username = 'john' # used in un-authenticated systems only gpus = 1 body = client . V1Job ( api_version = \"run.ai/v1\" , kind = \"RunaiJob\" ) body . metadata = client . V1ObjectMeta ( namespace = namespace , name = jobname ) template = client . V1PodTemplate () template . template = client . V1PodTemplateSpec () template . template . metadata = client . V1ObjectMeta ( labels = { 'user' : username }) resource = client . V1ResourceRequirements ( limits = { 'nvidia.com/gpu' : gpus }) container = client . V1Container ( name = jobname , image = 'gcr.io/run-ai-demo/quickstart' , resources = resource ) template . template . spec = client . V1PodSpec ( containers = [ container ], restart_policy = 'Never' , scheduler_name = 'runai-scheduler' ) body . spec = client . V1JobSpec ( template = template . template ) pprint ( body ) try : api_instance = client . CustomObjectsApi ( api_client ) api_response = api_instance . create_namespaced_custom_object ( \"run.ai\" , \"v1\" , namespace , \"runaijobs\" , body ) pprint ( api_response ) except client . rest . ApiException as e : print ( \"Exception when calling AppsV1Api->create_namespaced_job: %s \\n \" % e )","title":"Submit a Job via Kubernetes API"},{"location":"developer/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job-via-kubernetes-api","text":"The easiest way to submit jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases where you want to forgo the CLI and use direct API calls. A frequent scenario for using the Kubernetes YAML syntax to submit jobs is integrations . Researchers may already be working with an existing system that submits jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough. There are a couple of alternatives to achieving this: * Use the Researcher REST API to submit, list and delete Jobs. * Launch a Job via YAML * This article is a complementary article to the article Launching jobs via YAML . It shows how to use Kubernetes API to submit jobs. The article uses Python, though Kubernetes API is available several other programming languages.","title":"Submit a Run:AI Job via Kubernetes API"},{"location":"developer/k8s-api/launch-job-via-kubernetes-api/#submit-a-runai-job","text":"The following code builds the Job YAML from the article on launching jobs via YAML and sends it via Kubernetes API. from __future__ import print_function import kubernetes from kubernetes import client , config from pprint import pprint import json config . load_kube_config () with client . ApiClient () as api_client : namespace = 'runai-team-a' # Run:AI project name is prefixed by runai- jobname = 'my-job' username = 'john' # used in un-authenticated systems only gpus = 1 body = client . V1Job ( api_version = \"run.ai/v1\" , kind = \"RunaiJob\" ) body . metadata = client . V1ObjectMeta ( namespace = namespace , name = jobname ) template = client . V1PodTemplate () template . template = client . V1PodTemplateSpec () template . template . metadata = client . V1ObjectMeta ( labels = { 'user' : username }) resource = client . V1ResourceRequirements ( limits = { 'nvidia.com/gpu' : gpus }) container = client . V1Container ( name = jobname , image = 'gcr.io/run-ai-demo/quickstart' , resources = resource ) template . template . spec = client . V1PodSpec ( containers = [ container ], restart_policy = 'Never' , scheduler_name = 'runai-scheduler' ) body . spec = client . V1JobSpec ( template = template . template ) pprint ( body ) try : api_instance = client . CustomObjectsApi ( api_client ) api_response = api_instance . create_namespaced_custom_object ( \"run.ai\" , \"v1\" , namespace , \"runaijobs\" , body ) pprint ( api_response ) except client . rest . ApiException as e : print ( \"Exception when calling AppsV1Api->create_namespaced_job: %s \\n \" % e )","title":"Submit a Run:AI Job"},{"location":"developer/k8s-api/launch-job-via-yaml/","text":"Submit a Run:AI Job via YAML \u00b6 The easiest way to submit Jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases where you want to forgo the CLI and use direct YAML calls. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations . Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough. Terminology \u00b6 We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. The internal Kubernetes implementation of Train is a CRD (Customer Resource) named RunaiJob which is similar to a Kubernetes Job . The internal implementation of Buils is a Kubernetes StatesfulSet . A Kubernetes Job is used for Train workloads. A Job has a distinctive \"end\" at which time the Job is either \"Completed\" or \"Failed\" A Kubernetes StatefulSet is used for Build workloads. Build workloads are interactive sessions. StatefulSets do not end on their own. Instead, they must be manually stopped Run:AI extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler . The Run:AI scheduler schedules computing resources by associating Workloads with Run:AI Projects : A Project is assigned with a GPU quota through the Run:AI Administrator user interface. Each workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads Internally, Run:AI Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set. Submit Workloads \u00b6 Regular Jobs \u00b6 <JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart <USER-NAME> The name of the user submitting the Job. The name is used for display purposes only when Run:AI is installed in an unauthenticated mode . <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2 Copy the following into a file and change the parameters: apiVersion : run.ai/v1 kind : RunaiJob (* see note below) metadata : name : <JOB-NAME> labels : priorityClassName : \"build\" (** see note below) spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler Run: kubectl apply -f <FILE-NAME> to submit the Job. Note You can use either a regular Job or RunaiJob . The later is a Run:AI object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios ** Using 'build' in this field is equivilant to run a job with '--interactive' flag. To run Train job, delete this line The runai submit CLI command includes many more flags. These flags can be correlated to Kubernetes API functions and added to the YAML above. Using Fractional GPUs \u00b6 Jobs with Fractions requires a change in the above YAML. Specifically the limits section: limits : nvidia.com/gpu : <REQUESTED-GPUs> should be omitted and replaced with: spec : template : metadata : annotations : gpu-fraction : \"0.5\" where \"0.5\" is the requested GPU fraction. Delete Workloads \u00b6 To delete a Run:AI workload you need to delete the Job or StatefulSet according to the workload type kubectl delete job <JOB-NAME> or: kubectl delete sts <STS-NAME> See Also \u00b6 See how to use the above YAML syntax with Kubernetes API Use the Researcher REST API to submit, list and delete Jobs.","title":"Submit a Job via YAML"},{"location":"developer/k8s-api/launch-job-via-yaml/#submit-a-runai-job-via-yaml","text":"The easiest way to submit Jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases where you want to forgo the CLI and use direct YAML calls. A frequent scenario for using the Kubernetes YAML syntax to submit Jobs is integrations . Researchers may already be working with an existing system that submits Jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough.","title":"Submit a Run:AI Job via YAML"},{"location":"developer/k8s-api/launch-job-via-yaml/#terminology","text":"We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. The internal Kubernetes implementation of Train is a CRD (Customer Resource) named RunaiJob which is similar to a Kubernetes Job . The internal implementation of Buils is a Kubernetes StatesfulSet . A Kubernetes Job is used for Train workloads. A Job has a distinctive \"end\" at which time the Job is either \"Completed\" or \"Failed\" A Kubernetes StatefulSet is used for Build workloads. Build workloads are interactive sessions. StatefulSets do not end on their own. Instead, they must be manually stopped Run:AI extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler . The Run:AI scheduler schedules computing resources by associating Workloads with Run:AI Projects : A Project is assigned with a GPU quota through the Run:AI Administrator user interface. Each workload must be associated with a Project name and will receive resources according to the defined quota for the Project and the currently running Workloads Internally, Run:AI Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set.","title":"Terminology"},{"location":"developer/k8s-api/launch-job-via-yaml/#submit-workloads","text":"","title":"Submit Workloads"},{"location":"developer/k8s-api/launch-job-via-yaml/#regular-jobs","text":"<JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart <USER-NAME> The name of the user submitting the Job. The name is used for display purposes only when Run:AI is installed in an unauthenticated mode . <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2 Copy the following into a file and change the parameters: apiVersion : run.ai/v1 kind : RunaiJob (* see note below) metadata : name : <JOB-NAME> labels : priorityClassName : \"build\" (** see note below) spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler Run: kubectl apply -f <FILE-NAME> to submit the Job. Note You can use either a regular Job or RunaiJob . The later is a Run:AI object which solves various Kubernetes Bugs and provides a better naming for multiple pods in Hyper-Parameter Optimization scenarios ** Using 'build' in this field is equivilant to run a job with '--interactive' flag. To run Train job, delete this line The runai submit CLI command includes many more flags. These flags can be correlated to Kubernetes API functions and added to the YAML above.","title":"Regular Jobs"},{"location":"developer/k8s-api/launch-job-via-yaml/#using-fractional-gpus","text":"Jobs with Fractions requires a change in the above YAML. Specifically the limits section: limits : nvidia.com/gpu : <REQUESTED-GPUs> should be omitted and replaced with: spec : template : metadata : annotations : gpu-fraction : \"0.5\" where \"0.5\" is the requested GPU fraction.","title":"Using Fractional GPUs"},{"location":"developer/k8s-api/launch-job-via-yaml/#delete-workloads","text":"To delete a Run:AI workload you need to delete the Job or StatefulSet according to the workload type kubectl delete job <JOB-NAME> or: kubectl delete sts <STS-NAME>","title":"Delete Workloads"},{"location":"developer/k8s-api/launch-job-via-yaml/#see-also","text":"See how to use the above YAML syntax with Kubernetes API Use the Researcher REST API to submit, list and delete Jobs.","title":"See Also"},{"location":"developer/researcher-rest-api/overview/","text":"Researcher REST API \u00b6 The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing and deleting Jobs. There are other APIs that each the same functionality. Specifically: If your code is script-based, you may consider using the Run:AI command-line interface . You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:AI Job via Kubernetes API . Finding the API Endpoint URL \u00b6 The URL is composed of an IP address part and a port part ( <IP-ADDRESS>:<PORT> ). To find the endpoint, run: echo \"http:// $( kubectl get nodes -o = jsonpath = '{.items[0].status.addresses[0].address}' ) : $( kubectl get services -n runai -o = jsonpath = '{.items[?(@.metadata.name == \"researcher-service\")].spec.ports[0].nodePort}' ) \" Limitations \u00b6 The Researcher REST API does not work when the system is configured to authenticate Researchers . We are working to add this functionality.","title":"Overview"},{"location":"developer/researcher-rest-api/overview/#researcher-rest-api","text":"The purpose of the Researcher REST API is to provide an easy-to-use programming interface for submitting, listing and deleting Jobs. There are other APIs that each the same functionality. Specifically: If your code is script-based, you may consider using the Run:AI command-line interface . You can communicate directly with the underlying Kubernetes infrastructure by sending YAML files or by using a variety of programming languages to send requests to Kubernetes. See Submit a Run:AI Job via Kubernetes API .","title":"Researcher REST API"},{"location":"developer/researcher-rest-api/overview/#finding-the-api-endpoint-url","text":"The URL is composed of an IP address part and a port part ( <IP-ADDRESS>:<PORT> ). To find the endpoint, run: echo \"http:// $( kubectl get nodes -o = jsonpath = '{.items[0].status.addresses[0].address}' ) : $( kubectl get services -n runai -o = jsonpath = '{.items[?(@.metadata.name == \"researcher-service\")].spec.ports[0].nodePort}' ) \"","title":"Finding the API Endpoint URL"},{"location":"developer/researcher-rest-api/overview/#limitations","text":"The Researcher REST API does not work when the system is configured to authenticate Researchers . We are working to add this functionality.","title":"Limitations"},{"location":"developer/researcher-rest-api/rest-delete/","text":"Delete one or more Run:AI Jobs. General \u00b6 URL : http://<service-url>/api/v1/jobs Method : DELETE Request \u00b6 Following JSON: [<Job Identifier 1>, .... ,<Job Identifier n>] Job identifier definition: { \"name\" : \"<job-name>\" , \"project\" : \"<job-project>\" } Examples \u00b6 curl --location --request DELETE 'http://example.com/api/v1/jobs' \\ --header 'Content-Type: application/json' \\ --data-raw '[ {\"name\" : \"job-name-0\", \"project\" : \"team-a\"}, {\"name\" : \"job-name-1\", \"project\" : \"team-a\"} ]'","title":"Delete a Job"},{"location":"developer/researcher-rest-api/rest-delete/#general","text":"URL : http://<service-url>/api/v1/jobs Method : DELETE","title":"General"},{"location":"developer/researcher-rest-api/rest-delete/#request","text":"Following JSON: [<Job Identifier 1>, .... ,<Job Identifier n>] Job identifier definition: { \"name\" : \"<job-name>\" , \"project\" : \"<job-project>\" }","title":"Request"},{"location":"developer/researcher-rest-api/rest-delete/#examples","text":"curl --location --request DELETE 'http://example.com/api/v1/jobs' \\ --header 'Content-Type: application/json' \\ --data-raw '[ {\"name\" : \"job-name-0\", \"project\" : \"team-a\"}, {\"name\" : \"job-name-1\", \"project\" : \"team-a\"} ]'","title":"Examples"},{"location":"developer/researcher-rest-api/rest-list-jobs/","text":"Get a list of all Run:AI Jobs for a given project General \u00b6 URL : http://<service-url>/api/v1/jobs Method : GET Request \u00b6 project=<project-name> If the project parameter is omitted, then all Jobs will be returned. Response \u00b6 { \"data\": Array<Job> } Job: { \"id\" : \"<JOB ID>\" , \"project\" : \"<Job Project>\" , \"name\" : \"<Job Name>\" , \"status\" : \"<Job status>\" , \"type\" : \"<Job type>\" , \"nodes\" : \"<Node name>\" , \"createdAt\" : \"<Job creation time>\" , \"images\" : \"<Job image>\" , \"user\" : \"<User name>\" , \"currentAllocatedGPUs\" : \"<GPUs>\" } status will have the values: \"Pending\", \"Running\", \"Succeeded\", \"Failed\" or \"Unknown\". type will have the values: \"Train\" or \"Interactive\". createdAt Job Creation time in a UNIX timestamp format (in milliseconds). nodes shows the one or more nodes on which the Job is running. Example \u00b6 Request: curl --location --request GET 'http://www.example.com/api/v1/jobs?project=team-a' Response: { \"data\" : [ { \"id\" : \"32201782-a525-4939-9dcb-df6654b0b340\" , \"project\" : \"team-a\" , \"name\" : \"test1\" , \"status\" : \"Running\" , \"type\" : \"Train\" , \"nodes\" : [ \"dev1-worker-cpu\" ], \"createdAt\" : 1609494983000 , \"images\" : \"ubuntu\" , \"user\" : \"john\" , \"currentAllocatedGPUs\" : 1 }, { \"id\" : \"f4606bb5-10f6-4800-9590-933ff1606eba\" , \"project\" : \"team-a\" , \"name\" : \"job-0\" , \"status\" : \"ImagePullBackOff\" , \"type\" : \"Train\" , \"nodes\" : [ \"dev1-worker-cpu\" ], \"createdAt\" : 1609672319000 , \"images\" : \"gcr.io/run-ai-demo/quickstart\" , \"user\" : \"john\" , \"currentAllocatedGPUs\" : 0 }, { \"id\" : \"1b577b66-4ee4-440d-be13-9b732789c453\" , \"project\" : \"team-a\" , \"name\" : \"job-10\" , \"status\" : \"Succeeded\" , \"type\" : \"Train\" , \"nodes\" : [], \"createdAt\" : 1609251299000 , \"images\" : \"ubuntu\" , \"user\" : \"jill\" , \"currentAllocatedGPUs\" : 0 } ] }","title":"Get a list of Jobs"},{"location":"developer/researcher-rest-api/rest-list-jobs/#general","text":"URL : http://<service-url>/api/v1/jobs Method : GET","title":"General"},{"location":"developer/researcher-rest-api/rest-list-jobs/#request","text":"project=<project-name> If the project parameter is omitted, then all Jobs will be returned.","title":"Request"},{"location":"developer/researcher-rest-api/rest-list-jobs/#response","text":"{ \"data\": Array<Job> } Job: { \"id\" : \"<JOB ID>\" , \"project\" : \"<Job Project>\" , \"name\" : \"<Job Name>\" , \"status\" : \"<Job status>\" , \"type\" : \"<Job type>\" , \"nodes\" : \"<Node name>\" , \"createdAt\" : \"<Job creation time>\" , \"images\" : \"<Job image>\" , \"user\" : \"<User name>\" , \"currentAllocatedGPUs\" : \"<GPUs>\" } status will have the values: \"Pending\", \"Running\", \"Succeeded\", \"Failed\" or \"Unknown\". type will have the values: \"Train\" or \"Interactive\". createdAt Job Creation time in a UNIX timestamp format (in milliseconds). nodes shows the one or more nodes on which the Job is running.","title":"Response"},{"location":"developer/researcher-rest-api/rest-list-jobs/#example","text":"Request: curl --location --request GET 'http://www.example.com/api/v1/jobs?project=team-a' Response: { \"data\" : [ { \"id\" : \"32201782-a525-4939-9dcb-df6654b0b340\" , \"project\" : \"team-a\" , \"name\" : \"test1\" , \"status\" : \"Running\" , \"type\" : \"Train\" , \"nodes\" : [ \"dev1-worker-cpu\" ], \"createdAt\" : 1609494983000 , \"images\" : \"ubuntu\" , \"user\" : \"john\" , \"currentAllocatedGPUs\" : 1 }, { \"id\" : \"f4606bb5-10f6-4800-9590-933ff1606eba\" , \"project\" : \"team-a\" , \"name\" : \"job-0\" , \"status\" : \"ImagePullBackOff\" , \"type\" : \"Train\" , \"nodes\" : [ \"dev1-worker-cpu\" ], \"createdAt\" : 1609672319000 , \"images\" : \"gcr.io/run-ai-demo/quickstart\" , \"user\" : \"john\" , \"currentAllocatedGPUs\" : 0 }, { \"id\" : \"1b577b66-4ee4-440d-be13-9b732789c453\" , \"project\" : \"team-a\" , \"name\" : \"job-10\" , \"status\" : \"Succeeded\" , \"type\" : \"Train\" , \"nodes\" : [], \"createdAt\" : 1609251299000 , \"images\" : \"ubuntu\" , \"user\" : \"jill\" , \"currentAllocatedGPUs\" : 0 } ] }","title":"Example"},{"location":"developer/researcher-rest-api/rest-list-projects/","text":"Get a list of all Run:AI Projects General \u00b6 URL : http://<service-url>/api/v1/projects Method : GET Request \u00b6 Response \u00b6 { \"data\": Array<Project> } Project: { \"name\" : \"<Project Name>\" , \"createdAt\" : \"<Project creation time>\" , \"deservedGpus\" : \"<GPUs>\" , \"interactiveJobTimeLimitSecs\" : \"<TTL for Interactive Jobs>\" , \"trainNodeAffinity\" : \"Array<Affinity Group>\" , \"interactiveNodeAffinity\" : \"Array<Affinity Group>\" , \"departmentName\" : \"default\" } deservedGpus GPU deserved quota for this Project. createdAt Project Creation time in a UNIX timestamp format (in milliseconds). trainNodeAffinity Scheduler training Jobs only on these node groups. interactiveNodeAffinity - Scheduler interactive Jobs only on these node groups. For more information see Working with Projects . Example \u00b6 Request: curl --location --request GET 'http://www.example.com/api/v1/projects' Response: { \"data\" : [ { \"name\" : \"team-a\" , \"createdAt\" : 1609183432000 , \"deservedGpus\" : 2 , \"interactiveJobTimeLimitSecs\" : 0 , \"trainNodeAffinity\" : null , \"interactiveNodeAffinity\" : null , \"departmentName\" : \"default\" }, { \"name\" : \"team-b\" , \"createdAt\" : 1609183432000 , \"deservedGpus\" : 2 , \"interactiveJobTimeLimitSecs\" : 0 , \"trainNodeAffinity\" : [ \"gpu2\" , \"gpu1\" ], \"interactiveNodeAffinity\" : null , \"departmentName\" : \"default\" } ] }","title":"Get a list of Projects"},{"location":"developer/researcher-rest-api/rest-list-projects/#general","text":"URL : http://<service-url>/api/v1/projects Method : GET","title":"General"},{"location":"developer/researcher-rest-api/rest-list-projects/#request","text":"","title":"Request"},{"location":"developer/researcher-rest-api/rest-list-projects/#response","text":"{ \"data\": Array<Project> } Project: { \"name\" : \"<Project Name>\" , \"createdAt\" : \"<Project creation time>\" , \"deservedGpus\" : \"<GPUs>\" , \"interactiveJobTimeLimitSecs\" : \"<TTL for Interactive Jobs>\" , \"trainNodeAffinity\" : \"Array<Affinity Group>\" , \"interactiveNodeAffinity\" : \"Array<Affinity Group>\" , \"departmentName\" : \"default\" } deservedGpus GPU deserved quota for this Project. createdAt Project Creation time in a UNIX timestamp format (in milliseconds). trainNodeAffinity Scheduler training Jobs only on these node groups. interactiveNodeAffinity - Scheduler interactive Jobs only on these node groups. For more information see Working with Projects .","title":"Response"},{"location":"developer/researcher-rest-api/rest-list-projects/#example","text":"Request: curl --location --request GET 'http://www.example.com/api/v1/projects' Response: { \"data\" : [ { \"name\" : \"team-a\" , \"createdAt\" : 1609183432000 , \"deservedGpus\" : 2 , \"interactiveJobTimeLimitSecs\" : 0 , \"trainNodeAffinity\" : null , \"interactiveNodeAffinity\" : null , \"departmentName\" : \"default\" }, { \"name\" : \"team-b\" , \"createdAt\" : 1609183432000 , \"deservedGpus\" : 2 , \"interactiveJobTimeLimitSecs\" : 0 , \"trainNodeAffinity\" : [ \"gpu2\" , \"gpu1\" ], \"interactiveNodeAffinity\" : null , \"departmentName\" : \"default\" } ] }","title":"Example"},{"location":"developer/researcher-rest-api/rest-submit/","text":"Create a new Run:AI Job. General \u00b6 URL : http://<service-url>/api/v1/jobs Method : POST Headers RA-USER: <user-name> Content-Type: application/json The user name is used for display purposes only when Run:AI is installed in an unauthenticated mode . Request \u00b6 The Request body is a JSON object of a Run:AI Job as follows: { \"job\" : { \"name\" : \"string\" , \"project\" : \"string (*required)\" , \"interactive\" : \"boolean\" , \"image\" : \"string (*required)\" , \"command\" : \"string\" , \"arguments\" : [ \"-e\" , \"-f\" ], \"environment\" : { \"BATCH_SIZE\" : 50 , \"LEARNING_RATE\" : 0.2 , }, \"imagePullPolicy\" : \"string\" , \"stdin\" : \"boolean\" , \"tty\" : \"boolean\" , \"workingDir\" : \"string\" , \"createHomeDir\" : \"boolean\" , \"gpu\" : \"double\" , \"cpu\" : \"double\" , \"cpuLimit\" : \"integer\" , \"memory\" : \"string\" , \"memoryLimit\" : \"string\" , \"largeShm\" : \"boolean\" , \"pvc\" : [ { \"storageClass\" : \"my-storage1\" , \"size\" : \"3GB\" , \"path\" : \"/tmp/john\" , \"readOnly\" : true }, { \"storageClass\" : \"my-storage2\" , \"size\" : \"4GB\" , \"path\" : \"/tmp/jill\" , \"readOnly\" : false } ], \"volume\" : { \"/raid/public/john/data\" : \"/root/data\" , \"/raid/public/john/code\" : \"/root/code\" }, \"hostIpc\" : \"boolean\" , \"hostNetwork\" : \"boolean\" , \"ports\" : [ { \"container\" : 80 , \"external\" : 32188 , \"autoGenerate\" : false }, { \"container\" : 443 , \"autoGenerate\" : true } ], \"backoffLimit\" : \"integer\" , \"completions\" : \"integer\" , \"parallelism\" : \"integer\" , \"elastic\" : \"boolean\" , \"preemptible\" : \"boolean\" , \"serviceType\" : \"string\" , \"ttlAfterFinish\" : \"string\" , \"preventPrivilegeEscalation\" : \"boolean\" , \"nodeType\" : \"string\" , \"jobNamePrefix\" : \"string\" } } Job Parameters \u00b6 Full documentation of the above parameters can be found in runai-submit . Mandatory parameters are marked as required. Response \u00b6 { \"name\" : \"<new-job-name>\" } Examples \u00b6 Basic job with an auto-generated name curl -X POST 'http://www.example.com/api/job' \\ --header 'RA-USER: john' \\ --header 'Content-Type: application/json' \\ --data-raw ' { \"job\": { \"project\": \"team-a\", \"image\": \"gcr.io/run-ai-demo/quickstart\", \"gpu\": 1 } }'","title":"Submit a Job"},{"location":"developer/researcher-rest-api/rest-submit/#general","text":"URL : http://<service-url>/api/v1/jobs Method : POST Headers RA-USER: <user-name> Content-Type: application/json The user name is used for display purposes only when Run:AI is installed in an unauthenticated mode .","title":"General"},{"location":"developer/researcher-rest-api/rest-submit/#request","text":"The Request body is a JSON object of a Run:AI Job as follows: { \"job\" : { \"name\" : \"string\" , \"project\" : \"string (*required)\" , \"interactive\" : \"boolean\" , \"image\" : \"string (*required)\" , \"command\" : \"string\" , \"arguments\" : [ \"-e\" , \"-f\" ], \"environment\" : { \"BATCH_SIZE\" : 50 , \"LEARNING_RATE\" : 0.2 , }, \"imagePullPolicy\" : \"string\" , \"stdin\" : \"boolean\" , \"tty\" : \"boolean\" , \"workingDir\" : \"string\" , \"createHomeDir\" : \"boolean\" , \"gpu\" : \"double\" , \"cpu\" : \"double\" , \"cpuLimit\" : \"integer\" , \"memory\" : \"string\" , \"memoryLimit\" : \"string\" , \"largeShm\" : \"boolean\" , \"pvc\" : [ { \"storageClass\" : \"my-storage1\" , \"size\" : \"3GB\" , \"path\" : \"/tmp/john\" , \"readOnly\" : true }, { \"storageClass\" : \"my-storage2\" , \"size\" : \"4GB\" , \"path\" : \"/tmp/jill\" , \"readOnly\" : false } ], \"volume\" : { \"/raid/public/john/data\" : \"/root/data\" , \"/raid/public/john/code\" : \"/root/code\" }, \"hostIpc\" : \"boolean\" , \"hostNetwork\" : \"boolean\" , \"ports\" : [ { \"container\" : 80 , \"external\" : 32188 , \"autoGenerate\" : false }, { \"container\" : 443 , \"autoGenerate\" : true } ], \"backoffLimit\" : \"integer\" , \"completions\" : \"integer\" , \"parallelism\" : \"integer\" , \"elastic\" : \"boolean\" , \"preemptible\" : \"boolean\" , \"serviceType\" : \"string\" , \"ttlAfterFinish\" : \"string\" , \"preventPrivilegeEscalation\" : \"boolean\" , \"nodeType\" : \"string\" , \"jobNamePrefix\" : \"string\" } }","title":"Request"},{"location":"developer/researcher-rest-api/rest-submit/#job-parameters","text":"Full documentation of the above parameters can be found in runai-submit . Mandatory parameters are marked as required.","title":"Job Parameters"},{"location":"developer/researcher-rest-api/rest-submit/#response","text":"{ \"name\" : \"<new-job-name>\" }","title":"Response"},{"location":"developer/researcher-rest-api/rest-submit/#examples","text":"Basic job with an auto-generated name curl -X POST 'http://www.example.com/api/job' \\ --header 'RA-USER: john' \\ --header 'Content-Type: application/json' \\ --data-raw ' { \"job\": { \"project\": \"team-a\", \"image\": \"gcr.io/run-ai-demo/quickstart\", \"gpu\": 1 } }'","title":"Examples"},{"location":"home/components/","text":"Run:AI System Components \u00b6 Components \u00b6 Run:AI is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:AI Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Administrator User Interface The Run:AI Cluster \u00b6 The Run:AI Cluster contains: The Run:AI Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. The Run:AI agent. Responsible for sending Monitoring data to the Run:AI Cloud. Clusters require outbound network connectivity to the Run:AI Cloud. Kubernetes-Related Details \u00b6 The Run:AI cluster is installed as a Kubernetes Operator Run:AI is installed in its own Kubernetes namespace named runai Workloads are run in the context of Projects . Each Project is a Kubernetes namespace with its own settings and access control. The Run:AI Cloud \u00b6 The Run:AI Cloud is the basis of the Administrator User Interface. The Run:AI cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:AI clusters.","title":"System Components"},{"location":"home/components/#runai-system-components","text":"","title":"Run:AI System Components"},{"location":"home/components/#components","text":"Run:AI is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:AI Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Administrator User Interface","title":"Components"},{"location":"home/components/#the-runai-cluster","text":"The Run:AI Cluster contains: The Run:AI Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. The Run:AI agent. Responsible for sending Monitoring data to the Run:AI Cloud. Clusters require outbound network connectivity to the Run:AI Cloud.","title":"The Run:AI Cluster"},{"location":"home/components/#kubernetes-related-details","text":"The Run:AI cluster is installed as a Kubernetes Operator Run:AI is installed in its own Kubernetes namespace named runai Workloads are run in the context of Projects . Each Project is a Kubernetes namespace with its own settings and access control.","title":"Kubernetes-Related Details"},{"location":"home/components/#the-runai-cloud","text":"The Run:AI Cloud is the basis of the Administrator User Interface. The Run:AI cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:AI clusters.","title":"The Run:AI Cloud"},{"location":"home/whats-new/","text":"December 28th, 2020 \u00b6 It is now possible to allocate a specific amount of GPU memory rather than use the fraction syntax. Use --gpu-memory=5G . December 15th, 2020 \u00b6 Project and Departments can now be set to not allocate resources beyond the assigned GPUs. This is useful for budget-conscious Projects/Departments. December 1st, 2020 \u00b6 New integration documents: Apache Airflow TensorBoard November 25th, 2020 \u00b6 Syntax changes in CLI: runai <object> list has been replaced by runai list <object> . runai get has been replaced by runai describe job . runai <object> set has been replaced by runai config <object> . The older style will still work with a deprecation notice. runai top node has been revamped. November 12th, 2020 \u00b6 An Admin can now create templates for the Command-line interface. Both a default template and specific templates that can be used with the --template flag. The new templates allow for mandatory values, defaults and run-time environment variable resolution. See here for more. It is now also possible to pass Secrets to Job. see here November 2nd, 2020 \u00b6 Several changes and additions to the Command-line interface: Passing a command and arguments is now done docker-style by adding -- at the end of the command You no longer need to provide a Job name . If you don't, a Job name will be generated automatically. You can also control the job-name prefix using an additional flag. New --image-pull-policy flag, allowing Researcher support for updating images without tagging. For further information see runai submit September 6th, 2020 \u00b6 We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Quickstart September 3rd, 2020 \u00b6 GPU Fractions now run in training and not only interactive. GPU Fractions training Job can be preempted, bin-packed and consolidated like any integer Job. See Run:AI Scheduler Fraction for more. August 10th, 2020 \u00b6 Run:AI Now supports Distributed Training and Gang Scheduling . For further information , see the Launch Distributed Training Workloads quickstart. August 4th, 2020 \u00b6 There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments July 28th, 2020 \u00b6 You can now enforce a cluster-wise setting that mandates all containers running using the Run:AI CLI to run as non root . For further information, see Enforce non-root Containers July 21th, 2020 \u00b6 It is now possible to mount a Persistent Storage Claim using the Run:AI CLI. See the --pvc flag in the runai submit CLI flag June 13th, 2020 \u00b6 New Settings for the Allocation of CPU and Memory \u00b6 It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory June 3rd, 2020 \u00b6 Node Group Affinity \u00b6 Projects now support Node Affinity. This feature allows the Administrator to assign specific Projects to run only on specific nodes (machines). Example use cases: The Project team needs specialized hardware (e.g. with enough memory) The Project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive Job. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Job and close them automatically. For further information on how to set up duration limits see: Working with Projects May 24th, 2020 \u00b6 Kubernetes Operators \u00b6 Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: Upgrading a Run:AI Cluster Installation and Deleting a a Run:AI Cluster Installation March 3rd, 2020 \u00b6 Admin Overview Dashboard \u00b6 A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Whats New"},{"location":"home/whats-new/#december-28th-2020","text":"It is now possible to allocate a specific amount of GPU memory rather than use the fraction syntax. Use --gpu-memory=5G .","title":"December 28th, 2020"},{"location":"home/whats-new/#december-15th-2020","text":"Project and Departments can now be set to not allocate resources beyond the assigned GPUs. This is useful for budget-conscious Projects/Departments.","title":"December 15th, 2020"},{"location":"home/whats-new/#december-1st-2020","text":"New integration documents: Apache Airflow TensorBoard","title":"December 1st, 2020"},{"location":"home/whats-new/#november-25th-2020","text":"Syntax changes in CLI: runai <object> list has been replaced by runai list <object> . runai get has been replaced by runai describe job . runai <object> set has been replaced by runai config <object> . The older style will still work with a deprecation notice. runai top node has been revamped.","title":"November 25th, 2020"},{"location":"home/whats-new/#november-12th-2020","text":"An Admin can now create templates for the Command-line interface. Both a default template and specific templates that can be used with the --template flag. The new templates allow for mandatory values, defaults and run-time environment variable resolution. See here for more. It is now also possible to pass Secrets to Job. see here","title":"November 12th, 2020"},{"location":"home/whats-new/#november-2nd-2020","text":"Several changes and additions to the Command-line interface: Passing a command and arguments is now done docker-style by adding -- at the end of the command You no longer need to provide a Job name . If you don't, a Job name will be generated automatically. You can also control the job-name prefix using an additional flag. New --image-pull-policy flag, allowing Researcher support for updating images without tagging. For further information see runai submit","title":"November 2nd, 2020"},{"location":"home/whats-new/#september-6th-2020","text":"We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Quickstart","title":"September 6th, 2020"},{"location":"home/whats-new/#september-3rd-2020","text":"GPU Fractions now run in training and not only interactive. GPU Fractions training Job can be preempted, bin-packed and consolidated like any integer Job. See Run:AI Scheduler Fraction for more.","title":"September 3rd, 2020"},{"location":"home/whats-new/#august-10th-2020","text":"Run:AI Now supports Distributed Training and Gang Scheduling . For further information , see the Launch Distributed Training Workloads quickstart.","title":"August 10th, 2020"},{"location":"home/whats-new/#august-4th-2020","text":"There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments","title":"August 4th, 2020"},{"location":"home/whats-new/#july-28th-2020","text":"You can now enforce a cluster-wise setting that mandates all containers running using the Run:AI CLI to run as non root . For further information, see Enforce non-root Containers","title":"July 28th, 2020"},{"location":"home/whats-new/#july-21th-2020","text":"It is now possible to mount a Persistent Storage Claim using the Run:AI CLI. See the --pvc flag in the runai submit CLI flag","title":"July 21th, 2020"},{"location":"home/whats-new/#june-13th-2020","text":"","title":"June 13th, 2020"},{"location":"home/whats-new/#new-settings-for-the-allocation-of-cpu-and-memory","text":"It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory","title":"New Settings for the Allocation of CPU and Memory"},{"location":"home/whats-new/#june-3rd-2020","text":"","title":"June 3rd, 2020"},{"location":"home/whats-new/#node-group-affinity","text":"Projects now support Node Affinity. This feature allows the Administrator to assign specific Projects to run only on specific nodes (machines). Example use cases: The Project team needs specialized hardware (e.g. with enough memory) The Project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects","title":"Node Group Affinity"},{"location":"home/whats-new/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive Job. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive Job and close them automatically. For further information on how to set up duration limits see: Working with Projects","title":"Limit Duration of Interactive Jobs"},{"location":"home/whats-new/#may-24th-2020","text":"","title":"May 24th, 2020"},{"location":"home/whats-new/#kubernetes-operators","text":"Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: Upgrading a Run:AI Cluster Installation and Deleting a a Run:AI Cluster Installation","title":"Kubernetes Operators"},{"location":"home/whats-new/#march-3rd-2020","text":"","title":"March 3rd, 2020"},{"location":"home/whats-new/#admin-overview-dashboard","text":"A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Admin Overview Dashboard"}]}