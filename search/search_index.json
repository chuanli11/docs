{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Run:AI \u00b6","title":"Home"},{"location":"#welcome-to-runai","text":"","title":"Welcome to Run:AI"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/","text":"Introduction \u00b6 The Admin User Interface allows: The setup of Kubernetes GPU Clusters. Create, Update and Delete of users Create, Update and Delete Projects. Review short term and long term dashboards Review Node and Job-status This document is about the Creation, Update, and Deletion of Users. Notes: With Run:AI you need to differentiate between the users of the Admin UI and Researcher users which submit workloads on the GPU Kubernetes cluster. This document is about the former. It is possible to connect the Admin UI users module to the organization's LDAP directory. For further information please contact Run:AI customer support. Working with Users \u00b6 Create User \u00b6 Note: to be able to manipulate users, you must have Administrator access. if you do not have such access, please contact an administrator. The list of administrators is shown on the Users page (see below) Log in to https://app.run.ai On the top left, open the menu and select \"Users\" On the top right, select \"Add New Users\". Choose a user name and email. Leave password as blank, it will be set by the user Select Roles. Note -- more than one role can be selected Select a Cluster. This determines the Clusters accessible to this user Press \"Save\" The user will receive a join mail and will be able to set a password. Update a User \u00b6 Select an existing User. Right-click and press \"Edit\" Update the values and press \"Save\" Delete an existing User \u00b6 Select an existing User. Right-click and press \"Delete\"","title":"Adding, Updating and Deleting Admin UI Users"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#introduction","text":"The Admin User Interface allows: The setup of Kubernetes GPU Clusters. Create, Update and Delete of users Create, Update and Delete Projects. Review short term and long term dashboards Review Node and Job-status This document is about the Creation, Update, and Deletion of Users. Notes: With Run:AI you need to differentiate between the users of the Admin UI and Researcher users which submit workloads on the GPU Kubernetes cluster. This document is about the former. It is possible to connect the Admin UI users module to the organization's LDAP directory. For further information please contact Run:AI customer support.","title":"Introduction"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#working-with-users","text":"","title":"Working with Users"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#create-user","text":"Note: to be able to manipulate users, you must have Administrator access. if you do not have such access, please contact an administrator. The list of administrators is shown on the Users page (see below) Log in to https://app.run.ai On the top left, open the menu and select \"Users\" On the top right, select \"Add New Users\". Choose a user name and email. Leave password as blank, it will be set by the user Select Roles. Note -- more than one role can be selected Select a Cluster. This determines the Clusters accessible to this user Press \"Save\" The user will receive a join mail and will be able to set a password.","title":"Create User"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#update-a-user","text":"Select an existing User. Right-click and press \"Edit\" Update the values and press \"Save\"","title":"Update a User"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#delete-an-existing-user","text":"Select an existing User. Right-click and press \"Delete\"","title":"Delete an existing User"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/","text":"Introduction \u00b6 Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create prioritize, Run:AI introduced the concept of __Projects. __Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Modeling Projects \u00b6 As an Admin, you need to determine how to model projects. You can: Set a project per user Set a project per team of users Set a project per a real organizational project. Project Quotas \u00b6 Each project is associated with a quota of GPUs that can be allocated for this project at the same time. This is __guaranteed quota __in the sense that researchers using this project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this project can receive an __over-quota. __As long as GPUs are unused, a researcher using this project can get more GPUs. However, these GPUs can be taken away at a moment's notice. Important best practice: As a rule, the sum of the project allocation should be equal to the number of GPUs in the cluster Working with Projects \u00b6 Create a new Project \u00b6 Note: to be able to manipulate projects, you must have Editor access. See the \"Users\" Area Log in to https://app.run.ai On the top left, open the menu and select \"Projects\" On the top right, select \"Add New Project\". Choose a project name and a project quota. Press \"Save\" Update an existing Project \u00b6 Select an existing project. Right-click and press \"Edit\" Update the values and press \"Save\" Delete an existing project \u00b6 Select an existing project. Right-click and press \"Delete\" Limit Jobs to run on Specific Node Groups \u00b6 A frequent use case is to assign specific projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization Grouping Nodes \u00b6 To set node affinities, you must first annotate nodes with labels. These labels will later be associated with projects. Each node can only be annotated with a __single __name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 Setting Affinity for a Specific Project \u00b6 To mandate training jobs to run on specific node groups: Create a Project or edit an existing Project. Select \"Limit to specific node groups If the label does not yet exist, press the + sign and add the label. Press Enter to save the label Select the label To mandate interactive jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the project dialog. Further Affinity Refinement by the Researcher \u00b6 The researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information https://support.run.ai/hc/en-us/articles/360011436120-runai-submit Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive jobs: Create a Project or edit an existing Project. At the bottom, set a limit (day, hour, minute) The setting only takes effect for jobs that have started after the duration has been changed.","title":"Working with Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#introduction","text":"Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create prioritize, Run:AI introduced the concept of __Projects. __Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state.","title":"Introduction"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#modeling-projects","text":"As an Admin, you need to determine how to model projects. You can: Set a project per user Set a project per team of users Set a project per a real organizational project.","title":"Modeling Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#project-quotas","text":"Each project is associated with a quota of GPUs that can be allocated for this project at the same time. This is __guaranteed quota __in the sense that researchers using this project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this project can receive an __over-quota. __As long as GPUs are unused, a researcher using this project can get more GPUs. However, these GPUs can be taken away at a moment's notice. Important best practice: As a rule, the sum of the project allocation should be equal to the number of GPUs in the cluster","title":"Project Quotas"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#working-with-projects","text":"","title":"Working with Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#create-a-new-project","text":"Note: to be able to manipulate projects, you must have Editor access. See the \"Users\" Area Log in to https://app.run.ai On the top left, open the menu and select \"Projects\" On the top right, select \"Add New Project\". Choose a project name and a project quota. Press \"Save\"","title":"Create a new Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#update-an-existing-project","text":"Select an existing project. Right-click and press \"Edit\" Update the values and press \"Save\"","title":"Update an existing Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#delete-an-existing-project","text":"Select an existing project. Right-click and press \"Delete\"","title":"Delete an existing project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#limit-jobs-to-run-on-specific-node-groups","text":"A frequent use case is to assign specific projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization","title":"Limit Jobs to run on Specific Node Groups"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#grouping-nodes","text":"To set node affinities, you must first annotate nodes with labels. These labels will later be associated with projects. Each node can only be annotated with a __single __name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2","title":"Grouping Nodes&nbsp;"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#setting-affinity-for-a-specific-project","text":"To mandate training jobs to run on specific node groups: Create a Project or edit an existing Project. Select \"Limit to specific node groups If the label does not yet exist, press the + sign and add the label. Press Enter to save the label Select the label To mandate interactive jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the project dialog.","title":"Setting Affinity for a Specific Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#further-affinity-refinement-by-the-researcher","text":"The researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information https://support.run.ai/hc/en-us/articles/360011436120-runai-submit","title":"Further Affinity Refinement by the Researcher"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive jobs: Create a Project or edit an existing Project. At the bottom, set a limit (day, hour, minute) The setting only takes effect for jobs that have started after the duration has been changed.","title":"Limit Duration of Interactive Jobs"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/","text":"Cluster Setup: Start Here \u00b6 Following is a step by step guide for setting up a new Run:AI cluster. Prerequisites \u00b6 Kubernetes Cluster Prerequisites \u00b6 Run:AI is running on top of Kubernetes. For a list of prerequisites, see GPU Cluster Prerquisites Installation \u00b6 For step by step installation instructions see : Installing Run:AI on an on-premise Kubernetes-Cluster Troubleshooting tips can be found here: https://support.run.ai/hc/en-us/articles/360010569960-Troubleshooting-a-Run-AI-installation Advanced Topics \u00b6 Setting up the Cluster to expose ports from containers \u00b6 There are various ways to allow researchers to expose ports from containers. Typical use cases are: Use a Jupyter Notebook, Work remotely with PyCharm, Use TensorBoard, and more. Exposing ports requires a pre-configuration of the Kubernetes Cluster. For more details see: https://support.run.ai/hc/en-us/articles/360011813620-Exposing-Ports-from-Researcher-Containers Settings up Admin UI Authentication and Authorization \u00b6 You may want to set up authentication (user login) and authorization (granular access control). Without which, any researcher can access and change the workloads of others. For further details on how to set up researcher authentication and authorization see: https://support.run.ai/hc/en-us/articles/360011912339-Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization- Setting up a Run:AI to work with an Internet Proxy Server \u00b6 In some organizations, outbound connectivity is proxied. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. To setup Run:AI to work with a proxy server see: https://support.run.ai/hc/en-us/articles/360014226400-Installing-Run-AI-with-an-Internet-Proxy-Server- Next Steps \u00b6 After setting up the cluster, you may want to start setting up researchers. See: https://support.run.ai/hc/en-us/articles/360012060639-Researcher-Setup-Start-Here","title":"Cluster Setup - Start Here"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#cluster-setup-start-here","text":"Following is a step by step guide for setting up a new Run:AI cluster.","title":"Cluster Setup: Start Here"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#prerequisites","text":"","title":"Prerequisites"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#kubernetes-cluster-prerequisites","text":"Run:AI is running on top of Kubernetes. For a list of prerequisites, see GPU Cluster Prerquisites","title":"Kubernetes Cluster Prerequisites"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#installation","text":"For step by step installation instructions see : Installing Run:AI on an on-premise Kubernetes-Cluster Troubleshooting tips can be found here: https://support.run.ai/hc/en-us/articles/360010569960-Troubleshooting-a-Run-AI-installation","title":"Installation"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#setting-up-the-cluster-to-expose-ports-from-containers","text":"There are various ways to allow researchers to expose ports from containers. Typical use cases are: Use a Jupyter Notebook, Work remotely with PyCharm, Use TensorBoard, and more. Exposing ports requires a pre-configuration of the Kubernetes Cluster. For more details see: https://support.run.ai/hc/en-us/articles/360011813620-Exposing-Ports-from-Researcher-Containers","title":"Setting up the Cluster to expose ports from containers"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#settings-up-admin-ui-authentication-and-authorization","text":"You may want to set up authentication (user login) and authorization (granular access control). Without which, any researcher can access and change the workloads of others. For further details on how to set up researcher authentication and authorization see: https://support.run.ai/hc/en-us/articles/360011912339-Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-","title":"Settings up Admin UI Authentication and Authorization"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#setting-up-a-runai-to-work-with-an-internet-proxy-server","text":"In some organizations, outbound connectivity is proxied. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. To setup Run:AI to work with a proxy server see: https://support.run.ai/hc/en-us/articles/360014226400-Installing-Run-AI-with-an-Internet-Proxy-Server-","title":"Setting up a Run:AI to work with an Internet Proxy Server"},{"location":"Administrator/Cluster-Setup/Cluster-Setup-Start-Here/#next-steps","text":"After setting up the cluster, you may want to start setting up researchers. See: https://support.run.ai/hc/en-us/articles/360012060639-Researcher-Setup-Start-Here","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/","text":"Introduction \u00b6 There are a number of cases where you want to expose services running on the cluster. A few of them are: Allow Researchers who work with containers to expose ports to access the container from remote using tools such as a Jupyter notebook or PyCharm Integrate a Researcher authentication mechanism such as an organizational user directory. The Kubernetes mechanism for exposing services is called Ingress. Ingress \u00b6 Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here Requirements \u00b6 Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster. Ingress Configuration \u00b6 A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a yaml file runai-operator.yaml _and then _applying it to Kubernetes. You must edit the yaml file. Search for localLoadBalancer localLoadBalancer enabled: true ipRangeFrom: 10.0.2.1 ipRangeTo: 10.0.2.2 Set enabled to true and set the IP range appropriately. \u00b6","title":"Exposing Cluster Services via Ingress"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/#introduction","text":"There are a number of cases where you want to expose services running on the cluster. A few of them are: Allow Researchers who work with containers to expose ports to access the container from remote using tools such as a Jupyter notebook or PyCharm Integrate a Researcher authentication mechanism such as an organizational user directory. The Kubernetes mechanism for exposing services is called Ingress.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/#ingress","text":"Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here","title":"Ingress"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/#requirements","text":"Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster.","title":"Requirements"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/#ingress-configuration","text":"A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a yaml file runai-operator.yaml _and then _applying it to Kubernetes. You must edit the yaml file. Search for localLoadBalancer localLoadBalancer enabled: true ipRangeFrom: 10.0.2.1 ipRangeTo: 10.0.2.2 Set enabled to true and set the IP range appropriately.","title":"Ingress Configuration"},{"location":"Administrator/Cluster-Setup/Exposing-Cluster-Services-via-Ingress/#_1","text":"","title":"&nbsp;"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/","text":"Introduction \u00b6 Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress Ingress \u00b6 Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here To configure ingress see: https://support.run.ai/hc/en-us/articles/360013847900-Exposing-Cluster-Services-via-Ingress Usage \u00b6 The researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 -p team-ny \\ --interactive --service-type=ingress --port 8888:8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh After submitting a job through the Run:AI CLI, run: runai list You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the job name and the port (e.g. https://10.255.174.13/test-ingress-8888 For further details see CLI reference and walkthrough . Alternatives \u00b6 Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> See https://kubernetes.io/docs/concepts/services-networking/service/ for further details See Also \u00b6 To learn how to use port forwarding see: https://support.run.ai/hc/en-us/articles/360011131919-Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports","title":"Exposing Ports from Researcher Containers using Ingress"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/#introduction","text":"Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress","title":"Introduction"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/#ingress","text":"Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here To configure ingress see: https://support.run.ai/hc/en-us/articles/360013847900-Exposing-Cluster-Services-via-Ingress","title":"Ingress"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/#usage","text":"The researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 -p team-ny \\ --interactive --service-type=ingress --port 8888:8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh After submitting a job through the Run:AI CLI, run: runai list You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the job name and the port (e.g. https://10.255.174.13/test-ingress-8888 For further details see CLI reference and walkthrough .","title":"Usage"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/#alternatives","text":"Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> See https://kubernetes.io/docs/concepts/services-networking/service/ for further details","title":"Alternatives"},{"location":"Administrator/Cluster-Setup/Exposing-Ports-from-Researcher-Containers-using-Ingress/#see-also","text":"To learn how to use port forwarding see: https://support.run.ai/hc/en-us/articles/360011131919-Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports","title":"See Also"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/","text":"The following are instructions on how to install Run:AI on the customer Kubernetes Cluster. Prior to installation please review the installation prerequisites here: https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites Step 1: Install Kubernetes \u00b6 Installing Kubernetes is beyond the scope of this guide. There are plenty of good ways to install Kubernetes (listed here: https://kubernetes.io/docs/setup/>>>). We recommend Kubespray https://kubespray.io/#/ . Download the latest __stable__ version from <<<FLOATING LINK: https://github.com/kubernetes-sigs/kubespray . Some best practices on Kubernetes Configuration can be found here: https://support.run.ai/hc/en-us/articles/360015302379-Kubernetes-Cluster-Configuration-Best-Practices The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to the Kubernetes cluster (by running kubectl config use-context <name> ) Step 2: NVIDIA \u00b6 On each machine with GPUs run the following steps 2.1 - 2.3: Step 2.1 Install NVIDIA Drivers \u00b6 If NVIDIA drivers are not already installed on your GPU machines, please install them now. Note that on original NVIDIA hardware, these drivers are already installed by default. Step 2.2: Install NVIDIA Docker \u00b6 Run the following: distribution= $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Step 2.3: Make NVIDIA Docker the default docker runtime \u00b6 You will need to enable the Nvidia runtime as your default docker runtime on your node. We will be editing the docker daemon config file which is usually present at /etc/docker/daemon.json : { \" default-runtime \" : \" nvidia \" , \" runtimes \" : { \" nvidia \" : { \" path \" : \" /usr/bin/nvidia-container-runtime \" , \" runtimeArgs \" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd Note : Run:AI is customizing the NVIDIA device plugin ( https://github.com/NVIDIA/k8s-device-plugin ). Do __not __install this software as it is installed by Run:AI. Step 3: Install Run:AI \u00b6 Log in to Run:AI at https://app.run.ai. Use credentials provided by Run:AI Customer Support to log in to the system If this is the first time anyone from your company has logged in, you will receive a dialog with instructions on how to install Run:AI on your Kubernetes Cluster. If not, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Continue according to instructions to install Run:AI on your Kubernetes Cluster Step 4: Verifying your Installation \u00b6 Go to https://app.run.ai Go to the Overview Dashboard Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line Next Steps \u00b6 Researchers work via a Command-line interface (CLI). See https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface on how to install the CLI for users Set up Admin UI Users. https://support.run.ai/hc/en-us/articles/360011591340-Working-with-Admin-UI-Users","title":"Installing Run:AI on an on-premise Kubernetes Cluster"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-1-install-kubernetes","text":"Installing Kubernetes is beyond the scope of this guide. There are plenty of good ways to install Kubernetes (listed here: https://kubernetes.io/docs/setup/>>>). We recommend Kubespray https://kubespray.io/#/ . Download the latest __stable__ version from <<<FLOATING LINK: https://github.com/kubernetes-sigs/kubespray . Some best practices on Kubernetes Configuration can be found here: https://support.run.ai/hc/en-us/articles/360015302379-Kubernetes-Cluster-Configuration-Best-Practices The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to the Kubernetes cluster (by running kubectl config use-context <name> )","title":"Step 1: Install Kubernetes"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-2-nvidia","text":"On each machine with GPUs run the following steps 2.1 - 2.3:","title":"Step 2: NVIDIA"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-21-install-nvidia-drivers","text":"If NVIDIA drivers are not already installed on your GPU machines, please install them now. Note that on original NVIDIA hardware, these drivers are already installed by default.","title":"Step 2.1 Install NVIDIA Drivers"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-22-install-nvidia-docker","text":"Run the following: distribution= $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd","title":"Step 2.2: Install NVIDIA Docker"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-23-make-nvidia-docker-the-default-docker-runtime","text":"You will need to enable the Nvidia runtime as your default docker runtime on your node. We will be editing the docker daemon config file which is usually present at /etc/docker/daemon.json : { \" default-runtime \" : \" nvidia \" , \" runtimes \" : { \" nvidia \" : { \" path \" : \" /usr/bin/nvidia-container-runtime \" , \" runtimeArgs \" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd Note : Run:AI is customizing the NVIDIA device plugin ( https://github.com/NVIDIA/k8s-device-plugin ). Do __not __install this software as it is installed by Run:AI.","title":"Step 2.3: Make NVIDIA Docker the default docker runtime"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-3-install-runai","text":"Log in to Run:AI at https://app.run.ai. Use credentials provided by Run:AI Customer Support to log in to the system If this is the first time anyone from your company has logged in, you will receive a dialog with instructions on how to install Run:AI on your Kubernetes Cluster. If not, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Continue according to instructions to install Run:AI on your Kubernetes Cluster","title":"Step 3: Install Run:AI"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#step-4-verifying-your-installation","text":"Go to https://app.run.ai Go to the Overview Dashboard Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line","title":"Step 4: Verifying your Installation"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster/#next-steps","text":"Researchers work via a Command-line interface (CLI). See https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface on how to install the CLI for users Set up Admin UI Users. https://support.run.ai/hc/en-us/articles/360011591340-Working-with-Admin-UI-Users","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-with-an-Internet-Proxy-Server-/","text":"Introduction \u00b6 Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\" In some organizations, outbound connectivity is proxied. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work. Run:AI Configuration \u00b6 The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a yaml file runai-operator.yaml and then applying it to Kubernetes. You must edit the yaml file. Search for httpProxy global: httpProxy: enabled: false tlsCert: |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true and paste the contents of the certificate under tlsCert","title":"Installing Run:AI with an Internet Proxy Server "},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-with-an-Internet-Proxy-Server-/#introduction","text":"Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\" In some organizations, outbound connectivity is proxied. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/Installing-Run-AI-with-an-Internet-Proxy-Server-/#runai-configuration","text":"The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a yaml file runai-operator.yaml and then applying it to Kubernetes. You must edit the yaml file. Search for httpProxy global: httpProxy: enabled: false tlsCert: |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true and paste the contents of the certificate under tlsCert","title":"Run:AI Configuration"},{"location":"Administrator/Cluster-Setup/Kubernetes-Cluster-Configuration-Best-Practices/","text":"Node Memory Management \u00b6 It is possible for researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine). Symptoms \u00b6 The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue enhances on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes: Resource Reservation \u00b6 Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup. This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved: cpu: 100m memory: 1G systemReserved: cpu: 100m memory: 1G Eviction \u00b6 Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard: memory.available: \"500Mi\" For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/","title":"Kubernetes Cluster Configuration Best Practices"},{"location":"Administrator/Cluster-Setup/Kubernetes-Cluster-Configuration-Best-Practices/#node-memory-management","text":"It is possible for researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine).","title":"Node Memory Management"},{"location":"Administrator/Cluster-Setup/Kubernetes-Cluster-Configuration-Best-Practices/#symptoms","text":"The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue enhances on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes:","title":"Symptoms"},{"location":"Administrator/Cluster-Setup/Kubernetes-Cluster-Configuration-Best-Practices/#resource-reservation","text":"Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup. This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved: cpu: 100m memory: 1G systemReserved: cpu: 100m memory: 1G","title":"Resource Reservation"},{"location":"Administrator/Cluster-Setup/Kubernetes-Cluster-Configuration-Best-Practices/#eviction","text":"Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard: memory.available: \"500Mi\" For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/","title":"Eviction"},{"location":"Administrator/Cluster-Setup/Outbound-URL-Access-Requirements/","text":"Document has been merged into: https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites","title":"Outbound URL Access Requirements"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/","text":"Important note : This document relates to the cloud version of Run:AI and discusses the prerequisites for the GPU Cluster. Kubernetes Software \u00b6 Run:AI requires Kubernetes 1.15 or above. Kubernetes 1.17 is recommended (as of June 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16 NVIDIA Driver \u00b6 Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency. Hardware Requirements \u00b6 Kubernetes: Dedicated CPU-only machine: To save on expensive GPUs-based hardware, we recommend a dedicated, CPU-only machine, that is not running user workloads. Run:AI requires the following resources on top of the Kubernetes hardware requirements 2GB of RAM 20GB of Disk space Shared data volume: Run:AI, via Kubernetes, abstracts away the machine on which a container is running. For containers to run anywhere, they need to be able to access data from any machine in a uniform way. Typically, this requires a NAS (Network-attached storage) which allows any node to connect to storage outside the box. Network Requirements \u00b6 Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied: During Installation \u00b6 Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443 Post Installation \u00b6 In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443 User requirements \u00b6 Usage of containers and images: The individual researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software Fractional GPU Requirements \u00b6 The Run:AI platform provides a unique technology that allows the sharing of a single GPU between multiple containers. Each container receives an isolated subset of the GPU memory. For more details see https://support.run.ai/hc/en-us/articles/360014989740-Walkthrough-Using-GPU-Fractions. This technology has more stringent software requirements than the rest of the Run:AI system. Specifically, virtualization has been tested on: NVIDIA device driver 418 or later Cuda 9.0 or later TensorFlow, Keras or Pytorch We keep testing the technology on additional software.","title":"Run:AI GPU Cluster Prerequisites"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#kubernetes-software","text":"Run:AI requires Kubernetes 1.15 or above. Kubernetes 1.17 is recommended (as of June 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16","title":"Kubernetes Software"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#nvidia-driver","text":"Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency.","title":"NVIDIA Driver"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#hardware-requirements","text":"Kubernetes: Dedicated CPU-only machine: To save on expensive GPUs-based hardware, we recommend a dedicated, CPU-only machine, that is not running user workloads. Run:AI requires the following resources on top of the Kubernetes hardware requirements 2GB of RAM 20GB of Disk space Shared data volume: Run:AI, via Kubernetes, abstracts away the machine on which a container is running. For containers to run anywhere, they need to be able to access data from any machine in a uniform way. Typically, this requires a NAS (Network-attached storage) which allows any node to connect to storage outside the box.","title":"Hardware Requirements"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#network-requirements","text":"Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied:","title":"Network Requirements"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#during-installation","text":"Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443","title":"During Installation"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#post-installation","text":"In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443","title":"Post Installation"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#user-requirements","text":"Usage of containers and images: The individual researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software","title":"User requirements"},{"location":"Administrator/Cluster-Setup/Run-AI-GPU-Cluster-Prerequisites/#fractional-gpu-requirements","text":"The Run:AI platform provides a unique technology that allows the sharing of a single GPU between multiple containers. Each container receives an isolated subset of the GPU memory. For more details see https://support.run.ai/hc/en-us/articles/360014989740-Walkthrough-Using-GPU-Fractions. This technology has more stringent software requirements than the rest of the Run:AI system. Specifically, virtualization has been tested on: NVIDIA device driver 418 or later Cuda 9.0 or later TensorFlow, Keras or Pytorch We keep testing the technology on additional software.","title":"Fractional GPU Requirements"},{"location":"Administrator/Cluster-Setup/Troubleshooting-a-Run-AI-Cluster-Installation/","text":"Pods are not created \u00b6 run: kubectl get pods -n runai You will get a list of running \"pods\". All pods should be with status Running or Completed . There could be various reasons why pods are at a different status. Most notably, pods use Run:AI images that are downloaded from the web. If your company employs an outbound firewall, you may need to open the URLs and ports mentioned here GPU related metrics are not shown In the Admin portal ( app.run.ai ) Metrics such as \"number of GPUs\" and \"GPU utilization\" do not show This typically means that there is a disconnect between the Kubernetes pods that require access to GPUs and the NVIDIA software. This could happen if: Connecting GPU data requires a Kubernetes feature gate flag called KubeletPodResources. This feature gate is a default in Kubernetes 1.15, but must be added in older versions of Kubernetes. See https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ . Note that Run:AI has been tested on Kubernetes 1.15. Nvidia perquisites are not installed. See https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites for NVIDIA related prerequisites. To verify whether GPU metrics are exported run: kubectl port-forward -n runai prometheus-runai-prometheus-operator-prometheus-0 9090 Then using your browser go to http://localhost:9090/ . Verify that you see metrics for dcgm_gpu_utilization If no metrics are shown, you can get to the root cause by running: kubectl get pods -n runai --selector=app=pod-gpu-metrics-exporter There should be one GPU metrics exporter per node. For each pod run: kubectl logs -n runai <name> -c pod-nvidia-gpu-metrics-exporter Where <name> is the pod-gpu-metrics exporter names from above. The logs should contain further data about the issue A typical issue may be skipping the prerequisite of installing the Nvidia driver plugin. See here .","title":"Troubleshooting a Run:AI Cluster Installation"},{"location":"Administrator/Cluster-Setup/Troubleshooting-a-Run-AI-Cluster-Installation/#pods-are-not-created","text":"run: kubectl get pods -n runai You will get a list of running \"pods\". All pods should be with status Running or Completed . There could be various reasons why pods are at a different status. Most notably, pods use Run:AI images that are downloaded from the web. If your company employs an outbound firewall, you may need to open the URLs and ports mentioned here GPU related metrics are not shown In the Admin portal ( app.run.ai ) Metrics such as \"number of GPUs\" and \"GPU utilization\" do not show This typically means that there is a disconnect between the Kubernetes pods that require access to GPUs and the NVIDIA software. This could happen if: Connecting GPU data requires a Kubernetes feature gate flag called KubeletPodResources. This feature gate is a default in Kubernetes 1.15, but must be added in older versions of Kubernetes. See https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ . Note that Run:AI has been tested on Kubernetes 1.15. Nvidia perquisites are not installed. See https://support.run.ai/hc/en-us/articles/360010227960-Run-AI-GPU-Cluster-Prerequisites for NVIDIA related prerequisites. To verify whether GPU metrics are exported run: kubectl port-forward -n runai prometheus-runai-prometheus-operator-prometheus-0 9090 Then using your browser go to http://localhost:9090/ . Verify that you see metrics for dcgm_gpu_utilization If no metrics are shown, you can get to the root cause by running: kubectl get pods -n runai --selector=app=pod-gpu-metrics-exporter There should be one GPU metrics exporter per node. For each pod run: kubectl logs -n runai <name> -c pod-nvidia-gpu-metrics-exporter Where <name> is the pod-gpu-metrics exporter names from above. The logs should contain further data about the issue A typical issue may be skipping the prerequisite of installing the Nvidia driver plugin. See here .","title":"Pods are not created"},{"location":"Administrator/Cluster-Setup/Upgrading-and-Deleting-a-Run-AI-Cluster-Installation/","text":"Upgrading a Run:AI Cluster Installation \u00b6 To upgrade a Run:AI cluster installation you must get a version number from Run:AI customer support. Then, run: kubectl set image -n runai deployment/runai-operator \\ runai-operator=gcr.io/run-ai-prod/operator:NEW_VERSION To verify that the upgrade has succeeded run: kubectl get pods -n runai and make sure that all pods are running or completed. Deleting a Run:AI Cluster Installation To delete a Run:AI Cluster installation run the following commands: kubectl delete RunaiConfig runai -n runai kubectl delete deployment runai-operator -n runai kubectl delete crd runaiconfigs.run.ai kubectl delete namespace runai","title":"Upgrading and Deleting a Run:AI Cluster Installation"},{"location":"Administrator/Cluster-Setup/Upgrading-and-Deleting-a-Run-AI-Cluster-Installation/#upgrading-a-runai-cluster-installation","text":"To upgrade a Run:AI cluster installation you must get a version number from Run:AI customer support. Then, run: kubectl set image -n runai deployment/runai-operator \\ runai-operator=gcr.io/run-ai-prod/operator:NEW_VERSION To verify that the upgrade has succeeded run: kubectl get pods -n runai and make sure that all pods are running or completed.","title":"Upgrading a Run:AI Cluster Installation"},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/","text":"Introduction \u00b6 Run:AI uses its a mechanism for authentication and authorization which is based on a third-party ( auth0 ). This is good as a baseline, but for enterprises, such a scheme is not scalable. For an enterprise, keeping separate users and roles mechanisms requires manual work, is error-prone, and increases the attack vector. As such, organizations typically use an organizational directory to store users and roles, allowing a single point of change for multiple systems Run:AI uses the OpenID Connect protocol to allow organizations to integrate their authentication & authorization system with Run:AI. With such a connector, Run:AI no longer has a standalone login page. instead, it differs to the organization's directory for authenticating users and for retrieving their roles (authorization) OpenID provides simple wrappers for LDAP and SAML. LDAP and SAML are similar protocols. Most notably, LDAP which is the underlying protocol for Microsoft Active Directory as well as other directories. OpenID Connect Configuration \u00b6 With Run:AI OpenID Connect you synchronize: Users Users' groups The Run:AI login page is app.run.ai and is the point of access to all Run:AI customers using the default login mechanism. When enabling the Run:AI OpenID connector, your company will be allocated a subdomain e.g. company.app.run.ai. When the user is not yet authenticated, company.app.run.ai will automatically redirect to your generic company's authentication page. Post authentication, the user will be redirected back to company.app.run.ai and can start working. Installation and Configuration \u00b6 Your company will need to create an OpenID Connect provider. We recommend dex. \u00b6 After installing dex, you will want to create a client and perform the following configuration: Enter a redirect URL which has been provided to you by Run:AI Generate a unique secret . The secret should be sent to Run:AI If you are using LDAP or SAML, configure the relevant connector for dex. Locate the authentication redirection URL . The redirection URL should to be sent to Run:AI Create a public key in order for Run:AI to be able validate oauth tokens. The public key should be sent to Run:AI Users and Roles \u00b6 Now, go to the authorization page on Run:AI app and configure the required authorization using either specific users or groups in your organization.","title":"Use OpenID Connect, LDAP or SAML for Authentication and Authorization "},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/#introduction","text":"Run:AI uses its a mechanism for authentication and authorization which is based on a third-party ( auth0 ). This is good as a baseline, but for enterprises, such a scheme is not scalable. For an enterprise, keeping separate users and roles mechanisms requires manual work, is error-prone, and increases the attack vector. As such, organizations typically use an organizational directory to store users and roles, allowing a single point of change for multiple systems Run:AI uses the OpenID Connect protocol to allow organizations to integrate their authentication & authorization system with Run:AI. With such a connector, Run:AI no longer has a standalone login page. instead, it differs to the organization's directory for authenticating users and for retrieving their roles (authorization) OpenID provides simple wrappers for LDAP and SAML. LDAP and SAML are similar protocols. Most notably, LDAP which is the underlying protocol for Microsoft Active Directory as well as other directories.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/#openid-connect-configuration","text":"With Run:AI OpenID Connect you synchronize: Users Users' groups The Run:AI login page is app.run.ai and is the point of access to all Run:AI customers using the default login mechanism. When enabling the Run:AI OpenID connector, your company will be allocated a subdomain e.g. company.app.run.ai. When the user is not yet authenticated, company.app.run.ai will automatically redirect to your generic company's authentication page. Post authentication, the user will be redirected back to company.app.run.ai and can start working.","title":"OpenID Connect Configuration"},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/#installation-and-configuration","text":"","title":"Installation and Configuration"},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/#your-company-will-need-tocreate-an-openid-connect-providerwe-recommend-dex","text":"After installing dex, you will want to create a client and perform the following configuration: Enter a redirect URL which has been provided to you by Run:AI Generate a unique secret . The secret should be sent to Run:AI If you are using LDAP or SAML, configure the relevant connector for dex. Locate the authentication redirection URL . The redirection URL should to be sent to Run:AI Create a public key in order for Run:AI to be able validate oauth tokens. The public key should be sent to Run:AI","title":"Your company will need to&nbsp;create an OpenID Connect provider.&nbsp;We recommend dex."},{"location":"Administrator/Cluster-Setup/Use-OpenID-Connect-LDAP-or-SAML-for-Authentication-and-Authorization-/#users-and-roles","text":"Now, go to the authorization page on Run:AI app and configure the required authorization using either specific users or groups in your organization.","title":"Users and Roles"},{"location":"Administrator/Miscellaneous-/Launch-Workloads-using-Rancher-User-Interface/","text":"Rancher ( https://rancher.com/ ) is a software that manages Kubernetes clusters. Some customers provide Rancher to data scientists in order to launch workloads. This guide provides step by step instructions on how to launch workloads via Rancher. It assumes the reader has some familiarity with Rancher itself. There are other ways for data scientists to launch Workloads such as the Run:AI CLI or KubeFlow ( https://www.kubeflow.org/ ). The advantage of Rancher is the usage of a user interface. The disadvantage is that it exposes the data scientist to Kubernetes/Docker terminology that would otherwise remain hidden Types of Workloads \u00b6 We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Train session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports Terminology \u00b6 Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI projects\" (not to be confused with Rancher Projects). Each project contains a GPU quota. Each workload must be annotated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads Using Rancher to Launch Workloads \u00b6 Using your browser, navigate to Rancher Login to Rancher with your user name and password. Click on the top left menu, go to the company's assigned cluster and default Rancher project (not to be confused with a Run:AI project). Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud environments this would typically be a Layer-4 load balancer. On-premise environments depend on the how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" On the bottom and select \"show advanced options\". Expand labels and labels and add 2 labels, adding the name of the user and the name of the project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Launch Workloads using Rancher User Interface"},{"location":"Administrator/Miscellaneous-/Launch-Workloads-using-Rancher-User-Interface/#types-of-workloads","text":"We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Train session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports","title":"Types of Workloads&nbsp;"},{"location":"Administrator/Miscellaneous-/Launch-Workloads-using-Rancher-User-Interface/#terminology","text":"Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI projects\" (not to be confused with Rancher Projects). Each project contains a GPU quota. Each workload must be annotated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads","title":"Terminology"},{"location":"Administrator/Miscellaneous-/Launch-Workloads-using-Rancher-User-Interface/#using-rancher-to-launch-workloads","text":"Using your browser, navigate to Rancher Login to Rancher with your user name and password. Click on the top left menu, go to the company's assigned cluster and default Rancher project (not to be confused with a Run:AI project). Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud environments this would typically be a Layer-4 load balancer. On-premise environments depend on the how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" On the bottom and select \"show advanced options\". Expand labels and labels and add 2 labels, adding the name of the user and the name of the project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Using Rancher to Launch Workloads&nbsp;"},{"location":"Administrator/Presentations/Administrator-Onboarding-Presentation/","text":"","title":"Administrator Onboarding Presentation"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/","text":"What are Templates? \u00b6 Templates are a way to reduce the number of flags required when using the Command Line Interface to start workloads. Using Templates the researcher can: Review list of templates by running runai template list Review the contents of a specific template by running_ runai template get <template-name>_ Use a template by running runai submit --template <template-name> The purpose of this document is to provide the administrator with guidelines on how to create templates The Template Implementation \u00b6 CLI Templates are implemented as_ Kubernetes ConfigMaps . _ConfigMaps in Kubernetes are a standard way to save cluster-wide settings. To create a Run:AI CLI Template, you will need to save a ConfigMap on the cluster. The Run:AI CLI is then looking for ConfigMaps marked as Run:AI templates. Template Usage The Researcher can use the Run:AI CLI to Show a list of templates: runai template list Showing the properties of a specific template: runai template get <my-template> Use the template when submitting a workload runai submit <my-job> --template <my-template> For further details, see the Run:AI command line reference template and submit functions Template Syntax \u00b6 A template looks as follows: apiVersion: v1 kind: ConfigMap data: name: cli-template1 description: \"my first template\" values: | image: gcr.io/run-ai-lab/quickstart gpu: 1 elastic: true metadata: name: cli-template1 namespace: runai labels: runai/template: \"true\" Notes: The template above set 3 defaults: a specific image, a default of 1 gpu and sets the \"elastic\" flag to true The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the_ runai template list_ command To store this template run: kubectl apply -f <template-file-name> For a complete list of template values, see the end of this document The Default Template \u00b6 The administrator can also set a default template that is always used on runai submit whenever a template is not specified. To create a default template use the annotation runai/default : \"true\". Example: apiVersion: v1 kind: ConfigMap data: name: cli-template1 description: \"my first template\" values: | volume: - '/path/on/host:/dest/container/path' metadata: name: cli-template1 namespace: runai annotations: runai/default: \"true\" labels: runai/template: \"true\" Sets a default template which mounts a volume Syntax of all Values \u00b6 The following template sets all runai submit flags. apiVersion: v1 kind: ConfigMap data: name: \"all-cli-flags\" description: \"A sample showing all possible flag overrides via a template\" values: | project: \"team-ny\" image: ubuntu gpu: 1 cpu: 0.5 memory: 1G elastic: false interactive: true node_type: \"dgx-1\" # --node-type hostIPC: false # --host-ipc shm: false # --large-shm localImage: false # --local-image serviceType: \"loadbalancer\" # -- service-type ttlSecondsAfterFinished: 30 # --ttl-after-finish environmentDefault: - 'LEARNING_RATE=0.25' - 'TEMP=302' portsDefault: - '80:8888' - 9090 command: - 'sleep' args: - 'infinity' volumeDefault: - '/etc:/dest/container/path' workingDir: \"/etc\". # --working-dir metadata: name: cli-all-flags namespace: runai labels: runai/template: \"true\"","title":"Configure Command Line Interface Templates"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/#what-are-templates","text":"Templates are a way to reduce the number of flags required when using the Command Line Interface to start workloads. Using Templates the researcher can: Review list of templates by running runai template list Review the contents of a specific template by running_ runai template get <template-name>_ Use a template by running runai submit --template <template-name> The purpose of this document is to provide the administrator with guidelines on how to create templates","title":"What are Templates?"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/#the-template-implementation","text":"CLI Templates are implemented as_ Kubernetes ConfigMaps . _ConfigMaps in Kubernetes are a standard way to save cluster-wide settings. To create a Run:AI CLI Template, you will need to save a ConfigMap on the cluster. The Run:AI CLI is then looking for ConfigMaps marked as Run:AI templates. Template Usage The Researcher can use the Run:AI CLI to Show a list of templates: runai template list Showing the properties of a specific template: runai template get <my-template> Use the template when submitting a workload runai submit <my-job> --template <my-template> For further details, see the Run:AI command line reference template and submit functions","title":"The Template Implementation"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/#template-syntax","text":"A template looks as follows: apiVersion: v1 kind: ConfigMap data: name: cli-template1 description: \"my first template\" values: | image: gcr.io/run-ai-lab/quickstart gpu: 1 elastic: true metadata: name: cli-template1 namespace: runai labels: runai/template: \"true\" Notes: The template above set 3 defaults: a specific image, a default of 1 gpu and sets the \"elastic\" flag to true The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the_ runai template list_ command To store this template run: kubectl apply -f <template-file-name> For a complete list of template values, see the end of this document","title":"Template Syntax"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/#the-default-template","text":"The administrator can also set a default template that is always used on runai submit whenever a template is not specified. To create a default template use the annotation runai/default : \"true\". Example: apiVersion: v1 kind: ConfigMap data: name: cli-template1 description: \"my first template\" values: | volume: - '/path/on/host:/dest/container/path' metadata: name: cli-template1 namespace: runai annotations: runai/default: \"true\" labels: runai/template: \"true\" Sets a default template which mounts a volume","title":"The Default Template"},{"location":"Administrator/Researcher-Setup/Configure-Command-Line-Interface-Templates/#syntax-of-all-values","text":"The following template sets all runai submit flags. apiVersion: v1 kind: ConfigMap data: name: \"all-cli-flags\" description: \"A sample showing all possible flag overrides via a template\" values: | project: \"team-ny\" image: ubuntu gpu: 1 cpu: 0.5 memory: 1G elastic: false interactive: true node_type: \"dgx-1\" # --node-type hostIPC: false # --host-ipc shm: false # --large-shm localImage: false # --local-image serviceType: \"loadbalancer\" # -- service-type ttlSecondsAfterFinished: 30 # --ttl-after-finish environmentDefault: - 'LEARNING_RATE=0.25' - 'TEMP=302' portsDefault: - '80:8888' - 9090 command: - 'sleep' args: - 'infinity' volumeDefault: - '/etc:/dest/container/path' workingDir: \"/etc\". # --working-dir metadata: name: cli-all-flags namespace: runai labels: runai/template: \"true\"","title":"Syntax of all Values"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/","text":"The Run:AI Command-line Interface (CLI) is one of the ways for a researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI Prerequisites \u00b6 Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ Helm. See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster Installation \u00b6 Kubernetes Configuration \u00b6 The Run:AI CLI needs to be connected to the Kubernetes Cluster containing the GPU nodes: Create a directory .kube . Copy the Kubernetes configuration file into the directory Create a shell variable to point to the above configuration file. Example: export KUBECONFIG=~/.kube/config Test the connection by running: kubectl get nodes Run:AI CLI Installation \u00b6 Download the latest release from the Run:AI releases page https://github.com/run-ai/runai-cli/releases Unarchive the downloaded file. Install by running: sudo ./install-runai.sh To verify the installation run: runai list Troubleshooting the CLI Installation \u00b6 See: Troubleshooting-a-CLI-installation Updating the Run:AI CLI \u00b6 To update the CLI to the latest version run: sudo runai update","title":"Installing the Run:AI Command Line Interface"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#prerequisites","text":"Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ Helm. See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster","title":"Prerequisites"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#installation","text":"","title":"Installation"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#kubernetes-configuration","text":"The Run:AI CLI needs to be connected to the Kubernetes Cluster containing the GPU nodes: Create a directory .kube . Copy the Kubernetes configuration file into the directory Create a shell variable to point to the above configuration file. Example: export KUBECONFIG=~/.kube/config Test the connection by running: kubectl get nodes","title":"Kubernetes Configuration"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#runai-cli-installation","text":"Download the latest release from the Run:AI releases page https://github.com/run-ai/runai-cli/releases Unarchive the downloaded file. Install by running: sudo ./install-runai.sh To verify the installation run: runai list","title":"Run:AI CLI Installation"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#troubleshooting-the-cli-installation","text":"See: Troubleshooting-a-CLI-installation","title":"Troubleshooting the CLI Installation"},{"location":"Administrator/Researcher-Setup/Installing-the-Run-AI-Command-Line-Interface/#updating-the-runai-cli","text":"To update the CLI to the latest version run: sudo runai update","title":"Updating the Run:AI CLI"},{"location":"Administrator/Researcher-Setup/Limit-a-Workload-to-a-Specific-Node-Group/","text":"Why? \u00b6 In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions , hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation Configuring Node Groups \u00b6 To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value Using Node Groups via the CLI \u00b6 Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information Assigning Node Groups to a Project \u00b6 To assign specific node groups to a project see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the project","title":"Limit a Workload to a Specific Node Group"},{"location":"Administrator/Researcher-Setup/Limit-a-Workload-to-a-Specific-Node-Group/#why","text":"In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions , hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation","title":"Why?"},{"location":"Administrator/Researcher-Setup/Limit-a-Workload-to-a-Specific-Node-Group/#configuring-node-groups","text":"To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value","title":"Configuring Node Groups"},{"location":"Administrator/Researcher-Setup/Limit-a-Workload-to-a-Specific-Node-Group/#using-node-groups-via-the-cli","text":"Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information","title":"Using Node Groups via the CLI"},{"location":"Administrator/Researcher-Setup/Limit-a-Workload-to-a-Specific-Node-Group/#assigning-node-groups-to-a-project","text":"To assign specific node groups to a project see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the project","title":"Assigning Node Groups to a Project"},{"location":"Administrator/Researcher-Setup/Limit-the-Run-AI-Scheduler-to-Specific-GPUs-in-Specific-Nodes/","text":"Why? \u00b6 In a gradual approach for incorporating Run:AI in a research operation, it is possible that some researchers are using the system with Run:AI and some without. It is, therefore, possible to limit Run:AI to use specific nodes and specific GPUs within these nodes. How? \u00b6 To configure restrictions on certain nodes: Get the names of the nodes where you want to limit Run:AI and the GPU indices inside these nodes where you want Run:AI to be enabled . For example. let\u2019s say you want Run:AI scheduling for GPUs 1 and 3 on node_2, GPUs 0 and 2 on node_4, and for all GPUs on every other node. Run the following command: kubectl create configmap nvidia-device-plugin-config -n runai \\ --from-literal node_2=\u201c1,3\u201d --from-literal node_4=\u201c0,2\" && \\ kubectl delete pod -n runai -l app=pod-gpu-metrics-exporter && \\ kubectl delete pod -n runai -l name=nvidia-device-plugin-ds Note : if names of nodes contain dashes/hyphens (\u2018-\u2019), they should be replaced with underscores (\u2018_\u2019) inside the command from step 2 (e.g if a node is called node-2, we will write it as node_2 in the command)","title":"Limit the Run:AI Scheduler to Specific GPUs in Specific Nodes"},{"location":"Administrator/Researcher-Setup/Limit-the-Run-AI-Scheduler-to-Specific-GPUs-in-Specific-Nodes/#why","text":"In a gradual approach for incorporating Run:AI in a research operation, it is possible that some researchers are using the system with Run:AI and some without. It is, therefore, possible to limit Run:AI to use specific nodes and specific GPUs within these nodes.","title":"Why?"},{"location":"Administrator/Researcher-Setup/Limit-the-Run-AI-Scheduler-to-Specific-GPUs-in-Specific-Nodes/#how","text":"To configure restrictions on certain nodes: Get the names of the nodes where you want to limit Run:AI and the GPU indices inside these nodes where you want Run:AI to be enabled . For example. let\u2019s say you want Run:AI scheduling for GPUs 1 and 3 on node_2, GPUs 0 and 2 on node_4, and for all GPUs on every other node. Run the following command: kubectl create configmap nvidia-device-plugin-config -n runai \\ --from-literal node_2=\u201c1,3\u201d --from-literal node_4=\u201c0,2\" && \\ kubectl delete pod -n runai -l app=pod-gpu-metrics-exporter && \\ kubectl delete pod -n runai -l name=nvidia-device-plugin-ds Note : if names of nodes contain dashes/hyphens (\u2018-\u2019), they should be replaced with underscores (\u2018_\u2019) inside the command from step 2 (e.g if a node is called node-2, we will write it as node_2 in the command)","title":"How?"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/","text":"Following is a step by step guide for getting a new researcher up to speed with Run:AI and Kubernetes. Change of Paradigms: from Docker to Kubernetes \u00b6 As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI. Setup the Run:AI Command Line Interface \u00b6 Run:AI CLI needs to be installed on the researcher machine. This document provides step by step instructions. Provide the Researcher with a GPU Quota \u00b6 To submit workloads with Run:AI, the researcher must be provided with a \"project\" which contains a GPU quota. Please see this document on how to create projects and set a quota. Provide access to the Run:AI Administration UI \u00b6 Some organizations would want to provide researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI ( app.run.ai) . See this document for further information on how to provide access. Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears here.","title":"Researcher Setup - Start Here"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/#change-of-paradigms-from-docker-to-kubernetes","text":"As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI.","title":"Change of Paradigms: from Docker to Kubernetes&nbsp;"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/#setup-the-runai-command-line-interface","text":"Run:AI CLI needs to be installed on the researcher machine. This document provides step by step instructions.","title":"Setup the Run:AI Command Line Interface"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/#provide-the-researcher-with-a-gpu-quota","text":"To submit workloads with Run:AI, the researcher must be provided with a \"project\" which contains a GPU quota. Please see this document on how to create projects and set a quota.","title":"Provide the Researcher with a GPU Quota"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/#provide-access-to-the-runai-administration-ui","text":"Some organizations would want to provide researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI ( app.run.ai) . See this document for further information on how to provide access.","title":"Provide access to the Run:AI Administration UI"},{"location":"Administrator/Researcher-Setup/Researcher-Setup-Start-Here/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears here.","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/","text":"Dockers, Images, and Kubernetes \u00b6 Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the researcher's workflow as follows. Image Repository \u00b6 If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when submitting workloads, the researcher must use the flag --local-image). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, _nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io which is the Nvidia image repository on the web. Data \u00b6 Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command). Working with Containers \u00b6 Starting a container using docker usually involves a single command line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME=`hostname` \\ -v /raid/public/my_datasets:/root/dataset:ro \\ nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference . Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears here: https://support.run.ai/hc/en-us/articles/360012125099-Researcher-Onboarding-Presentation","title":"Switch from working with Docker to working with Run:AI "},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/#dockers-images-and-kubernetes","text":"Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the researcher's workflow as follows.","title":"Dockers, Images, and Kubernetes"},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/#image-repository","text":"If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when submitting workloads, the researcher must use the flag --local-image). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, _nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io which is the Nvidia image repository on the web.","title":"Image Repository"},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/#data","text":"Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).","title":"Data"},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/#working-with-containers","text":"Starting a container using docker usually involves a single command line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME=`hostname` \\ -v /raid/public/my_datasets:/root/dataset:ro \\ nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference .","title":"Working with Containers&nbsp;"},{"location":"Administrator/Researcher-Setup/Switch-from-working-with-Docker-to-working-with-Run-AI-/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears here: https://support.run.ai/hc/en-us/articles/360012125099-Researcher-Onboarding-Presentation","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/Troubleshooting-a-CLI-installation/","text":"When running the CLI you get an error an invalid configuration error \u00b6 When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided Solution \u00b6 Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory which contains a config file pointing to the Kubernetes cluster When running the CLI you get an error: open .../.kube/config.lock: permission denied \u00b6 When running any CLI command you get a permission denied error. Solution \u00b6 The user running the CLI does not have read permissions to the .kube directory","title":"Troubleshooting a CLI installation"},{"location":"Administrator/Researcher-Setup/Troubleshooting-a-CLI-installation/#when-running-the-cli-you-get-an-error-an-invalid-configuration-error","text":"When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided","title":"When running the CLI you get an error an invalid configuration error"},{"location":"Administrator/Researcher-Setup/Troubleshooting-a-CLI-installation/#solution","text":"Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory which contains a config file pointing to the Kubernetes cluster","title":"Solution"},{"location":"Administrator/Researcher-Setup/Troubleshooting-a-CLI-installation/#when-running-the-cli-you-get-an-error-open-kubeconfiglock-permission-denied","text":"When running any CLI command you get a permission denied error.","title":"When running the CLI you get an error: open .../.kube/config.lock: permission denied"},{"location":"Administrator/Researcher-Setup/Troubleshooting-a-CLI-installation/#solution_1","text":"The user running the CLI does not have read permissions to the .kube directory","title":"Solution"},{"location":"Administrator/Researcher-Setup/Using-a-Docker-Registry-with-Credentials/","text":"Why? \u00b6 Some Docker images are stored in private docker registries. In order for the researcher to access the images, we will need to provide credentials for the registry. How? \u00b6 For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret-name may be any arbitrary string. user_name and password are the repository user and password. Note : the secret may take up to a minute to update in the system","title":"Using a Docker Registry with Credentials"},{"location":"Administrator/Researcher-Setup/Using-a-Docker-Registry-with-Credentials/#why","text":"Some Docker images are stored in private docker registries. In order for the researcher to access the images, we will need to provide credentials for the registry.","title":"Why?"},{"location":"Administrator/Researcher-Setup/Using-a-Docker-Registry-with-Credentials/#how","text":"For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret-name may be any arbitrary string. user_name and password are the repository user and password. Note : the secret may take up to a minute to update in the system","title":"How?"},{"location":"Product/Documents/What-s-New/","text":"June 13th, 2020 \u00b6 New Settings for the Allocation of CPU and Memory \u00b6 It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: https://support.run.ai/hc/en-us/articles/360014087199-Allocation-of-CPU-and-Memory June 3rd, 2020 \u00b6 Node Group Affinity \u00b6 Projects now support _Node Affinity. _This feature allows the administrator to assign specific projects to run only on specific nodes (machines). Example use cases: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. For further information on how to set up duration limits see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects May 24th, 2020 \u00b6 Kubernetes Operators \u00b6 Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: https://support.run.ai/hc/en-us/articles/360013843140-Upgrading-and-Deleting-a-Run-AI-Cluster-Installation March 3rd, 2020 \u00b6 Admin Overview Dashboard \u00b6 A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster","title":"Whats New"},{"location":"Product/Documents/What-s-New/#june-13th-2020","text":"","title":"June 13th, 2020"},{"location":"Product/Documents/What-s-New/#new-settings-for-the-allocation-of-cpu-and-memory","text":"It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: https://support.run.ai/hc/en-us/articles/360014087199-Allocation-of-CPU-and-Memory","title":"New Settings for the Allocation of CPU and Memory"},{"location":"Product/Documents/What-s-New/#june-3rd-2020","text":"","title":"June 3rd, 2020"},{"location":"Product/Documents/What-s-New/#node-group-affinity","text":"Projects now support _Node Affinity. _This feature allows the administrator to assign specific projects to run only on specific nodes (machines). Example use cases: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects","title":"Node Group Affinity"},{"location":"Product/Documents/What-s-New/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. For further information on how to set up duration limits see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects","title":"Limit Duration of Interactive Jobs"},{"location":"Product/Documents/What-s-New/#may-24th-2020","text":"","title":"May 24th, 2020"},{"location":"Product/Documents/What-s-New/#kubernetes-operators","text":"Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: https://support.run.ai/hc/en-us/articles/360013843140-Upgrading-and-Deleting-a-Run-AI-Cluster-Installation","title":"Kubernetes Operators"},{"location":"Product/Documents/What-s-New/#march-3rd-2020","text":"","title":"March 3rd, 2020"},{"location":"Product/Documents/What-s-New/#admin-overview-dashboard","text":"A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster","title":"Admin Overview Dashboard"},{"location":"Researcher/Command-Line-Interface/Automatically-Delete-Jobs-After-Job-Finish/","text":"Introduction \u00b6 Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old jobs by running the command: runai list You can delete the job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a job for automatic deletion some period of time after its finish. Important note : Deleting a job, deletes the container behind it, and with it all related information such as job logs. Data that was saved by the researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface Enable Automatic Deletion in Cluster (Admin only) \u00b6 In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray https://github.com/kubernetes-sigs/kubespray ). Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to /etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec: containers: - command: - kube-apiserver ..... - --feature-gates=TTLAfterFinished=true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec: containers: - command: - kube-controller-manager ..... - --feature-gates=TTLAfterFinished=true Automatic Deletion \u00b6 When starting the job, add the flag --ttl-after-finish <duration>. <duration> is the duration, post job finish, after which the job is automatically deleted. Example durations are:5s, 2m, 3h, 4d etc. For example, the following call will delete the job 2 hours after job finish: runai submit myjob1 --ttl-after-finish 2h Using Templates to set Automatic Deletion as Default \u00b6 You can use Run:AI templates to set auto-delete to be the default. See https://support.run.ai/hc/en-us/articles/360011548039-runai-template for more.","title":"Automatically Delete Jobs After Job Finish"},{"location":"Researcher/Command-Line-Interface/Automatically-Delete-Jobs-After-Job-Finish/#introduction","text":"Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old jobs by running the command: runai list You can delete the job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a job for automatic deletion some period of time after its finish. Important note : Deleting a job, deletes the container behind it, and with it all related information such as job logs. Data that was saved by the researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface","title":"Introduction"},{"location":"Researcher/Command-Line-Interface/Automatically-Delete-Jobs-After-Job-Finish/#enable-automatic-deletion-in-cluster-admin-only","text":"In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray https://github.com/kubernetes-sigs/kubespray ). Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to /etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec: containers: - command: - kube-apiserver ..... - --feature-gates=TTLAfterFinished=true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec: containers: - command: - kube-controller-manager ..... - --feature-gates=TTLAfterFinished=true","title":"Enable Automatic Deletion in Cluster (Admin only)"},{"location":"Researcher/Command-Line-Interface/Automatically-Delete-Jobs-After-Job-Finish/#automatic-deletion","text":"When starting the job, add the flag --ttl-after-finish <duration>. <duration> is the duration, post job finish, after which the job is automatically deleted. Example durations are:5s, 2m, 3h, 4d etc. For example, the following call will delete the job 2 hours after job finish: runai submit myjob1 --ttl-after-finish 2h","title":"Automatic Deletion"},{"location":"Researcher/Command-Line-Interface/Automatically-Delete-Jobs-After-Job-Finish/#using-templates-to-set-automatic-deletion-as-default","text":"You can use Run:AI templates to set auto-delete to be the default. See https://support.run.ai/hc/en-us/articles/360011548039-runai-template for more.","title":"Using Templates to set Automatic Deletion as Default"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/","text":"Introduction \u00b6 The current Run:AI command-line interface (CLI) is based on a single permission model. If you can run a workload using the CLI, then you can, in principle, use all projects, as well as view, change and delete other people's workloads. This is not a sustainable model for most organizations We now introduce a new version of the CLI which is permission-aware. This guide is about how to migrate to this new version. Recap on Projects \u00b6 in Run:AI terminology, Projects are used to manage resource allocations for Researchers. Projects are defined by the administrator user interface (accessible at app.run.ai ) and are associated, amongst other things, with a GPU quota available for users of this project. Projects are utilized by using their name with the -p or --project flag in the CLI, for example when submitting a workload: runai submit <job-name> ..... --project team-a Permissions are based on Projects \u00b6 With the new Run:AI architecture, permissions are based on the granularity of projects. Administrators can provide designated people with access to a project and only these people will be able to submit and view workloads based on that project. This creates changes in the way the CLI works. Changes with the new CLI \u00b6 A workload now exists in the context of a project . When using a command, you must add the project name with the flag -p <project name>. Examples: runai list -p team-a runai bash <job-name> -p team-a runai logs <job-name> -p team-a If you want to avoid adding the -p flag, you can set a default project . To set 'team-a' as the default project, run: runai project set team-a Henceforth, all other CLI commands will be run in the context of the default project you have set. You can also see the list of projects and view the current default project by running: runai project list To view all workloads you are authorized to, regardless of their associated project, use the -A flag: runai list -A Upgrade \u00b6 After installing the new Run:AI CLI, workloads that have been submitted using the older version of the CLI may still be running. Such workloads will be accessible to all researchers using the old methods , without the need for the -p flag, until a default project is set . Once a default project has been set, these older workloads will only be accessible by using the flag ' --backward-compatibility ' or ' -b '. Example: runai bash <old-job-name> -b You can identify old jobs by adding the -b flag to the runai list command as such: runai list -b or by viewing __all __workloads using the -A flag: runai list -A Next Steps \u00b6 For further information on how to authenticate users as well as providing user-access to Projects, please contact Run:AI customer support.","title":"Migrating to Permission-Aware CLI"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#introduction","text":"The current Run:AI command-line interface (CLI) is based on a single permission model. If you can run a workload using the CLI, then you can, in principle, use all projects, as well as view, change and delete other people's workloads. This is not a sustainable model for most organizations We now introduce a new version of the CLI which is permission-aware. This guide is about how to migrate to this new version.","title":"Introduction"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#recap-on-projects","text":"in Run:AI terminology, Projects are used to manage resource allocations for Researchers. Projects are defined by the administrator user interface (accessible at app.run.ai ) and are associated, amongst other things, with a GPU quota available for users of this project. Projects are utilized by using their name with the -p or --project flag in the CLI, for example when submitting a workload: runai submit <job-name> ..... --project team-a","title":"Recap on Projects"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#permissions-are-based-on-projects","text":"With the new Run:AI architecture, permissions are based on the granularity of projects. Administrators can provide designated people with access to a project and only these people will be able to submit and view workloads based on that project. This creates changes in the way the CLI works.","title":"Permissions are based on Projects&nbsp;"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#changes-with-the-new-cli","text":"A workload now exists in the context of a project . When using a command, you must add the project name with the flag -p <project name>. Examples: runai list -p team-a runai bash <job-name> -p team-a runai logs <job-name> -p team-a If you want to avoid adding the -p flag, you can set a default project . To set 'team-a' as the default project, run: runai project set team-a Henceforth, all other CLI commands will be run in the context of the default project you have set. You can also see the list of projects and view the current default project by running: runai project list To view all workloads you are authorized to, regardless of their associated project, use the -A flag: runai list -A","title":"Changes with the new CLI"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#upgrade","text":"After installing the new Run:AI CLI, workloads that have been submitted using the older version of the CLI may still be running. Such workloads will be accessible to all researchers using the old methods , without the need for the -p flag, until a default project is set . Once a default project has been set, these older workloads will only be accessible by using the flag ' --backward-compatibility ' or ' -b '. Example: runai bash <old-job-name> -b You can identify old jobs by adding the -b flag to the runai list command as such: runai list -b or by viewing __all __workloads using the -A flag: runai list -A","title":"Upgrade"},{"location":"Researcher/Command-Line-Interface/Migrating-to-Permission-Aware-CLI/#next-steps","text":"For further information on how to authenticate users as well as providing user-access to Projects, please contact Run:AI customer support.","title":"Next Steps"},{"location":"Researcher/Command-Line-Interface-API-Reference/Introduction/","text":"runai is the command line interface to RunAI Usage: \u00b6 runai [command] Available Commands: \u00b6 Command Description See bash Get a bash session inside a running job https://support.run.ai/hc/en-us/articles/360011544099-runai-bash delete Delete a training job and its associated pods https://support.run.ai/hc/en-us/articles/360011544779-runai-delete exec Execute a command inside a running job https://support.run.ai/hc/en-us/articles/360011544859-runai-exec get Display details of a training job https://support.run.ai/hc/en-us/articles/360011545919-runai-get help Help about any command - list List all the training jobs https://support.run.ai/hc/en-us/articles/360011436080-runai-list logs Print the logs for a task of the training job https://support.run.ai/hc/en-us/articles/360011547759-runai-logs submit Submit a Runai job https://support.run.ai/hc/en-us/articles/360011436120-runai-submit top node Display node (GPU) usage https://support.run.ai/hc/en-us/articles/360011436160-runai-top-node update Update the cli to the latest version https://support.run.ai/hc/en-us/articles/360011436200-runai-update template Show different settings templates https://support.run.ai/hc/en-us/articles/360011548039-runai-template version Print version information -","title":"Introduction"},{"location":"Researcher/Command-Line-Interface-API-Reference/Introduction/#usage","text":"runai [command]","title":"Usage:"},{"location":"Researcher/Command-Line-Interface-API-Reference/Introduction/#available-commands","text":"Command Description See bash Get a bash session inside a running job https://support.run.ai/hc/en-us/articles/360011544099-runai-bash delete Delete a training job and its associated pods https://support.run.ai/hc/en-us/articles/360011544779-runai-delete exec Execute a command inside a running job https://support.run.ai/hc/en-us/articles/360011544859-runai-exec get Display details of a training job https://support.run.ai/hc/en-us/articles/360011545919-runai-get help Help about any command - list List all the training jobs https://support.run.ai/hc/en-us/articles/360011436080-runai-list logs Print the logs for a task of the training job https://support.run.ai/hc/en-us/articles/360011547759-runai-logs submit Submit a Runai job https://support.run.ai/hc/en-us/articles/360011436120-runai-submit top node Display node (GPU) usage https://support.run.ai/hc/en-us/articles/360011436160-runai-top-node update Update the cli to the latest version https://support.run.ai/hc/en-us/articles/360011436200-runai-update template Show different settings templates https://support.run.ai/hc/en-us/articles/360011548039-runai-template version Print version information -","title":"Available Commands:&nbsp;"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/","text":"Description \u00b6 Get a bash session inside a running job This command is a shortcut to runai exec ( runai exec -it <job-name> bash ). See https://support.run.ai/hc/en-us/articles/360011544859-runai-exec for full documentation of the exec command. Synopsis \u00b6 runai bash <job-name> [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 Global Flags \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 The command will access the container that should be currently running in the current cluster and attempt to create a command line shell based on bash. The command will return an error if the container does not exist or has not been in running state yet. See also \u00b6 Build Workloads: https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads-","title":"runai bash"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#description","text":"Get a bash session inside a running job This command is a shortcut to runai exec ( runai exec -it <job-name> bash ). See https://support.run.ai/hc/en-us/articles/360011544859-runai-exec for full documentation of the exec command.","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#synopsis","text":"runai bash <job-name> [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#options","text":"","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#global-flags","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#output","text":"The command will access the container that should be currently running in the current cluster and attempt to create a command line shell based on bash. The command will return an error if the container does not exist or has not been in running state yet.","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-bash/#see-also","text":"Build Workloads: https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads-","title":"See also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/","text":"Description \u00b6 Delete a training job and its associated pods Note that once you delete a job, its entire data will be gone: You will no longer be able to enter it via bash You will no longer be able access logs Any data saved on the container and not stored on a shared repository will be lost Synopsis \u00b6 runai delete <job-name> [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 The job will be deleted and not available via the command runai list The job will not be deleted from the Run:AI user interface Job list See Also \u00b6 Build Workloads: https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- Training Workloads: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"runai delete"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/#description","text":"Delete a training job and its associated pods Note that once you delete a job, its entire data will be gone: You will no longer be able to enter it via bash You will no longer be able access logs Any data saved on the container and not stored on a shared repository will be lost","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/#synopsis","text":"runai delete <job-name> [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/#options","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/#output","text":"The job will be deleted and not available via the command runai list The job will not be deleted from the Run:AI user interface Job list","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-delete/#see-also","text":"Build Workloads: https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- Training Workloads: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/","text":"Description \u00b6 Execute a command inside a running job Note: to execute a bash command, you can also use the shortcut runai bash https://support.run.ai/hc/en-us/articles/360011544859-runai-exec Synopsis \u00b6 runai exec <job-name> <command> [--stdin | -i] [--tty | -t] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 <job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --stdin | -i Keep STDIN open even if not attached --tty | -t Allocate a pseudo-TTY Global Flags \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 The command will run in the context of the container See Also \u00b6","title":"runai exec"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#description","text":"Execute a command inside a running job Note: to execute a bash command, you can also use the shortcut runai bash https://support.run.ai/hc/en-us/articles/360011544859-runai-exec","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#synopsis","text":"runai exec <job-name> <command> [--stdin | -i] [--tty | -t] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#options","text":"<job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --stdin | -i Keep STDIN open even if not attached --tty | -t Allocate a pseudo-TTY","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#global-flags","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#output","text":"The command will run in the context of the container","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-exec/#see-also","text":"","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/","text":"Description \u00b6 Display details of a training job Synopsis \u00b6 runai get <job-name> [--output <value> | -o <value>] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 <job-name> the name of the job to run the command in -o | --output Output format. One of: json|yaml|wide Global Flags \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 The command will show the job properties and status as well as lifecycle events and list of pods See Also \u00b6","title":"runai get"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#description","text":"Display details of a training job","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#synopsis","text":"runai get <job-name> [--output <value> | -o <value>] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#options","text":"<job-name> the name of the job to run the command in -o | --output Output format. One of: json|yaml|wide","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#global-flags","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#output","text":"The command will show the job properties and status as well as lifecycle events and list of pods","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-get/#see-also","text":"","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/","text":"Description \u00b6 Show list of jobs Synopsis runai list <job-name> [--all-projects | -A] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 --all-projects | -A Show jobs from all projects Global Flags \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 A list of jobs will show. To show details for a specific job see runai get See Also \u00b6","title":"runai list"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/#description","text":"Show list of jobs Synopsis runai list <job-name> [--all-projects | -A] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/#options","text":"--all-projects | -A Show jobs from all projects","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/#global-flags","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/#output","text":"A list of jobs will show. To show details for a specific job see runai get","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-list/#see-also","text":"","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/","text":"Description \u00b6 Show logs of training job Synopsis runai logs <job-name> [--follow | -f] [--pod <string> | -p <string>] [--since <duration>] [--since-time <date-time>] [--tail <int> | -t <int>] [--timestamps] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h] Options \u00b6 --follow | -f Specify if the logs should be streamed. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai get <job-name> --instance (string) | -i (string) show logs for a specific instance in cases where a job contains multiple pods --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together --since-time (date-time) Return logs after specified date. Date format should be RFC3339, example: 2020-01-26T15:00:00Z --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output. Global Flags \u00b6 --backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Output \u00b6 The jobs log will show See Also \u00b6 Training Workloads: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"runai logs"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/#description","text":"Show logs of training job Synopsis runai logs <job-name> [--follow | -f] [--pod <string> | -p <string>] [--since <duration>] [--since-time <date-time>] [--tail <int> | -t <int>] [--timestamps] [--backward-compatibility | -b] [--loglevel <value>] [--project <string> | -p <string>] [--help | -h]","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/#options","text":"--follow | -f Specify if the logs should be streamed. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai get <job-name> --instance (string) | -i (string) show logs for a specific instance in cases where a job contains multiple pods --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together --since-time (date-time) Return logs after specified date. Date format should be RFC3339, example: 2020-01-26T15:00:00Z --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output.","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/#global-flags","text":"--backward-compatibility | -b Backward compatibility mode to provide support for Jobs created with older versions of the CLI. See here for further information --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/#output","text":"The jobs log will show","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-logs/#see-also","text":"Training Workloads: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/","text":"Description \u00b6 Submit a Distributed Training (MPI) Run:AI job for execution Synopsis \u00b6 runai submit-mpi <job-name> [--always-pull-image] [--args <stringArray>] [--backoffLimit <int>] [--command <stringArray>] [--cpu <double>] [--cpu-limit <double>] [--environment <stringArray> | -e <stringArray>] [--gpu <int> | -g <int>] [--host-ipc] [--host-network] [--image <string> | -i <string>] [--interactive] [--large-shm] [--local-image] [--memory <string>] [--memory-limit <string>] [--node-type <string>] [--processes <int>] [-- run-as-user] [--volume <stringArray> | -v stringArray] [ --working-dir] [--loglevel <string>] [--project <string> | -p <string>] [--help | -h] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Options \u00b6 <job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --always-pull-image <stringArray> When starting a container, always pull the image from repository, even if cached on running node. This is useful when you are re-saving updates to the image using the same tag. --args <stringArray> Arguments to pass to the command run on container start. Use together with --command. Example: --command sleep --args 10000 -- backoffLimit <int> The number of times the job will be retried before failing. Default is 6. This flag will only work with training workloads (when the --interactive flag is not specified) --command <stringArray> Command to run at container start. Use together with --args . --cpu <double> CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit <double> Limitations of the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. -e <stringArray> | --environment <stringArray> Define environment variables to be set in the container. To set multiple values add the flag multiple times (-e BATCH_SIZE=50 -e LEARNING_RATE=0.2) or separate by a comma (-e BATCH_SIZE:50,LEARNING_RATE:0.2) --gpu <int> | -g <int> Number of GPUs to allocate to the Job. Default is no GPUs. --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack For further information see docker documentation --host-network Use the host's network stack inside the container For further information see docker documentation --image <string> | -i <string> Image to use when creating the container for this Job --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system --large-shm Mount a large /dev/shm device. shm is a shared file system mounted on RAM --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --memory <string> CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit <string> CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. --name <string> (Deprecated) Job Name. Add the name without the --name flag. --node-type <string> Allows defining specific nodes (machines) or group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: https://support.run.ai/hc/en-us/articles/360011591500-Limit-a-Workload-to-a-Specific-Node-Group This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects --processes <int> Number of distributed training processes. Default is 1. --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. --volume <stringArray> | -v <stringArray> Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. --working-dir <string> Starts the container with the specified directory Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Examples \u00b6 start an unattended mpi training job of name dist1, based on project team-a using a quickstart-distributed image: runai submit-mpi dist1 --num-processes=2 -g 1 -i gcr.io/run-ai-demo/quickstart-distributed Output \u00b6 The command will attempt to submit an mpi job. You can follow up on the job by running runai list or runai get <job-name> See Also \u00b6 See Walkthrough documents here: XXX","title":"runai submit-mpi"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#description","text":"Submit a Distributed Training (MPI) Run:AI job for execution","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#synopsis","text":"runai submit-mpi <job-name> [--always-pull-image] [--args <stringArray>] [--backoffLimit <int>] [--command <stringArray>] [--cpu <double>] [--cpu-limit <double>] [--environment <stringArray> | -e <stringArray>] [--gpu <int> | -g <int>] [--host-ipc] [--host-network] [--image <string> | -i <string>] [--interactive] [--large-shm] [--local-image] [--memory <string>] [--memory-limit <string>] [--node-type <string>] [--processes <int>] [-- run-as-user] [--volume <stringArray> | -v stringArray] [ --working-dir] [--loglevel <string>] [--project <string> | -p <string>] [--help | -h] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#options","text":"<job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --always-pull-image <stringArray> When starting a container, always pull the image from repository, even if cached on running node. This is useful when you are re-saving updates to the image using the same tag. --args <stringArray> Arguments to pass to the command run on container start. Use together with --command. Example: --command sleep --args 10000 -- backoffLimit <int> The number of times the job will be retried before failing. Default is 6. This flag will only work with training workloads (when the --interactive flag is not specified) --command <stringArray> Command to run at container start. Use together with --args . --cpu <double> CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit <double> Limitations of the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. -e <stringArray> | --environment <stringArray> Define environment variables to be set in the container. To set multiple values add the flag multiple times (-e BATCH_SIZE=50 -e LEARNING_RATE=0.2) or separate by a comma (-e BATCH_SIZE:50,LEARNING_RATE:0.2) --gpu <int> | -g <int> Number of GPUs to allocate to the Job. Default is no GPUs. --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack For further information see docker documentation --host-network Use the host's network stack inside the container For further information see docker documentation --image <string> | -i <string> Image to use when creating the container for this Job --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system --large-shm Mount a large /dev/shm device. shm is a shared file system mounted on RAM --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --memory <string> CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit <string> CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. --name <string> (Deprecated) Job Name. Add the name without the --name flag. --node-type <string> Allows defining specific nodes (machines) or group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: https://support.run.ai/hc/en-us/articles/360011591500-Limit-a-Workload-to-a-Specific-Node-Group This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects --processes <int> Number of distributed training processes. Default is 1. --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. --volume <stringArray> | -v <stringArray> Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. --working-dir <string> Starts the container with the specified directory","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#examples","text":"start an unattended mpi training job of name dist1, based on project team-a using a quickstart-distributed image: runai submit-mpi dist1 --num-processes=2 -g 1 -i gcr.io/run-ai-demo/quickstart-distributed","title":"Examples"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#output","text":"The command will attempt to submit an mpi job. You can follow up on the job by running runai list or runai get <job-name>","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit-mpi/#see-also","text":"See Walkthrough documents here: XXX","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/","text":"Description \u00b6 Submit a Run:AI job for execution Synopsis \u00b6 runai submit <job-name> [--always-pull-image] [--args <stringArray>] [--backoffLimit <int>] [--command <stringArray>] [--cpu <double>] [--cpu-limit <double>] [--elastic] [--environment <stringArray> | -e <stringArray>] [--gpu <int> | -g <int>] [--host-ipc] [--host-network] [--image <string> | -i <string>] [--interactive] [--jupyter] [--large-shm] [--local-image] [--memory <string>] [--memory-limit <string>] [--node-type <string>] [--port <stringArray>] [--preemptible] [-- run-as-user] [--service-type <string> | -s <string>] [--template <string>] [--ttl-after-finish <duration>] [--volume <stringArray> | -v stringArray] [ --working-dir] [--loglevel <string>] [--project <string> | -p <string>] [--help | -h] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Options \u00b6 <job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --always-pull-image <stringArray> When starting a container, always pull the image from repository, even if cached on running node. This is useful when you are re-saving updates to the image using the same tag. --args <stringArray> Arguments to pass to the command run on container start. Use together with --command. Example: --command sleep --args 10000 -- backoffLimit <int> The number of times the job will be retried before failing. Default is 6. This flag will only work with training workloads (when the --interactive flag is not specified) --command <stringArray> Command to run at container start. Use together with --args . --cpu <double> CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit <double> Limitations of the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. -- elastic Mark the job as elastic. For further information on Elasticity see https://support.run.ai/hc/en-us/articles/360011347560-Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability -e <stringArray> | --environment <stringArray> Define environment variables to be set in the container. To set multiple values add the flag multiple times (-e BATCH_SIZE=50 -e LEARNING_RATE=0.2) or separate by a comma (-e BATCH_SIZE:50,LEARNING_RATE:0.2) --gpu <int> | -g <int> Number of GPUs to allocation to the Job. Default is no GPUs. --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack For further information see docker documentation --host-network Use the host's network stack inside the container For further information see docker documentation --image <string> | -i <string> Image to use when creating the container for this Job --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system --jupyter (Deprecated) Shortcut for running a jupyter notebook container. Uses a pre-created image and a default notebook configuration. Use the templates flag instead. --large-shm Mount a large /dev/shm device. shm is a shared file system mounted on RAM --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --memory <string> CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit <string> CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. --name <string> (Deprecated) Job Name. Add the name without the --name flag. --node-type <string> Allows defining specific nodes (machines) or group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: https://support.run.ai/hc/en-us/articles/360011591500-Limit-a-Workload-to-a-Specific-Node-Group This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects --port <stringArray> Expose ports from the Job container. Used together with --service-type. Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --preemptible Mark an interactive job as preemptible. Preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time. --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. --service-type <string> | -s <string> Service exposure method for interactive Job. Options are: portforward, loadbalancer, nodeport, ingress. Use the command runai list to obtain the endpoint to use the service when the job is running. Different service methods have different endpoint structure --template <string> Use a specific template when running this job. Templates are set by the cluster administrator and provide predefined values to flags under the submit command. If a template is not set, a default template will be use if such exists --ttl-after-finish <duration> Define the duration, post job finish, after which the job is automatically deleted (5s, 2m, 3h, .etc). Note: This setting must first be enabled at the cluster level. See https://support.run.ai/hc/en-us/articles/360011623839-Automatically-Delete-Jobs-After-Job-Finish --volume <stringArray> | -v <stringArray> Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. --working-dir <string> Starts the container with the specified directory Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text Examples \u00b6 start an unattended training job of name run1, based on project team-ny using a quickstart image: runai submit run1 -i gcr.io/run-ai-lab/quickstart -g 1 -p team-ny start an interactive job of name run2, based on project team-ny using a jupyter notebook image. The Notebook will be externalized via a load balancer on port 8888: runai submit run2 -i jupyter/base-notebook -g 1 \\ -p team-ny --interactive --service-type=loadbalancer --port 8888:8888 Output \u00b6 The command will attempt to submit a job. You can follow up on the job by running runai list or runai get <job-name> -e Note that the submit call may use templates to provide defaults to any of the above flags. See Also \u00b6 See any of the Walkthrough documents here: https://support.run.ai/hc/en-us/articles/360010773460-Run-AI-Walkthroughs See runai template https://support.run.ai/hc/en-us/articles/360011548039-runai-template for a description on how templates work","title":"runai submit"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#description","text":"Submit a Run:AI job for execution","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#synopsis","text":"runai submit <job-name> [--always-pull-image] [--args <stringArray>] [--backoffLimit <int>] [--command <stringArray>] [--cpu <double>] [--cpu-limit <double>] [--elastic] [--environment <stringArray> | -e <stringArray>] [--gpu <int> | -g <int>] [--host-ipc] [--host-network] [--image <string> | -i <string>] [--interactive] [--jupyter] [--large-shm] [--local-image] [--memory <string>] [--memory-limit <string>] [--node-type <string>] [--port <stringArray>] [--preemptible] [-- run-as-user] [--service-type <string> | -s <string>] [--template <string>] [--ttl-after-finish <duration>] [--volume <stringArray> | -v stringArray] [ --working-dir] [--loglevel <string>] [--project <string> | -p <string>] [--help | -h] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#options","text":"<job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --always-pull-image <stringArray> When starting a container, always pull the image from repository, even if cached on running node. This is useful when you are re-saving updates to the image using the same tag. --args <stringArray> Arguments to pass to the command run on container start. Use together with --command. Example: --command sleep --args 10000 -- backoffLimit <int> The number of times the job will be retried before failing. Default is 6. This flag will only work with training workloads (when the --interactive flag is not specified) --command <stringArray> Command to run at container start. Use together with --args . --cpu <double> CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit <double> Limitations of the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. -- elastic Mark the job as elastic. For further information on Elasticity see https://support.run.ai/hc/en-us/articles/360011347560-Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability -e <stringArray> | --environment <stringArray> Define environment variables to be set in the container. To set multiple values add the flag multiple times (-e BATCH_SIZE=50 -e LEARNING_RATE=0.2) or separate by a comma (-e BATCH_SIZE:50,LEARNING_RATE:0.2) --gpu <int> | -g <int> Number of GPUs to allocation to the Job. Default is no GPUs. --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack For further information see docker documentation --host-network Use the host's network stack inside the container For further information see docker documentation --image <string> | -i <string> Image to use when creating the container for this Job --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system --jupyter (Deprecated) Shortcut for running a jupyter notebook container. Uses a pre-created image and a default notebook configuration. Use the templates flag instead. --large-shm Mount a large /dev/shm device. shm is a shared file system mounted on RAM --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --memory <string> CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit <string> CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. --name <string> (Deprecated) Job Name. Add the name without the --name flag. --node-type <string> Allows defining specific nodes (machines) or group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: https://support.run.ai/hc/en-us/articles/360011591500-Limit-a-Workload-to-a-Specific-Node-Group This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Projects --port <stringArray> Expose ports from the Job container. Used together with --service-type. Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --preemptible Mark an interactive job as preemptible. Preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time. --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. --service-type <string> | -s <string> Service exposure method for interactive Job. Options are: portforward, loadbalancer, nodeport, ingress. Use the command runai list to obtain the endpoint to use the service when the job is running. Different service methods have different endpoint structure --template <string> Use a specific template when running this job. Templates are set by the cluster administrator and provide predefined values to flags under the submit command. If a template is not set, a default template will be use if such exists --ttl-after-finish <duration> Define the duration, post job finish, after which the job is automatically deleted (5s, 2m, 3h, .etc). Note: This setting must first be enabled at the cluster level. See https://support.run.ai/hc/en-us/articles/360011623839-Automatically-Delete-Jobs-After-Job-Finish --volume <stringArray> | -v <stringArray> Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. --working-dir <string> Starts the container with the specified directory","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use 'runai project set <project name>'. --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#examples","text":"start an unattended training job of name run1, based on project team-ny using a quickstart image: runai submit run1 -i gcr.io/run-ai-lab/quickstart -g 1 -p team-ny start an interactive job of name run2, based on project team-ny using a jupyter notebook image. The Notebook will be externalized via a load balancer on port 8888: runai submit run2 -i jupyter/base-notebook -g 1 \\ -p team-ny --interactive --service-type=loadbalancer --port 8888:8888","title":"Examples"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#output","text":"The command will attempt to submit a job. You can follow up on the job by running runai list or runai get <job-name> -e Note that the submit call may use templates to provide defaults to any of the above flags.","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-submit/#see-also","text":"See any of the Walkthrough documents here: https://support.run.ai/hc/en-us/articles/360010773460-Run-AI-Walkthroughs See runai template https://support.run.ai/hc/en-us/articles/360011548039-runai-template for a description on how templates work","title":"See Also"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/","text":"Description \u00b6 Templates are a way to reduce the amount of flags required when running the command runai submit . A template is added by the administrator and is out of scope for this article. A researcher can: Review list of templates by running runai template list Review the contents of a specific template by running_ runai template get <template-name>_ Use a template by running runai submit --template <template-name> The administrator can also set a default template which is always used on runai submit whenever a template is not specified. Synopsis \u00b6 runai template get <template-name> runai template list Options \u00b6 <template-name> the name of the template to run the command on runai template list will show the list of existing templates. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") Output \u00b6 runai template list will show a list of templates. Example: _runai template get _to get the template details Use the template: runai submit my-pytorch1 --template pytorch-default See Also See: https://support.run.ai/hc/en-us/articles/360011627459-Command-Line-Interface-Templates on how to configure templates","title":"runai template"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/#description","text":"Templates are a way to reduce the amount of flags required when running the command runai submit . A template is added by the administrator and is out of scope for this article. A researcher can: Review list of templates by running runai template list Review the contents of a specific template by running_ runai template get <template-name>_ Use a template by running runai submit --template <template-name> The administrator can also set a default template which is always used on runai submit whenever a template is not specified.","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/#synopsis","text":"runai template get <template-name> runai template list","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/#options","text":"<template-name> the name of the template to run the command on runai template list will show the list of existing templates.","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\")","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-template/#output","text":"runai template list will show a list of templates. Example: _runai template get _to get the template details Use the template: runai submit my-pytorch1 --template pytorch-default See Also See: https://support.run.ai/hc/en-us/articles/360011627459-Command-Line-Interface-Templates on how to configure templates","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-top-node/","text":"Description \u00b6 Show list of nodes (machines) and their properties Synopsis runai top node [--details | -d] [--help | -h] Options \u00b6 --details | -d Show additional details for each node Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 Shows a list of nodes and their properties See Also","title":"runai top node"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-top-node/#description","text":"Show list of nodes (machines) and their properties Synopsis runai top node [--details | -d] [--help | -h]","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-top-node/#options","text":"--details | -d Show additional details for each node","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-top-node/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-top-node/#output","text":"Shows a list of nodes and their properties See Also","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/","text":"Description \u00b6 Find and install the latest version of the runai command line utility. On mac and linux the command must be run with sudo: sudo runai update Synopsis \u00b6 runai update [--loglevel <value>] [--help | -h] Options \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 Update of the runai command line interface See Also \u00b6","title":"runai update"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/#description","text":"Find and install the latest version of the runai command line utility. On mac and linux the command must be run with sudo: sudo runai update","title":"Description"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/#synopsis","text":"runai update [--loglevel <value>] [--help | -h]","title":"Synopsis"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/#options","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Options"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/#output","text":"Update of the runai command line interface","title":"Output"},{"location":"Researcher/Command-Line-Interface-API-Reference/runai-update/#see-also","text":"","title":"See Also"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/","text":"Motivation \u00b6 Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyper-parameter-tuning runs. Best Practices \u00b6 Docker Image \u00b6 A docker container is based on a docker image. Some researchers use generic images such as ones provided by Nvidia (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow ). Others, use generic images as the base image to a more customized image using Dockerfiles . https://docs.docker.com/develop/develop-images/dockerfile_best-practices/. Realizing that researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice: Create a Startup Script \u00b6 All the commands you run inside the interactive job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the researcher to add/remove libraries without needing changes to the image itself. Support Variance Between Different Runs \u00b6 Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: 1. Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ 2. Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case. Checkpoints \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: https://support.run.ai/hc/en-us/articles/360014636380-Saving-Deep-Learning-Checkpoints Running the Job \u00b6 Using runai submit , drop the flag --interactive . For submitting a job using the script created above, please use --command , and pass arguments and/or environment variables using the runai submit flags --args and --environment . Example with Environment variables: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ -e 'EPOCHS=30' \\ -e 'LEARNING_RATE=0.02' Example with Command-line arguments: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ --args=' number-of-epochs=30' \\ --args= 'batch-size=64' Please refer to https://support.run.ai/hc/en-us/articles/360011436120-runai-submit for a list of all arguments accepted by the Run:AI CLI. Use CLI Templates \u00b6 Different run configurations may vary significantly and can be tedious to be written each time on the command line. To make life easier, our CLI offers a way to template those configurations and use preconfigured configuration when submitting a job. Please refer to https://support.run.ai/hc/en-us/articles/360011627459-Configure-Command-Line-Interface-Templates Attached Files \u00b6 The 3 relevant files are attached to this document for reference See Also \u00b6 See the unattended training walkthrough: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"Converting your Workload to use Unattended \"Training\" Execution"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#motivation","text":"Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyper-parameter-tuning runs.","title":"Motivation"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#bestpractices","text":"","title":"Best&nbsp;Practices"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#docker-image","text":"A docker container is based on a docker image. Some researchers use generic images such as ones provided by Nvidia (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow ). Others, use generic images as the base image to a more customized image using Dockerfiles . https://docs.docker.com/develop/develop-images/dockerfile_best-practices/. Realizing that researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice:","title":"Docker Image"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#create-a-startup-script","text":"All the commands you run inside the interactive job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the researcher to add/remove libraries without needing changes to the image itself.","title":"Create a Startup Script"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#support-variance-between-different-runs","text":"Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: 1. Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ 2. Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case.","title":"Support Variance Between Different Runs"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#checkpoints","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: https://support.run.ai/hc/en-us/articles/360014636380-Saving-Deep-Learning-Checkpoints","title":"Checkpoints"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#running-the-job","text":"Using runai submit , drop the flag --interactive . For submitting a job using the script created above, please use --command , and pass arguments and/or environment variables using the runai submit flags --args and --environment . Example with Environment variables: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ -e 'EPOCHS=30' \\ -e 'LEARNING_RATE=0.02' Example with Command-line arguments: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ --args=' number-of-epochs=30' \\ --args= 'batch-size=64' Please refer to https://support.run.ai/hc/en-us/articles/360011436120-runai-submit for a list of all arguments accepted by the Run:AI CLI.","title":"Running the Job"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#use-cli-templates","text":"Different run configurations may vary significantly and can be tedious to be written each time on the command line. To make life easier, our CLI offers a way to template those configurations and use preconfigured configuration when submitting a job. Please refer to https://support.run.ai/hc/en-us/articles/360011627459-Configure-Command-Line-Interface-Templates","title":"Use CLI Templates"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#attached-files","text":"The 3 relevant files are attached to this document for reference","title":"Attached Files"},{"location":"Researcher/Image-Creation-Best-Practices/Converting-your-Workload-to-use-Unattended-Training-Execution/#see-also","text":"See the unattended training walkthrough: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"See Also"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/","text":"Introduction \u00b6 Some researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task. Why Use Docker Images? Docker images and 'containerization' in general provide a level of abstraction which, by large, frees developers and researchers from the mundane tasks of 'setting up an environment'. The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image. Moving a Data Science Environment to Docker A data science environment typically includes: Training data. Machine Learning (ML) code and inputs. Libraries: Code dependencies that must be installed before the ML code can be run. Training data \u00b6 Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the researcher is currently using, allowing the researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine. Machine Learning Code and Inputs \u00b6 As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system. Code Dependencies Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically. ML Lifecycle: Build and Train \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly . Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results . A Train session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running. Build Workloads \u00b6 With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image \" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service). Training Workloads \u00b6 For training workloads you can use a well-known image (e.g. nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software. (Optional) Run a script. The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training job. For further information on how to set up and parameterize a training workload via docker or runai see https://support.run.ai/hc/en-us/articles/360012065440-Converting-your-Workload-to-use-Unattended-Training-Execution","title":"From Bare-Metal to using Docker Images"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#introduction","text":"Some researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task.","title":"Introduction"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#training-data","text":"Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the researcher is currently using, allowing the researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine.","title":"Training data"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#machine-learning-code-and-inputs","text":"As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.","title":"Machine Learning Code and Inputs"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#ml-lifecycle-build-and-train","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly . Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results . A Train session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running.","title":"ML Lifecycle: Build and Train"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#build-workloads","text":"With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image \" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).","title":"Build Workloads"},{"location":"Researcher/Image-Creation-Best-Practices/From-Bare-Metal-to-using-Docker-Images/#training-workloads","text":"For training workloads you can use a well-known image (e.g. nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile. A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software. (Optional) Run a script. The script can be part of the image or can be provided as part of the command line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training job. For further information on how to set up and parameterize a training workload via docker or runai see https://support.run.ai/hc/en-us/articles/360012065440-Converting-your-Workload-to-use-Unattended-Training-Execution","title":"Training Workloads"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/","text":"Introduction \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs). How to Save Checkpoints \u00b6 TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). Where to Save Checkpoints \u00b6 It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. When to Save Checkpoints \u00b6 Save Periodically \u00b6 It is a best practice to save checkpoints at intervals. For example, every epoch. Save on Exit Signal \u00b6 If periodic checkpoints are not enough, you can use a_ signal-hook_ provided by Run:AI (via Kubernetes). The hook is python code that is called before your job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler(signum, frame): # save your checkpoints to shared storage # exit with status \"1\" is important for the job to return later. exit(1) if __name__ == \"__main__\": signal.signal(signal.SIGTERM, graceful_exit_handler) # rest of code By default, you will have 30 seconds to save your checkpoints. Resuming using Saved Checkpoints \u00b6 A Run:AI unattended workload that is resumed, will run the __same startup script __as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist If saved checkpoints exist, load them and start the run using these checkpoints.","title":"Saving Deep Learning Checkpoints"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#introduction","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).","title":"Introduction"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#how-to-save-checkpoints","text":"TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch).","title":"How to Save Checkpoints"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#where-to-save-checkpoints","text":"It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node.","title":"Where to Save Checkpoints"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#when-to-save-checkpoints","text":"","title":"When to Save Checkpoints"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#save-periodically","text":"It is a best practice to save checkpoints at intervals. For example, every epoch.","title":"Save Periodically"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#save-on-exit-signal","text":"If periodic checkpoints are not enough, you can use a_ signal-hook_ provided by Run:AI (via Kubernetes). The hook is python code that is called before your job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler(signum, frame): # save your checkpoints to shared storage # exit with status \"1\" is important for the job to return later. exit(1) if __name__ == \"__main__\": signal.signal(signal.SIGTERM, graceful_exit_handler) # rest of code By default, you will have 30 seconds to save your checkpoints.","title":"Save on Exit Signal"},{"location":"Researcher/Image-Creation-Best-Practices/Saving-Deep-Learning-Checkpoints/#resuming-using-saved-checkpoints","text":"A Run:AI unattended workload that is resumed, will run the __same startup script __as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist If saved checkpoints exist, load them and start the run using these checkpoints.","title":"Resuming using Saved Checkpoints"},{"location":"Researcher/Presentations/Researcher-Onboarding-Presentation/","text":"","title":"Researcher Onboarding Presentation"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/","text":"Introduction \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability Shrinking a Workload \u00b6 Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a researcher to run, albeit slower than intended. Shrinking a training job uses an algorithm called _Gradient __Accumulation. _For more information about the algorithm see https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa Expanding a Workload \u00b6 Expanding a training job allows your GPUs to runs on more GPUs than the researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing a researcher to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs. Installation \u00b6 Python Deep-Learning Code \u00b6 In your command line run: pip install runai In your python code add: import runai.elastic Initialize Elasticity by calling: runai.elastic.init(global_batch_size, max_gpu_batch_size) Create a Keras model: model = runai.elastic.keras.models.Model(model) Model Fitting: model.fit(x_train, y_train, batch_size= runai . elastic .batch_size, epochs=100, validation_data=(x_test, y_test), shuffle=False, verbose= runai.elastic .master, callbacks=[StepTimeReporter()] if runai.elastic .master else []) Running a Training Workload \u00b6 Run the training workload by using the \"elastic\" flag: When launching the job with the runai submit command use --elastic When launching a job via yaml code, use the label \"elastic\" with the value \"true\" Limitations \u00b6 Elasticity currently works with Keras-based deep learning code only Any training job with Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Save checkpoints in your code and allow it to resume from the latest checkpoint rather than start from the beginning","title":"Elasticity: Dynamically Stretch/Compress Jobs According to GPU Availability"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#introduction","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability","title":"Introduction"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#shrinking-a-workload","text":"Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a researcher to run, albeit slower than intended. Shrinking a training job uses an algorithm called _Gradient __Accumulation. _For more information about the algorithm see https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa","title":"Shrinking a Workload"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#expanding-a-workload","text":"Expanding a training job allows your GPUs to runs on more GPUs than the researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing a researcher to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs.","title":"Expanding a Workload"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#installation","text":"","title":"Installation&nbsp;"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#python-deep-learning-code","text":"In your command line run: pip install runai In your python code add: import runai.elastic Initialize Elasticity by calling: runai.elastic.init(global_batch_size, max_gpu_batch_size) Create a Keras model: model = runai.elastic.keras.models.Model(model) Model Fitting: model.fit(x_train, y_train, batch_size= runai . elastic .batch_size, epochs=100, validation_data=(x_test, y_test), shuffle=False, verbose= runai.elastic .master, callbacks=[StepTimeReporter()] if runai.elastic .master else [])","title":"Python Deep-Learning Code"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#running-a-training-workload","text":"Run the training workload by using the \"elastic\" flag: When launching the job with the runai submit command use --elastic When launching a job via yaml code, use the label \"elastic\" with the value \"true\"","title":"Running a Training Workload"},{"location":"Researcher/Run-AI-Researcher-Library/Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability/#limitations","text":"Elasticity currently works with Keras-based deep learning code only Any training job with Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Save checkpoints in your code and allow it to resume from the latest checkpoint rather than start from the beginning","title":"Limitations"},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The reporting module in the library will externalize information about the run which can then be available for users of the Run:AI user interface ( https://app.run.ai ) With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing. Sending Metrics \u00b6 Python Deep-Learning Code \u00b6 In your command line run: pip install runai In your python code add: import runai.reporter To send a number-based metric report, write: reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reportMetric(\"accuracy\", 0.34) To send a text-based metric report, write: reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reportParameter(\"state\", \"Training Model\") Recommended Metrics to send \u00b6 For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric East Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the job progress bar is computed by dividing these parameters. Automatic Sending of Metrics for Keras-Based Scripts \u00b6 For Keras based deep learning runs, there is a python code that automates the task of sending metrics. Install the library as above and reference runai.reporter from your code. Then write: runai.reporter.autolog() The above metrics will automatically be sent going forward. Adding the Metrics to the User interface \u00b6 The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send a metrics via the reporter library Run the workload once to send initial data. Go to: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Reporting via the Run:AI Researcher Library  "},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/#sending-metrics","text":"","title":"Sending Metrics"},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/#python-deep-learning-code","text":"In your command line run: pip install runai In your python code add: import runai.reporter To send a number-based metric report, write: reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reportMetric(\"accuracy\", 0.34) To send a text-based metric report, write: reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reportParameter(\"state\", \"Training Model\")","title":"Python Deep-Learning Code"},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/#recommended-metrics-to-send","text":"For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric East Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the job progress bar is computed by dividing these parameters.","title":"Recommended Metrics to send"},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/#automatic-sending-of-metrics-for-keras-based-scripts","text":"For Keras based deep learning runs, there is a python code that automates the task of sending metrics. Install the library as above and reference runai.reporter from your code. Then write: runai.reporter.autolog() The above metrics will automatically be sent going forward.","title":"Automatic Sending of Metrics for Keras-Based Scripts"},{"location":"Researcher/Run-AI-Researcher-Library/Reporting-via-the-Run-AI-Researcher-Library-/#adding-the-metrics-to-the-user-interface","text":"The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send a metrics via the reporter library Run the workload once to send initial data. Go to: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Adding the Metrics to the User interface"},{"location":"Researcher/Run-AI-Researcher-Library/The-Run-AI-Researcher-Library/","text":"Introduction \u00b6 Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability. Installing the Run:AI Researcher Library \u00b6 In your command line run: pip install runai Run:AI Researcher Library Modules \u00b6 To review details on the specific Run:AI Researcher Library modules see: Reporting: https://support.run.ai/hc/en-us/articles/360011179379-Using-the-Run-AI-Reporter-Library- Elasticity: https://support.run.ai/hc/en-us/articles/360011347560-Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability","title":"The Run:AI Researcher Library"},{"location":"Researcher/Run-AI-Researcher-Library/The-Run-AI-Researcher-Library/#introduction","text":"Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability.","title":"Introduction"},{"location":"Researcher/Run-AI-Researcher-Library/The-Run-AI-Researcher-Library/#installing-the-runai-researcher-library","text":"In your command line run: pip install runai","title":"Installing the Run:AI Researcher Library"},{"location":"Researcher/Run-AI-Researcher-Library/The-Run-AI-Researcher-Library/#runai-researcher-library-modules","text":"To review details on the specific Run:AI Researcher Library modules see: Reporting: https://support.run.ai/hc/en-us/articles/360011179379-Using-the-Run-AI-Reporter-Library- Elasticity: https://support.run.ai/hc/en-us/articles/360011347560-Elasticity-Dynamically-Stretch-Compress-Jobs-According-to-GPU-Availability","title":"&nbsp;Run:AI Researcher Library Modules"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/","text":"Introduction \u00b6 When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs. Requesting CPU & Memory \u00b6 When submitting a job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit CPU over allocation \u00b6 The number of CPUs your job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively. Memory over allocation \u00b6 The amount of Memory your job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your job may receive an out of memory exception and terminate. CPU and Memory limits \u00b6 You can limit your job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your job will never be allocated with more than the amount stated in the --cpu-limit flag If your job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. For further details on these flags see: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit Flag Defaults \u00b6 Defaults for --cpu flag \u00b6 If your job has not specified --cpu, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your job has only specified --gpu 2 and has not specified --cpu, then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change this default please contact Run:AI customer support Default for the --memory flag \u00b6 If your job has not specified --memory, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change this default please contact Run:AI customer support","title":"Allocation of CPU and Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#introduction","text":"When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs.","title":"Introduction"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#requesting-cpu-memory","text":"When submitting a job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit","title":"Requesting CPU &amp; Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-over-allocation","text":"The number of CPUs your job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively.","title":"CPU over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#memory-over-allocation","text":"The amount of Memory your job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your job may receive an out of memory exception and terminate.","title":"Memory over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-and-memory-limits","text":"You can limit your job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your job will never be allocated with more than the amount stated in the --cpu-limit flag If your job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. For further details on these flags see: https://support.run.ai/hc/en-us/articles/360011436120-runai-submit","title":"CPU and Memory limits"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#flag-defaults","text":"","title":"Flag Defaults"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-cpu-flag","text":"If your job has not specified --cpu, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your job has only specified --gpu 2 and has not specified --cpu, then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change this default please contact Run:AI customer support","title":"Defaults for --cpu flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#default-for-the-memory-flag","text":"If your job has not specified --memory, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change this default please contact Run:AI customer support","title":"Default for the --memory flag"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/","text":"Introduction \u00b6 At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works. Terminology \u00b6 Workload Types \u00b6 Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly . Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the researcher can examine the results . A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the researcher is to save checkpoints and allow the code to restore from the last checkpoint. Projects \u00b6 Projects are quota entities that associate a project name with a deserved GPU quota as well as other preferences. A researcher submitting a workload must associate a project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on projects and how to configure them, see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Project-Quotas Basic Scheduling Concepts Interactive vs. Unattended \u00b6 The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted Guaranteed Quota and Over-Quota Every new workload is associated with a Project. The project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a project does not end up with over-quota made fully of interactive workloads. For additional details see below Scheduler Details \u00b6 Allocation & Preemption \u00b6 The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' project and continues with the same flow until it finishes attempting to schedule all workloads Reclaim \u00b6 During the above process, there may be a pending workload whose project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another project which has gone over-quota while preserving fairness between projects. Fairness \u00b6 The Run:AI scheduler determines fairness between multiple over-quota projects according to their GPU quota. Consider for example two projects, each spawning a significant amount of workloads (e.g. for Hyper-parameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different projects regardless of the time they entered the system. The fairness works according to the relative portion of GPU quota for each project. To further illustrate that, suppose that: project A has been allocated with a quota of 3 GPUs, and project B has been allocated with a quota of 1 GPU. Then, if both projects go over quota, project A will receive 25% (=1/(1+3)) of the idle GPUs and project B will receive 75% (=3/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new job is submitted to the system or existing job ends. Bin-packing & Consolidation Part of an efficient scheduler is the ability to eliminate defragmentation: The first step in avoiding defragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate jobs on demand. If a workload cannot be allocated due to defragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload. Elasticity \u00b6 Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of _consolidation _helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload.","title":"The Run:AI Scheduler"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#introduction","text":"At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works.","title":"Introduction"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#terminology","text":"","title":"Terminology"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#workload-types","text":"Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly . Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response Unattended (or \"non-interactive\") training workloads. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the researcher can examine the results . A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the researcher is to save checkpoints and allow the code to restore from the last checkpoint.","title":"Workload Types"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#projects","text":"Projects are quota entities that associate a project name with a deserved GPU quota as well as other preferences. A researcher submitting a workload must associate a project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on projects and how to configure them, see: https://support.run.ai/hc/en-us/articles/360011591300-Working-with-Project-Quotas Basic Scheduling Concepts","title":"Projects"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#interactive-vs-unattended","text":"The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted Guaranteed Quota and Over-Quota Every new workload is associated with a Project. The project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a project does not end up with over-quota made fully of interactive workloads. For additional details see below","title":"Interactive vs. Unattended&nbsp;"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#scheduler-details","text":"","title":"Scheduler Details"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#allocation-preemption","text":"The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' project and continues with the same flow until it finishes attempting to schedule all workloads","title":"Allocation &amp; Preemption"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#reclaim","text":"During the above process, there may be a pending workload whose project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another project which has gone over-quota while preserving fairness between projects.","title":"Reclaim"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#fairness","text":"The Run:AI scheduler determines fairness between multiple over-quota projects according to their GPU quota. Consider for example two projects, each spawning a significant amount of workloads (e.g. for Hyper-parameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different projects regardless of the time they entered the system. The fairness works according to the relative portion of GPU quota for each project. To further illustrate that, suppose that: project A has been allocated with a quota of 3 GPUs, and project B has been allocated with a quota of 1 GPU. Then, if both projects go over quota, project A will receive 25% (=1/(1+3)) of the idle GPUs and project B will receive 75% (=3/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new job is submitted to the system or existing job ends.","title":"Fairness"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#elasticity","text":"Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of _consolidation _helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload.","title":"Elasticity"},{"location":"Researcher/Walkthroughs/Run-AI-Walkthroughs/","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly . Build workloads typically do not maximize usage of the GPU. Unattended \"training\" sessions. Training is characterized by a deep learning run that has a start and a finish With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results . A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Follow the Walkthroughs for each using the links below Unattended training sessions: https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads- Interactive build sessions: https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- Interactive build sessions with externalized services: https://support.run.ai/hc/en-us/articles/360011131919-Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports Using GPU Fractions: https://support.run.ai/hc/en-us/articles/360014989740-Walkthrough-Using-GPU-Fractions","title":"Run:AI Walkthroughs"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/","text":"Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walkthrough you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload View training status and resource consumption using the Run:AI user interface and the Run:AI CLI View training logs Stop the training Prerequisites \u00b6 To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command line run: runai project set team-a runai submit hyper1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training job for team-a with an allocation of a single GPU. The job is based on a sample docker image gcr.io/run-ai-lab/quickstart . We named the job hyper1 Follow up on the job's progress by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running Succeeded - the job has ended To get additional status on your job run: runai get hyper1 View Logs \u00b6 Run the following: runai logs hyper1 You should see a log of a running deep learning session: View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai Under Dashboards | Overview you should see: Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization: Stop Workload \u00b6 Run the following: runai delete hyper1 This would stop the training workload. You can verify this by running runai list again. Next Steps \u00b6 Follow the Walkthrough: Launch Interactive Workloads https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- Use your own containers to run an unattended training workload","title":"Walkthrough: Launch Unattended Training Workloads "},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walkthrough you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload View training status and resource consumption using the Run:AI user interface and the Run:AI CLI View training logs Stop the training","title":"Introduction"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#prerequisites","text":"To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface","title":"Prerequisites&nbsp;"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#run-workload","text":"At the command line run: runai project set team-a runai submit hyper1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training job for team-a with an allocation of a single GPU. The job is based on a sample docker image gcr.io/run-ai-lab/quickstart . We named the job hyper1 Follow up on the job's progress by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running Succeeded - the job has ended To get additional status on your job run: runai get hyper1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#view-logs","text":"Run the following: runai logs hyper1 You should see a log of a running deep learning session:","title":"View Logs"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai Under Dashboards | Overview you should see: Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#stop-workload","text":"Run the following: runai delete hyper1 This would stop the training workload. You can verify this by running runai list again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-Unattended-Training-Workloads-/#next-steps","text":"Follow the Walkthrough: Launch Interactive Workloads https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- Use your own containers to run an unattended training workload","title":"Next Steps"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/","text":"Introduction \u00b6 This walkthrough is an extension of https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- When starting a container with the Run:AI Command Line Interface (CLI), it is possible to expose internal ports to the container user. Exposing a Container Port \u00b6 There are a number of alternative ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> Contact your administrator to see which methods are available in your cluster Port Forwarding, Step by Step Walkthrough \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command line run: runai project set team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 \\ --interactive --service-type=portforward --port 8888:8888 \\ --args=\"--NotebookApp.base_url=jupyter1\" --command=start-notebook.sh The job is based on a generic Jupyter notebook docker image j upyter/base-notebook We named the job _jupyter1. _Note that in this Jupyter implementation, the name of the job should also be copied to the Notbook base URL. Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the_ runai submit_ command is not stopped Open the Jupyter notebook \u00b6 Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook. Ingress, Step by Step Walkthrough \u00b6 __Note: __Ingress must be set up by your administrator prior to usage. For more information see: https://support.run.ai/hc/en-us/articles/360011813620-Exposing-Ports-from-Researcher-Containers Setup \u00b6 Perform the setup steps for port forwarding above. Run Workload \u00b6 At the command line run: runai project set team-a runai submit test-ingress -i jupyter/base-notebook -g 1 \\ --interactive --service-type=ingress --port 8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh An ingress service url will be created, run: runai list You will see the service URL with which to access the Jupyter notebook","title":"Walkthrough: Launch an Interactive Build Workload with Connected Ports"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#introduction","text":"This walkthrough is an extension of https://support.run.ai/hc/en-us/articles/360010894959-Walkthrough-Start-and-Use-Interactive-Build-Workloads- When starting a container with the Run:AI Command Line Interface (CLI), it is possible to expose internal ports to the container user.","title":"Introduction"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#exposing-a-container-port","text":"There are a number of alternative ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> Contact your administrator to see which methods are available in your cluster","title":"Exposing a Container Port"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#port-forwarding-step-by-step-walkthrough","text":"","title":"Port Forwarding, Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#run-workload","text":"At the command line run: runai project set team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 \\ --interactive --service-type=portforward --port 8888:8888 \\ --args=\"--NotebookApp.base_url=jupyter1\" --command=start-notebook.sh The job is based on a generic Jupyter notebook docker image j upyter/base-notebook We named the job _jupyter1. _Note that in this Jupyter implementation, the name of the job should also be copied to the Notbook base URL. Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the_ runai submit_ command is not stopped","title":"Run Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#open-the-jupyter-notebook","text":"Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook.","title":"Open the Jupyter notebook"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#ingress-step-by-step-walkthrough","text":"__Note: __Ingress must be set up by your administrator prior to usage. For more information see: https://support.run.ai/hc/en-us/articles/360011813620-Exposing-Ports-from-Researcher-Containers","title":"Ingress, Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#setup_1","text":"Perform the setup steps for port forwarding above.","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports/#run-workload_1","text":"At the command line run: runai project set team-a runai submit test-ingress -i jupyter/base-notebook -g 1 \\ --interactive --service-type=ingress --port 8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh An ingress service url will be created, run: runai list You will see the service URL with which to access the Jupyter notebook","title":"Run Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/","text":"Introduction \u00b6 Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod ( https://eng.uber.com/horovod/ ) is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a walkthrough of such a scenario. Prerequisites \u00b6 To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Training Workload \u00b6 At the command line run: runai project set team-a runai submit-mpi dist --processes=2 -g 1 -i gcr.io/run-ai-demo/quickstart-distributed We named the job dist The job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed the image contains a startup script that runs a deep learning Horovod-based workload. The script runs the following Horovod command: horovodrun -np % RUNAI_MPI_NUM_WORKERS% python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model=resnet20 --num_batches=1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size=64 --variable_update=horovod Where RUNAI_MPI_NUM_WORKERS is a Run:AI environment variable containing the number of worker processes provided to the runai submit-mpi command (in this example it's 2). Follow up on the job's status by running: runai list The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai get dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist Run an Interactive Distributed training Workload \u00b6 It is also possible to run a distributed training job as \"interactive\". This is useful if you want to test your distributed training job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed \\ --command=\"sh\" --args=\"-c\" --args=\"sleep infinity\" --interactive When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed session. For examples, with Horovod: horovodrun -np 2 python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model=resnet20 --num_batches=1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size=64 --variable_update=horovod Next Steps For more information on how to convert an interactive session into a training job, see: https://support.run.ai/hc/en-us/articles/360012065440-Converting-your-Workload-to-use-Unattended-Training-Execution For a full list of the submit-mpi options see https://support.run.ai/hc/en-us/articles/360015125180-runai-submit-mpi","title":"Walkthrough: Running Distributed Training"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#introduction","text":"Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod ( https://eng.uber.com/horovod/ ) is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a walkthrough of such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#prerequisites","text":"To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface","title":"Prerequisites&nbsp;"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#run-training-workload","text":"At the command line run: runai project set team-a runai submit-mpi dist --processes=2 -g 1 -i gcr.io/run-ai-demo/quickstart-distributed We named the job dist The job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed the image contains a startup script that runs a deep learning Horovod-based workload. The script runs the following Horovod command: horovodrun -np % RUNAI_MPI_NUM_WORKERS% python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model=resnet20 --num_batches=1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size=64 --variable_update=horovod Where RUNAI_MPI_NUM_WORKERS is a Run:AI environment variable containing the number of worker processes provided to the runai submit-mpi command (in this example it's 2). Follow up on the job's status by running: runai list The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai get dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist","title":"Run Training Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Running-Distributed-Training/#run-an-interactive-distributed-training-workload","text":"It is also possible to run a distributed training job as \"interactive\". This is useful if you want to test your distributed training job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed \\ --command=\"sh\" --args=\"-c\" --args=\"sleep infinity\" --interactive When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed session. For examples, with Horovod: horovodrun -np 2 python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model=resnet20 --num_batches=1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size=64 --variable_update=horovod Next Steps For more information on how to convert an interactive session into a training job, see: https://support.run.ai/hc/en-us/articles/360012065440-Converting-your-Workload-to-use-Unattended-Training-Execution For a full list of the submit-mpi options see https://support.run.ai/hc/en-us/articles/360015125180-runai-submit-mpi","title":"Run an Interactive Distributed training Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/","text":"Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walkthrough you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning build workload Open an ssh session to the build workload Stop the build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article. Prerequisites \u00b6 To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command line run: runai project set team-a runai submit build1 -i gcr.io/run-ai-lab/build-demo -g 1 --interactive The job is based on a sample docker image gcr.io/run-ai-lab/build-demo We named the job build1. Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running To get additional status on your job run: runai get build1 Get a Shell to the container \u00b6 Run: runai bash build1 This should provide a direct shell into the computer View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai Under Dashboards | Overview you should see: Under \"Jobs\" you can view the new Workload: Stop Workload \u00b6 Run the following: runai delete build1 This would stop the training workload. You can verify this by running_ runai list_ again. Next Steps \u00b6 Expose internal ports to your interactive build workload: https://support.run.ai/hc/en-us/articles/360011131919-Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports Follow the Walkthrough: Launch unattended training workloads https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"Walkthrough: Start and Use Interactive Build Workloads "},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walkthrough you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning build workload Open an ssh session to the build workload Stop the build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.","title":"Introduction"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#prerequisites","text":"To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface","title":"Prerequisites&nbsp;"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#run-workload","text":"At the command line run: runai project set team-a runai submit build1 -i gcr.io/run-ai-lab/build-demo -g 1 --interactive The job is based on a sample docker image gcr.io/run-ai-lab/build-demo We named the job build1. Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running To get additional status on your job run: runai get build1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#get-a-shell-to-the-container","text":"Run: runai bash build1 This should provide a direct shell into the computer","title":"Get a Shell to the container"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai Under Dashboards | Overview you should see: Under \"Jobs\" you can view the new Workload:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#stop-workload","text":"Run the following: runai delete build1 This would stop the training workload. You can verify this by running_ runai list_ again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/Walkthrough-Start-and-Use-Interactive-Build-Workloads-/#next-steps","text":"Expose internal ports to your interactive build workload: https://support.run.ai/hc/en-us/articles/360011131919-Walkthrough-Launch-an-Interactive-Build-Workload-with-Connected-Ports Follow the Walkthrough: Launch unattended training workloads https://support.run.ai/hc/en-us/articles/360010706360-Walkthrough-Launch-Unattended-Training-Workloads-","title":"Next Steps"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/","text":"Introduction \u00b6 Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 jobs running on the same GPU, meaning you could do eight times the work with the same hardware. Prerequisites \u00b6 To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface Step by Step Walkthrough \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 1 GPU to the project Run Workload \u00b6 At the command line run: runai project set team-a runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive runai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3 --interactive The jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the jobs frac05 _and frac03 respectively . _ Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to delete the job. Currently, all jobs using GPU fractions must be interactive. The jobs are assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Note that both jobs were allocated to the same node. When both jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"Walkthrough: Using GPU Fractions"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/#introduction","text":"Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 jobs running on the same GPU, meaning you could do eight times the work with the same hardware.","title":"Introduction"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/#prerequisites","text":"To complete this walkthrough you must have: Run:AI software is installed on your Kubernetes cluster. See: https://support.run.ai/hc/en-us/articles/360010280179-Installing-Run-AI-on-an-on-premise-Kubernetes-Cluster Run:AI CLI installed on your machine. See: https://support.run.ai/hc/en-us/articles/360010706120-Installing-the-Run-AI-Command-Line-Interface","title":"Prerequisites&nbsp;"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/#step-by-step-walkthrough","text":"","title":"Step by Step Walkthrough"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 1 GPU to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/Walkthrough-Using-GPU-Fractions/#run-workload","text":"At the command line run: runai project set team-a runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 --interactive runai submit frac03 -i gcr.io/run-ai-demo/quickstart -g 0.3 --interactive The jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the jobs frac05 _and frac03 respectively . _ Note the \"interactive\" flag which means the job will not have a start or end. It is the researcher's responsibility to delete the job. Currently, all jobs using GPU fractions must be interactive. The jobs are assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Note that both jobs were allocated to the same node. When both jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"Run Workload"}]}