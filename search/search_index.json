{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Run:AI Product Documentation \u00b6 Welcome to the Run:AI documentation area. For an introduction about what is the Run:AI Platform see Run:AI platform on the run.ai website The Run:AI documentation is targeting two personas: Run:AI Administrator - Responsible for the setup and the day to day administration of the product. Administrator documentation can be found here . Researcher - Using Run:AI to submit jobs. Researcher documentation can be found here . How to get Support \u00b6 To get support use the following channels: Write to support@run.ai . On our website , under Support use the support form. On the bottom right of the administrator user interface , use the Help widget. On the bottom right of this page , use the Help widget.","title":"Overview"},{"location":"#runai-product-documentation","text":"Welcome to the Run:AI documentation area. For an introduction about what is the Run:AI Platform see Run:AI platform on the run.ai website The Run:AI documentation is targeting two personas: Run:AI Administrator - Responsible for the setup and the day to day administration of the product. Administrator documentation can be found here . Researcher - Using Run:AI to submit jobs. Researcher documentation can be found here .","title":"Run:AI Product Documentation"},{"location":"#how-to-get-support","text":"To get support use the following channels: Write to support@run.ai . On our website , under Support use the support form. On the bottom right of the administrator user interface , use the Help widget. On the bottom right of this page , use the Help widget.","title":"How to get Support"},{"location":"old-library/","text":"We have a new documentation library \u00b6 You have landed here because you are using links from the old Run:AI support site. To locate your original page, please use the search box on the top right, navigate by menus or write to support@run.ai to get your new link","title":"We have a new documentation library"},{"location":"old-library/#we-have-a-new-documentation-library","text":"You have landed here because you are using links from the old Run:AI support site. To locate your original page, please use the search box on the top right, navigate by menus or write to support@run.ai to get your new link","title":"We have a new documentation library"},{"location":"Administrator/overview-administrator/","text":"Overview: Administrator Documentation \u00b6 Administrators setting up Run:AI and day to day monitoring and maintenance. As part of the Administrator documentation you will find: Cluster Setup . How to setup and modify a GPU cluster with Run:AI Researcher Setup How to setup Researchers to work with Run:AI. Setting and maintaining the cluster via the Administrator User Interface . Introductory Presentations .","title":"Overview"},{"location":"Administrator/overview-administrator/#overview-administrator-documentation","text":"Administrators setting up Run:AI and day to day monitoring and maintenance. As part of the Administrator documentation you will find: Cluster Setup . How to setup and modify a GPU cluster with Run:AI Researcher Setup How to setup Researchers to work with Run:AI. Setting and maintaining the cluster via the Administrator User Interface . Introductory Presentations .","title":"Overview: Administrator Documentation"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/","text":"Adding, Updating and Deleting Users \u00b6 Introduction \u00b6 The Admin User Interface allows: The setup of Kubernetes GPU Clusters. Create, Update and Delete of users Create, Update and Delete Projects & Departments. Review short term and long term dashboards Review Node and Job-status This document is about the Creation, Update, and Deletion of Users. Notes: With Run:AI you need to differentiate between the users of the Admin UI and Researcher users which submit workloads on the GPU Kubernetes cluster. This document is about the former. It is possible to connect the Admin UI users module to the organization's LDAP directory. For further information please contact Run:AI customer support. Working with Users \u00b6 Create User \u00b6 Note: In order to be able to manipulate users, you must have Administrator access. if you do not have such access, please contact an administrator. The list of administrators is shown on the Users page (see below) Log in to https://app.run.ai On the top left, open the menu and select \"Users\" On the top right, select \"Add New Users\". Choose a user name and email. Leave password as blank, it will be set by the user Select Roles. Note -- more than one role can be selected. The available roles are: Administrator : Can manage users and install clusters. Editor : Can manage projects and departments. Viewer : View-only access to Admin UI. Researcher : Can run ML workloads (subject to the adding of the user to a specific project and an existing authentication integration) Select a Cluster. This determines what Clusters are accessible to this user Press \"Save\" The user will receive a join mail and will be able to set a password. Update a User \u00b6 Select an existing User. Right-click and press \"Edit\" Update the values and press \"Save\" Delete an existing User \u00b6 Select an existing User. Right-click and press \"Delete\"","title":"Users"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#adding-updating-and-deleting-users","text":"","title":"Adding, Updating and Deleting Users"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#introduction","text":"The Admin User Interface allows: The setup of Kubernetes GPU Clusters. Create, Update and Delete of users Create, Update and Delete Projects & Departments. Review short term and long term dashboards Review Node and Job-status This document is about the Creation, Update, and Deletion of Users. Notes: With Run:AI you need to differentiate between the users of the Admin UI and Researcher users which submit workloads on the GPU Kubernetes cluster. This document is about the former. It is possible to connect the Admin UI users module to the organization's LDAP directory. For further information please contact Run:AI customer support.","title":"Introduction"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#working-with-users","text":"","title":"Working with Users"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#create-user","text":"Note: In order to be able to manipulate users, you must have Administrator access. if you do not have such access, please contact an administrator. The list of administrators is shown on the Users page (see below) Log in to https://app.run.ai On the top left, open the menu and select \"Users\" On the top right, select \"Add New Users\". Choose a user name and email. Leave password as blank, it will be set by the user Select Roles. Note -- more than one role can be selected. The available roles are: Administrator : Can manage users and install clusters. Editor : Can manage projects and departments. Viewer : View-only access to Admin UI. Researcher : Can run ML workloads (subject to the adding of the user to a specific project and an existing authentication integration) Select a Cluster. This determines what Clusters are accessible to this user Press \"Save\" The user will receive a join mail and will be able to set a password.","title":"Create User"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#update-a-user","text":"Select an existing User. Right-click and press \"Edit\" Update the values and press \"Save\"","title":"Update a User"},{"location":"Administrator/Admin-User-Interface-Setup/Adding-Updating-and-Deleting-Admin-UI-Users/#delete-an-existing-user","text":"Select an existing User. Right-click and press \"Delete\"","title":"Delete an existing User"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/","text":"Introduction \u00b6 Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create priorities, Run:AI introduced the concept of Projects . Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Administrators manage Projects as detailed here . At some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached with a quota. There are organizational quotas at a higher level. Departments \u00b6 Departments are a second hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. A Department quota supersedes a Project quota. Overquota behavior \u00b6 Consider an example from an academic use case: the Computer Science department and the GeoPhysics department have each purchased 10 DGXs with 80 GPUs, totaling a cluster of 160 GPUs. The two departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need it. As such, there could be many Projects in the GeoPhysics department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:AI scheduler as over-quota. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the department allocation should be equal to the number of GPUs in the cluster. Creating and Managing Departments \u00b6 Enable Departments \u00b6 Departments are disabled by default. To start working with departments: Go to Settings | General Enable Departments Once departments are enabled, the menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All projects created before the Department feature was enabled will belong to the default department. Adding Departments \u00b6 You can add new Departments by pressing the Add New Department at the top right of the Department view. Add department name and quota allocation. Assigning Projects to Departments \u00b6 Under Projects edit an existing project, you will see a new Department drop down with which you can associate a project with a department.","title":"Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#introduction","text":"Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create priorities, Run:AI introduced the concept of Projects . Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Administrators manage Projects as detailed here . At some organizations, Projects may not be enough, this is because: There are simply too many individual entities that are attached with a quota. There are organizational quotas at a higher level.","title":"Introduction"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#departments","text":"Departments are a second hierarchy of resource allocation: A Project is associated with a single Department. Multiple Projects can be associated with the same Department. A Department, like a Project is associated with a Quota. A Department quota supersedes a Project quota.","title":"Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#overquota-behavior","text":"Consider an example from an academic use case: the Computer Science department and the GeoPhysics department have each purchased 10 DGXs with 80 GPUs, totaling a cluster of 160 GPUs. The two departments do not mind sharing GPUs as long as they always get their 80 GPUs when they truly need it. As such, there could be many Projects in the GeoPhysics department, totaling an allocation of 100 GPUs, but anything above 80 GPUs will be considered by the Run:AI scheduler as over-quota. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the department allocation should be equal to the number of GPUs in the cluster.","title":"Overquota behavior"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#creating-and-managing-departments","text":"","title":"Creating and Managing Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#enable-departments","text":"Departments are disabled by default. To start working with departments: Go to Settings | General Enable Departments Once departments are enabled, the menu will have a new item named \"Departments\". Under Departments there will be a single Department named default . All projects created before the Department feature was enabled will belong to the default department.","title":"Enable Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#adding-departments","text":"You can add new Departments by pressing the Add New Department at the top right of the Department view. Add department name and quota allocation.","title":"Adding Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Departments/#assigning-projects-to-departments","text":"Under Projects edit an existing project, you will see a new Department drop down with which you can associate a project with a department.","title":"Assigning Projects to Departments"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/","text":"Introduction \u00b6 Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create prioritize, Run:AI introduced the concept of Projects . Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state. Modeling Projects \u00b6 As an Admin, you need to determine how to model projects. You can: Set a project per user Set a project per team of users Set a project per a real organizational project. Project Quotas \u00b6 Each project is associated with a quota of GPUs that can be allocated for this project at the same time. This is guaranteed quota in the sense that researchers using this project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this project can receive an over-quota . As long as GPUs are unused, a researcher using this project can get more GPUs. However, these GPUs can be taken away at a moment's notice. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the project allocation should be equal to the number of GPUs in the cluster. Working with Projects \u00b6 Create a new Project \u00b6 Note In order to be able to manipulate projects, you must have Editor access. See the \"Users\" Area Log in to https://app.run.ai On the top left, open the menu and select \"Projects\" On the top right, select \"Add New Project\" Choose a project name and a project quota Press \"Save\" Update an existing Project \u00b6 Select an existing project. Right-click and press \"Edit\". Update the values and press \"Save\". Delete an existing project \u00b6 Select an existing project. Right-click and press \"Delete\". Limit Jobs to run on Specific Node Groups \u00b6 A frequent use case is to assign specific projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes. While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization. Grouping Nodes \u00b6 To set node affinities, you must first annotate nodes with labels. These labels will later be associated with projects. Each node can only be annotated with a single name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2 Setting Affinity for a Specific Project \u00b6 To mandate training jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the project dialog. Further Affinity Refinement by the Researcher \u00b6 The researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information runai submit Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive jobs: Create a Project or edit an existing Project. Go to the Time Limit tab and set a limit (day, hour, minute). The setting only takes effect for jobs that have started after the duration has been changed.","title":"Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#introduction","text":"Researchers are submitting workloads via The Run:AI CLI, Kubeflow or similar. To streamline resource allocation and create prioritize, Run:AI introduced the concept of Projects . Projects are quota entities that associate a project name with GPU allocation and preferences. A researcher submitting a workload needs to associate a project with a workload request. The Run:AI scheduler will compare the request against the current allocations and the project and determine whether the workload can be allocated resources or whether it should remain in a pending state.","title":"Introduction"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#modeling-projects","text":"As an Admin, you need to determine how to model projects. You can: Set a project per user Set a project per team of users Set a project per a real organizational project.","title":"Modeling Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#project-quotas","text":"Each project is associated with a quota of GPUs that can be allocated for this project at the same time. This is guaranteed quota in the sense that researchers using this project are guaranteed to get this number of GPUs, no matter what the status in the cluster is. Beyond that, a user of this project can receive an over-quota . As long as GPUs are unused, a researcher using this project can get more GPUs. However, these GPUs can be taken away at a moment's notice. For more details on over-quota scheduling see: The Run AI Scheduler . Important best practice: As a rule, the sum of the project allocation should be equal to the number of GPUs in the cluster.","title":"Project Quotas"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#working-with-projects","text":"","title":"Working with Projects"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#create-a-new-project","text":"Note In order to be able to manipulate projects, you must have Editor access. See the \"Users\" Area Log in to https://app.run.ai On the top left, open the menu and select \"Projects\" On the top right, select \"Add New Project\" Choose a project name and a project quota Press \"Save\"","title":"Create a new Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#update-an-existing-project","text":"Select an existing project. Right-click and press \"Edit\". Update the values and press \"Save\".","title":"Update an existing Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#delete-an-existing-project","text":"Select an existing project. Right-click and press \"Delete\".","title":"Delete an existing project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#limit-jobs-to-run-on-specific-node-groups","text":"A frequent use case is to assign specific projects to run only on specific nodes (machines). This can happen for various reasons. Examples: The project team needs specialized hardware (e.g. with enough memory). The project team is the owner of specific hardware which was acquired with a specialized budget. We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes. While such 'affinities' are sometimes needed, its worth mentioning that at the end of the day any affinity settings have a negative impact on the overall system utilization.","title":"Limit Jobs to run on Specific Node Groups"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#grouping-nodes","text":"To set node affinities, you must first annotate nodes with labels. These labels will later be associated with projects. Each node can only be annotated with a single name. To get the list of nodes, run: kubectl get nodes To annotate a specific node with the label \"dgx-2\", run: kubectl label node <node-name> run.ai/type=dgx-2","title":"Grouping Nodes"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#setting-affinity-for-a-specific-project","text":"To mandate training jobs to run on specific node groups: Create a Project or edit an existing Project. Go to the Node Affinity tab and set a limit to specific node groups. If the label does not yet exist, press the + sign and add the label. Press Enter to save the label. Select the label. To mandate interactive jobs to run on specific node groups, perform the same steps under the \"interactive\" section in the project dialog.","title":"Setting Affinity for a Specific Project"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#further-affinity-refinement-by-the-researcher","text":"The researcher can limit the selection of node groups by using the CLI flag --node-type with a specific label. When setting specific project affinity, the CLI flag can only be used to with a node group out of the previously chosen list. See CLI reference for further information runai submit","title":"Further Affinity Refinement by the Researcher"},{"location":"Administrator/Admin-User-Interface-Setup/Working-with-Projects/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. Warning : This feature will cause containers to automatically stop. Any work not saved to a shared volume will be lost To set a duration limit for interactive jobs: Create a Project or edit an existing Project. Go to the Time Limit tab and set a limit (day, hour, minute). The setting only takes effect for jobs that have started after the duration has been changed.","title":"Limit Duration of Interactive Jobs"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/","text":"Introduction \u00b6 Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax. Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress. Ingress \u00b6 Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Setup \u00b6 Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster. A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for localLoadBalancer localLoadBalancer enabled : true ipRangeFrom : 10.0.2.1 ipRangeTo : 10.0.2.2 Set enabled to true and set the IP range appropriately. Usage \u00b6 The researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 -p team-ny \\ --interactive --service-type=ingress --port 8888:8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh After submitting a job through the Run:AI CLI, run: runai list You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the job name and the port (e.g. https://10.255.174.13/test-ingress-8888 . For further details see CLI command runai submit and Launch an Interactive Workload walk-through . Alternatives \u00b6 Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port>. See https://kubernetes.io/docs/concepts/services-networking/service/ for further details. See Also \u00b6 To learn how to use port forwarding see: Walk-through Launch an Interactive Build Workload with Connected Ports .","title":"Allow external access to Containers"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#introduction","text":"Researchers who work with containers sometimes need to expose ports to access the container from remote. Some examples: Using a Jupyter notebook that runs within the container Using PyCharm to run python commands remotely. Using TensorBoard to view machine learning visualizations When using docker, the way researchers expose ports is by declaring them when starting the container. Run:AI has similar syntax. Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports. With Run:AI you can use all of these options (see the Alternatives section below), however, Run:AI comes built-in with ingress.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#ingress","text":"Ingress allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here .","title":"Ingress"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#setup","text":"Before installing ingress, you must obtain an IP Address or an IP address range which is external to the cluster. A Run:AI cluster is installed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for localLoadBalancer localLoadBalancer enabled : true ipRangeFrom : 10.0.2.1 ipRangeTo : 10.0.2.2 Set enabled to true and set the IP range appropriately.","title":"Setup"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#usage","text":"The researcher uses the Run:AI CLI to set the method type and the ports when submitting the Workload. Example: runai submit test-ingress -i jupyter/base-notebook -g 1 -p team-ny \\ --interactive --service-type=ingress --port 8888:8888 \\ --args=\"--NotebookApp.base_url=test-ingress\" --command=start-notebook.sh After submitting a job through the Run:AI CLI, run: runai list You will see the service URL with which to access the Jupyter notebook The URL will be composed of the ingress end-point, the job name and the port (e.g. https://10.255.174.13/test-ingress-8888 . For further details see CLI command runai submit and Launch an Interactive Workload walk-through .","title":"Usage"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#alternatives","text":"Run:AI is based on Kubernetes. Kubernetes offers an abstraction of the container's location. This complicates the exposure of ports. Kubernetes offers a number of alternative ways to expose ports: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. More information about ingress can be found here . Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port>. See https://kubernetes.io/docs/concepts/services-networking/service/ for further details.","title":"Alternatives"},{"location":"Administrator/Cluster-Setup/allow-external-access-to-containers/#see-also","text":"To learn how to use port forwarding see: Walk-through Launch an Interactive Build Workload with Connected Ports .","title":"See Also"},{"location":"Administrator/Cluster-Setup/cluster-delete/","text":"Deleting a Cluster Installation \u00b6 To delete a Run:AI Cluster installation run the following commands: kubectl delete RunaiConfig runai -n runai kubectl delete deployment runai-operator -n runai kubectl delete crd runaiconfigs.run.ai kubectl delete namespace runai","title":"Cluster Delete"},{"location":"Administrator/Cluster-Setup/cluster-delete/#deleting-a-cluster-installation","text":"To delete a Run:AI Cluster installation run the following commands: kubectl delete RunaiConfig runai -n runai kubectl delete deployment runai-operator -n runai kubectl delete crd runaiconfigs.run.ai kubectl delete namespace runai","title":"Deleting a Cluster Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/","text":"The following are instructions on how to install Run:AI on the customer's Kubernetes Cluster. Before installation please review the installation prerequisites here: Run AI GPU Cluster Prerequisites . Step 1: NVIDIA \u00b6 On each machine with GPUs run the following steps 1.1 - 1.3: Step 1.1 Install NVIDIA Drivers \u00b6 If NVIDIA drivers are not already installed on your GPU machines, please install them now. Note that on original NVIDIA hardware, these drivers are already installed by default. After installing NVIDIA drivers, reboot the machine. Then verify that the installation succeeded by running: nvidia-smi Step 1.2: Install NVIDIA Docker \u00b6 This step assumes that Docker is already installed on the machine. If not, please install using https://docs.docker.com/engine/install/ To install NVIDIA Docker on Debian-based distributions (such as Ubuntu), run the following: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For RHEL-based distributions, see nvidia-docker installation instructions , or run: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Step 1.3: Make NVIDIA Docker the default docker runtime \u00b6 You will need to enable the Nvidia runtime as your default docker runtime on your node. Edit the docker daemon config file at /etc/docker/daemon.json and add the default-runtime key as follows: { \"default-runtime\" : \"nvidia\" , \"runtimes\" : { \"nvidia\" : { \"path\" : \"/usr/bin/nvidia-container-runtime\" , \"runtimeArgs\" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd Step 2: Install & Configure Kubernetes \u00b6 Step 2.1 Install Kubernetes \u00b6 Installing Kubernetes is beyond the scope of this guide. There are plenty of good ways to install Kubernetes (listed here: https://kubernetes.io/docs/setup/ . We recommend Kubespray https://kubespray.io/ . Download the latest stable version from : https://github.com/kubernetes-sigs/kubespray . Note : Run:AI is customizing the NVIDIA Kubernetes device plugin ( https://github.com/NVIDIA/k8s-device-plugin ). Do not install this software as it is installed by Run:AI. Some best practices on Kubernetes Configuration can be found here: Kubernetes Cluster Configuration Best Practices . The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to the Kubernetes cluster. Step 2.2 Storage \u00b6 Run:AI is storing data on a filesystem. How this storage is configured differs according to the customer environment and usage: * If the purpose of this installation is testing/proof-of-concept, then a local storage on one of the nodes is enough. * If the purpose of this installation is production, then it is a good practice to setup the system such that if one node is down, the Run:AI software will seamlessly migrate to another node. For this, the storage has to reside on shared storage By default, Run:AI installs on local storage. To verify that this is indeed the default, run: kubectl get storageclass If the output list contains a default storage class you must, in step 3.2 below, remove the Run:AI default storage class. To install on shared storage, you must, in step 3.2 below, provide information about your NFS (Network File Storage). Step 2.3 Label CPU-Only Worker Nodes \u00b6 If you have CPU-only worker nodes (non-master) in your cluster (see Hardware Requirements ), you will need to label them. Labels help Run:AI to place its software correctly, by avoiding placement of Run:AI containers on GPU nodes used for processing data science and by placing monitoring software on the GPU nodes. To get the list of nodes, run: kubectl get nodes To label CPU-only nodes, run the following on each CPU-only node: kubectl label node <node-name> run.ai/cpu-node=true Where <node-name> is the name of the node. Node names can be obtained by running kubectl get nodes Note Kubernetes master node(s) typically have a \"NoSchedule\" taint so as to avoid non-system pods running on a master node. Pressuring master nodes may lead to the Kubernetes system not functioning properly. If your master node is not a GPU node, make sure that this taint exists so that Run:AI too, does not run on a master node. Step 3: Install Run:AI \u00b6 Step 3.1: Install Run:AI \u00b6 Log in to Run:AI Admin UI at https://app.run.ai. Use credentials provided by Run:AI Customer Support to log in to the system. If this is the first time anyone from your company has logged in, you will receive a dialog with instructions on how to install Run:AI on your Kubernetes Cluster. If not, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Continue according to UI instructions to install Run:AI on your Kubernetes Cluster. Take care to read the next section (Customize Installation) before proceeding to apply the file you download during the process. Step 3.2: Customize Installation \u00b6 The Run:AI Admin UI cluster creation wizard asks you to download a YAML file runai-operator-<cluster-name>.yaml . You must then apply the file to Kubernetes. Before applying to Kubernetes, you may need to edit this file. Examples: To allow access to containers (e.g. for Jupyter Notebooks, PyCharm etc) you will need to add an ingress load-balancing point. See: Exposing Ports from Researcher Containers . To allow outbound internet connectivity in a proxied environment. See: Installing Run:AI with an Internet Proxy Server . (See step 2.2) To remove the Run:AI default Storage Class when a default storage class already exists. See: remove default storage class . (See step 2.2) To install Run:AI on NFS, see: Installing Run:AI over network file storage Step 4: Verify your Installation \u00b6 Go to https://app.run.ai . Go to the Overview Dashboard. Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line. For a more extensive verification of cluster health, see Determining the health of a cluster . Next Steps \u00b6 Set up Admin UI Users Working with Admin UI Users . Set up Projects Working with Projects . Researchers work via a Command-line interface (CLI). See Installing the Run AI Command-line Interface on how to install the CLI for users.","title":"Cluster Install"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-1-nvidia","text":"On each machine with GPUs run the following steps 1.1 - 1.3:","title":"Step 1: NVIDIA"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-11-install-nvidia-drivers","text":"If NVIDIA drivers are not already installed on your GPU machines, please install them now. Note that on original NVIDIA hardware, these drivers are already installed by default. After installing NVIDIA drivers, reboot the machine. Then verify that the installation succeeded by running: nvidia-smi","title":"Step 1.1 Install NVIDIA Drivers"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-12-install-nvidia-docker","text":"This step assumes that Docker is already installed on the machine. If not, please install using https://docs.docker.com/engine/install/ To install NVIDIA Docker on Debian-based distributions (such as Ubuntu), run the following: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd For RHEL-based distributions, see nvidia-docker installation instructions , or run: distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo sudo yum install -y nvidia-docker2 sudo pkill -SIGHUP dockerd","title":"Step 1.2: Install NVIDIA Docker"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-13-make-nvidia-docker-the-default-docker-runtime","text":"You will need to enable the Nvidia runtime as your default docker runtime on your node. Edit the docker daemon config file at /etc/docker/daemon.json and add the default-runtime key as follows: { \"default-runtime\" : \"nvidia\" , \"runtimes\" : { \"nvidia\" : { \"path\" : \"/usr/bin/nvidia-container-runtime\" , \"runtimeArgs\" : [] } } } Then run the following again: sudo pkill -SIGHUP dockerd","title":"Step 1.3: Make NVIDIA Docker the default docker runtime"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-2-install-configure-kubernetes","text":"","title":"Step 2: Install &amp; Configure Kubernetes"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-21-install-kubernetes","text":"Installing Kubernetes is beyond the scope of this guide. There are plenty of good ways to install Kubernetes (listed here: https://kubernetes.io/docs/setup/ . We recommend Kubespray https://kubespray.io/ . Download the latest stable version from : https://github.com/kubernetes-sigs/kubespray . Note : Run:AI is customizing the NVIDIA Kubernetes device plugin ( https://github.com/NVIDIA/k8s-device-plugin ). Do not install this software as it is installed by Run:AI. Some best practices on Kubernetes Configuration can be found here: Kubernetes Cluster Configuration Best Practices . The following next steps assume that you have the Kubernetes command-line kubectl on your laptop and that it is configured to point to the Kubernetes cluster.","title":"Step 2.1 Install Kubernetes"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-22-storage","text":"Run:AI is storing data on a filesystem. How this storage is configured differs according to the customer environment and usage: * If the purpose of this installation is testing/proof-of-concept, then a local storage on one of the nodes is enough. * If the purpose of this installation is production, then it is a good practice to setup the system such that if one node is down, the Run:AI software will seamlessly migrate to another node. For this, the storage has to reside on shared storage By default, Run:AI installs on local storage. To verify that this is indeed the default, run: kubectl get storageclass If the output list contains a default storage class you must, in step 3.2 below, remove the Run:AI default storage class. To install on shared storage, you must, in step 3.2 below, provide information about your NFS (Network File Storage).","title":"Step 2.2 Storage"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-23-label-cpu-only-worker-nodes","text":"If you have CPU-only worker nodes (non-master) in your cluster (see Hardware Requirements ), you will need to label them. Labels help Run:AI to place its software correctly, by avoiding placement of Run:AI containers on GPU nodes used for processing data science and by placing monitoring software on the GPU nodes. To get the list of nodes, run: kubectl get nodes To label CPU-only nodes, run the following on each CPU-only node: kubectl label node <node-name> run.ai/cpu-node=true Where <node-name> is the name of the node. Node names can be obtained by running kubectl get nodes Note Kubernetes master node(s) typically have a \"NoSchedule\" taint so as to avoid non-system pods running on a master node. Pressuring master nodes may lead to the Kubernetes system not functioning properly. If your master node is not a GPU node, make sure that this taint exists so that Run:AI too, does not run on a master node.","title":"Step 2.3 Label CPU-Only Worker Nodes"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-3-install-runai","text":"","title":"Step 3: Install Run:AI"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-31-install-runai","text":"Log in to Run:AI Admin UI at https://app.run.ai. Use credentials provided by Run:AI Customer Support to log in to the system. If this is the first time anyone from your company has logged in, you will receive a dialog with instructions on how to install Run:AI on your Kubernetes Cluster. If not, open the menu on the top left and select \"Clusters\". On the top right-click \"Add New Cluster\". Continue according to UI instructions to install Run:AI on your Kubernetes Cluster. Take care to read the next section (Customize Installation) before proceeding to apply the file you download during the process.","title":"Step 3.1: Install Run:AI"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-32-customize-installation","text":"The Run:AI Admin UI cluster creation wizard asks you to download a YAML file runai-operator-<cluster-name>.yaml . You must then apply the file to Kubernetes. Before applying to Kubernetes, you may need to edit this file. Examples: To allow access to containers (e.g. for Jupyter Notebooks, PyCharm etc) you will need to add an ingress load-balancing point. See: Exposing Ports from Researcher Containers . To allow outbound internet connectivity in a proxied environment. See: Installing Run:AI with an Internet Proxy Server . (See step 2.2) To remove the Run:AI default Storage Class when a default storage class already exists. See: remove default storage class . (See step 2.2) To install Run:AI on NFS, see: Installing Run:AI over network file storage","title":"Step 3.2: Customize Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/#step-4-verify-your-installation","text":"Go to https://app.run.ai . Go to the Overview Dashboard. Verify that the number of GPUs on the top right reflects your GPU resources on your cluster and the list of machines with GPU resources appear on the bottom line. For a more extensive verification of cluster health, see Determining the health of a cluster .","title":"Step 4: Verify your Installation"},{"location":"Administrator/Cluster-Setup/cluster-install/#next-steps","text":"Set up Admin UI Users Working with Admin UI Users . Set up Projects Working with Projects . Researchers work via a Command-line interface (CLI). See Installing the Run AI Command-line Interface on how to install the CLI for users.","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/","text":"Below are the prerequisites of a cluster installed with Run:AI. Kubernetes Software \u00b6 Run:AI requires Kubernetes 1.15 or above. Kubernetes 1.17 is recommended (as of June 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16. NVIDIA Driver \u00b6 Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency. Hardware Requirements \u00b6 Kubernetes: Dedicated CPU-only worker node: To save on expensive GPUs-based hardware, we recommend (though not a must), a dedicated, CPU-only worker machine. Run:AI requires the following resources: 4 CPUs 4GB of RAM At least 20GB of Disk space Shared data volume: Run:AI uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, so as to access training data and code as well as save checkpoints, weights, and other machine-learning related artifacts. The Run:AI system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). NFS is usually the preferred method for Researchers which may require multi-read/write capabilities. Docker Registry With Run:AI, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-premise. Run:AI can assist with setting up the repository. Network Requirements \u00b6 Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied: During Installation \u00b6 Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443 Post Installation \u00b6 In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443 User requirements \u00b6 Usage of containers and images: The individual researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software. Fractional GPU Requirements \u00b6 The Run:AI platform provides a unique technology that allows the sharing of a single GPU between multiple containers. Each container receives an isolated subset of the GPU memory. For more details see Walk-through: Using GPU Fractions . This technology has more stringent software requirements than the rest of the Run:AI system. Specifically, virtualization has been tested on: NVIDIA device driver 410.104 or later CUDA 9.0 or later TensorFlow, Keras or PyTorch We keep testing the technology on additional software.","title":"Prerequisites"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#kubernetes-software","text":"Run:AI requires Kubernetes 1.15 or above. Kubernetes 1.17 is recommended (as of June 2020). If you are using Red Hat OpenShift. The minimal version is OpenShift 4.3 which runs on Kubernetes 1.16.","title":"Kubernetes Software"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#nvidia-driver","text":"Run:AI requires all GPU nodes to be installed with NVIDIA driver version 384.81 or later due to this dependency.","title":"NVIDIA Driver"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#hardware-requirements","text":"Kubernetes: Dedicated CPU-only worker node: To save on expensive GPUs-based hardware, we recommend (though not a must), a dedicated, CPU-only worker machine. Run:AI requires the following resources: 4 CPUs 4GB of RAM At least 20GB of Disk space Shared data volume: Run:AI uses Kubernetes to abstract away the machine on which a container is running: Researcher containers: The Researcher's containers need to be able to access data from any machine in a uniform way, so as to access training data and code as well as save checkpoints, weights, and other machine-learning related artifacts. The Run:AI system needs to save data on a storage device that is not dependent on a specific node. Typically, this is achieved via Network File Storage (NFS) or Network-attached storage (NAS). NFS is usually the preferred method for Researchers which may require multi-read/write capabilities. Docker Registry With Run:AI, Workloads are based on Docker images. For container images to run on any machine, these images must be downloaded from a docker registry rather than reside on the local machine (though this also is possible ). You can use a public registry such as docker hub or set up a local registry on-premise. Run:AI can assist with setting up the repository.","title":"Hardware Requirements"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#network-requirements","text":"Run:AI user interface runs from the cloud. All container nodes must be able to connect to the Run:AI cloud. Inbound connectivity (connecting from the cloud into nodes) is not required. If outbound connectivity is proxied/limited, the following exceptions should be applied:","title":"Network Requirements"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#during-installation","text":"Run:AI requires an installation over the Kubernetes cluster. The installation access the web to download various images and registries. Some organizations place limitations on what you can pull from the internet. The following list shows the various solution components and their origin: Name Description URLs Ports Run:AI Repository The Run:AI Package Repository is hosted on Run:AI\u2019s account on Google Cloud runai-charts.storage.googleapis.com 443 Docker Images Repository Various Run:AI images hub.docker.com gcr.io/run-ai-prod 443 Docker Images Repository Various third party Images quay.io 443","title":"During Installation"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#post-installation","text":"In addition, once running, Run:AI will send metrics to two sources: Name Description URLs Ports Grafana Grafana Metrics Server prometheus-us-central1.grafana.net 443 Run:AI Run:AI Cloud instance app.run.ai 443","title":"Post Installation"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#user-requirements","text":"Usage of containers and images: The individual researcher's work is based on container images. Containers allow IT to create standard software environments based on mix and match of various cutting-edge software.","title":"User requirements"},{"location":"Administrator/Cluster-Setup/cluster-prerequisites/#fractional-gpu-requirements","text":"The Run:AI platform provides a unique technology that allows the sharing of a single GPU between multiple containers. Each container receives an isolated subset of the GPU memory. For more details see Walk-through: Using GPU Fractions . This technology has more stringent software requirements than the rest of the Run:AI system. Specifically, virtualization has been tested on: NVIDIA device driver 410.104 or later CUDA 9.0 or later TensorFlow, Keras or PyTorch We keep testing the technology on additional software.","title":"Fractional GPU Requirements"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/","text":"This section is a step by step guide for setting up a Run:AI cluster. A Run:AI cluster is installed on top of a Kubernetes cluster. A Run:AI cluster connects to the Run:AI backend on the cloud. The backend provides a control point as well as a monitoring and control user interface for Administrators. A customer may have multiple Run:AI Clusters, all connecting to a single backend. For additional details see the Run:AI system components Documents \u00b6 Review Run:AI cluster prerequisites . Step by step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions. Customization \u00b6 As part of the installation process, you will download a Run:AI Operator YAML file and apply it to the cluster. Before applying the file, you will often need to customize the file. For a list of customization see Cluster Install, section 3.2 Advanced Setup \u00b6 For advanced scenarios such as limiting the installation to specific cluster nodes or enforcing none-root containers see the Advanced section. Next Steps \u00b6 After setting up the cluster, you may want to start setting up researchers. See: Researcher Setup .","title":"Introduction"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#documents","text":"Review Run:AI cluster prerequisites . Step by step installation instructions . Look for troubleshooting tips if required. Upgrade cluster and delete cluster instructions.","title":"Documents"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#customization","text":"As part of the installation process, you will download a Run:AI Operator YAML file and apply it to the cluster. Before applying the file, you will often need to customize the file. For a list of customization see Cluster Install, section 3.2","title":"Customization"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#advanced-setup","text":"For advanced scenarios such as limiting the installation to specific cluster nodes or enforcing none-root containers see the Advanced section.","title":"Advanced Setup"},{"location":"Administrator/Cluster-Setup/cluster-setup-intro/#next-steps","text":"After setting up the cluster, you may want to start setting up researchers. See: Researcher Setup .","title":"Next Steps"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/","text":"Troubleshooting \u00b6 Determining the Health of a Run:AI Cluster \u00b6 To understand whether your Run:AI cluster is healthy you need perform the following verification tests: All Run:AI services are running. Data is sent to the cloud. A job dan be sumbitted. 1. Run:AI services are running \u00b6 Run: kubectl get pods -n runai Verify that all pods are in Running status. Run: kubectl get deployments -n runai kubectl get sts -n runai Verify that all items (deployments and statefulsets alike) are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:AI daemon-sets run on all nodes. Others run only on nodes which contain GPUs. Verify that for all daemon-sets the desired number is equal to current and to ready . 2. Data is sent to the cloud \u00b6 Log in to app.run.ai Verify that all metrics in the overview dashboard are showing. Specifically the list of nodes and the numeric indicators Go to Projects and create a new project. Find the new project using the CLI command: runai project list 3. Submit a job \u00b6 Submitting a job will allow you to verify that Run:AI scheduling service are in order. Make sure that the project you have created has a quota of at least 1 GPU Run: runai project set <project-name> runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the job is a Running state when running: runai list Verify that the job is showing on the job area in app.run.ai/Jobs Symptoms \u00b6 Metrics are not showing on Overview Dashboard \u00b6 Symptom: Some or all metrics are not showing in app.run.ai Typical root causes: NVIDIA prerequisites have not been met. Firewall related issues. Internal clock is not synced. NVIDIA related issues \u00b6 Run: runai pods -n runai | grep nvidia Select one of the nvidia pods and run: kubectl logs -n runai nvidia-device-plugin-daemonset-<id> If the log contains an error, it means that NVIDIA related prerequisites have not been met. Review step 1 in NVIDIA prerequisites . Verify that: Step 1.1: NVIDIA drivers are installed Step 1.2: NVIDIA Docker is installed. A typical issue here is the installation of the NVIDIA Container Toolkit instead of NVIDIA Docker 2 . Step 1.3: Verify that NVIDIA Docker is the default docker runtime If the system has recently been installed, verify that docker has restarted by running the aforementioned pkill command Check the status of Docker by running: sudo systemctl status docker Firewall issues \u00b6 Run: runai pods -n runai | grep agent Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server Remove the Run:AI default Storage Class if a default already exists. See: remove default storage class Clock is not synced \u00b6 Run: date on cluster nodes and see that date is in sync. Internal Database has not started \u00b6 Typical root cause: more than one default storage class is installed The Run:AI Cluster installation includes, by default, a storage class named local path provisioner which is installed as a default storage class. In some cases, your k8s cluster may already have a default storage class installed. In such cases you should disable the local path provisioner. Having two default storage classes will disable both the internal database and some of the metrics. Run: kubectl get storageclass And look for default storage classes. Run: kubectl describe pod -n runai runai-db-0 See that there is indeed a storage class error appearing To disable local path provisioner please run: kubectl edit runaiconfig -n runai Add the following lines under spec : local-path-provisioner : enabled : false","title":"Troubleshooting"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#determining-the-health-of-a-runai-cluster","text":"To understand whether your Run:AI cluster is healthy you need perform the following verification tests: All Run:AI services are running. Data is sent to the cloud. A job dan be sumbitted.","title":"Determining the Health of a Run:AI Cluster"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#1-runai-services-are-running","text":"Run: kubectl get pods -n runai Verify that all pods are in Running status. Run: kubectl get deployments -n runai kubectl get sts -n runai Verify that all items (deployments and statefulsets alike) are in a ready state (1/1) Run: kubectl get daemonset -n runai A Daemonset runs on every node. Some of the Run:AI daemon-sets run on all nodes. Others run only on nodes which contain GPUs. Verify that for all daemon-sets the desired number is equal to current and to ready .","title":"1. Run:AI services are running"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#2-data-is-sent-to-the-cloud","text":"Log in to app.run.ai Verify that all metrics in the overview dashboard are showing. Specifically the list of nodes and the numeric indicators Go to Projects and create a new project. Find the new project using the CLI command: runai project list","title":"2. Data is sent to the cloud"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#3-submit-a-job","text":"Submitting a job will allow you to verify that Run:AI scheduling service are in order. Make sure that the project you have created has a quota of at least 1 GPU Run: runai project set <project-name> runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 Verify that the job is a Running state when running: runai list Verify that the job is showing on the job area in app.run.ai/Jobs","title":"3. Submit a job"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#symptoms","text":"","title":"Symptoms"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#metrics-are-not-showing-on-overview-dashboard","text":"Symptom: Some or all metrics are not showing in app.run.ai Typical root causes: NVIDIA prerequisites have not been met. Firewall related issues. Internal clock is not synced.","title":"Metrics are not showing on Overview Dashboard"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#nvidia-related-issues","text":"Run: runai pods -n runai | grep nvidia Select one of the nvidia pods and run: kubectl logs -n runai nvidia-device-plugin-daemonset-<id> If the log contains an error, it means that NVIDIA related prerequisites have not been met. Review step 1 in NVIDIA prerequisites . Verify that: Step 1.1: NVIDIA drivers are installed Step 1.2: NVIDIA Docker is installed. A typical issue here is the installation of the NVIDIA Container Toolkit instead of NVIDIA Docker 2 . Step 1.3: Verify that NVIDIA Docker is the default docker runtime If the system has recently been installed, verify that docker has restarted by running the aforementioned pkill command Check the status of Docker by running: sudo systemctl status docker","title":"NVIDIA related issues"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#firewall-issues","text":"Run: runai pods -n runai | grep agent Select the agent's full name and run: kubectl logs -n runai runai-agent-<id> Verify that there are no errors. If there are connectivity related errors you may need to: Check your firewall for outbound connections. See the required permitted URL list in: Network requirements . If you need to setup an internet proxy or certificate, review: Installing Run:AI with an Internet Proxy Server Remove the Run:AI default Storage Class if a default already exists. See: remove default storage class","title":"Firewall issues"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#clock-is-not-synced","text":"Run: date on cluster nodes and see that date is in sync.","title":"Clock is not synced"},{"location":"Administrator/Cluster-Setup/cluster-troubleshooting/#internal-database-has-not-started","text":"Typical root cause: more than one default storage class is installed The Run:AI Cluster installation includes, by default, a storage class named local path provisioner which is installed as a default storage class. In some cases, your k8s cluster may already have a default storage class installed. In such cases you should disable the local path provisioner. Having two default storage classes will disable both the internal database and some of the metrics. Run: kubectl get storageclass And look for default storage classes. Run: kubectl describe pod -n runai runai-db-0 See that there is indeed a storage class error appearing To disable local path provisioner please run: kubectl edit runaiconfig -n runai Add the following lines under spec : local-path-provisioner : enabled : false","title":"Internal Database has not started"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/","text":"Upgrading a Cluster Installation \u00b6 Upgrade \u00b6 To upgrade a Run:AI cluster installation run the following kubectl set image -n runai deployment/runai-operator \\ runai-operator=gcr.io/run-ai-prod/operator:<NEW_VERSION> Replace NEW_VERSION with a version number you receive from Run:AI customer support To verify that the upgrade has succeeded run: kubectl get pods -n runai and make sure that all pods are running or completed. Find the current Run:AI version \u00b6 To find the current version of the Run:AI cluster, run: kubectl get deployment runai-operator -n runai -o jsonpath='{.spec.template.spec.containers[0].image}'","title":"Cluster Upgrade"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#upgrading-a-cluster-installation","text":"","title":"Upgrading a Cluster Installation"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#upgrade","text":"To upgrade a Run:AI cluster installation run the following kubectl set image -n runai deployment/runai-operator \\ runai-operator=gcr.io/run-ai-prod/operator:<NEW_VERSION> Replace NEW_VERSION with a version number you receive from Run:AI customer support To verify that the upgrade has succeeded run: kubectl get pods -n runai and make sure that all pods are running or completed.","title":"Upgrade"},{"location":"Administrator/Cluster-Setup/cluster-upgrade/#find-the-current-runai-version","text":"To find the current version of the Run:AI cluster, run: kubectl get deployment runai-operator -n runai -o jsonpath='{.spec.template.spec.containers[0].image}'","title":"Find the current Run:AI version"},{"location":"Administrator/Cluster-Setup/enforce-run-as-user/","text":"Introduction \u00b6 In docker, as well as in Kubernetes, the default for running containers is running as 'root'. The implication of running as root is that processes running within the container have enough permissions to change anything on the machine itself. This gives a lot of power to containers, but does not sit well with modern security standards. Specifically enterprise security. There are two runai submit flags which limit this behavior at the Researcher level: The flag --run-as-user starts the container without root access. The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.) However, these flags are voluntary. They are not enforced by the system. It is possible to set these flags as a cluster-wide default for the Run:AI CLI, such that all CLI users will be limited to non-root containers. Setting a Cluster-Wide Default \u00b6 Save the following in a file (cluster-config.yaml) apiVersion : v1 data : config : | enforceRunAsUser: true enforcePreventPrivilegeEscalation: true kind : ConfigMap metadata : name : cluster-config namespace : runai labels : runai/cluster-config : \"true\" Run: kubectl apply -f cluster-config.yaml Limitation This configuration limits non-root for all Run:AI CLI users. However, it does not prevent users or malicious actors from starting containers directly via Kubernetes API (e.g. via YAML files). There are third party enterprise tools that can provide this level of security.","title":"Enforce non-root Containers"},{"location":"Administrator/Cluster-Setup/enforce-run-as-user/#introduction","text":"In docker, as well as in Kubernetes, the default for running containers is running as 'root'. The implication of running as root is that processes running within the container have enough permissions to change anything on the machine itself. This gives a lot of power to containers, but does not sit well with modern security standards. Specifically enterprise security. There are two runai submit flags which limit this behavior at the Researcher level: The flag --run-as-user starts the container without root access. The flag --prevent-privilege-escalation prevents the container from elevating its own privileges into root (e.g. running sudo or changing system files.) However, these flags are voluntary. They are not enforced by the system. It is possible to set these flags as a cluster-wide default for the Run:AI CLI, such that all CLI users will be limited to non-root containers.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/enforce-run-as-user/#setting-a-cluster-wide-default","text":"Save the following in a file (cluster-config.yaml) apiVersion : v1 data : config : | enforceRunAsUser: true enforcePreventPrivilegeEscalation: true kind : ConfigMap metadata : name : cluster-config namespace : runai labels : runai/cluster-config : \"true\" Run: kubectl apply -f cluster-config.yaml Limitation This configuration limits non-root for all Run:AI CLI users. However, it does not prevent users or malicious actors from starting containers directly via Kubernetes API (e.g. via YAML files). There are third party enterprise tools that can provide this level of security.","title":"Setting a Cluster-Wide Default"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/","text":"Safely Remove a Node \u00b6 Every now and again, you may need to take down a node. Typically for maintenance purposes. The node about to be taken down may be running Jobs. Without additional preparations, these Jobs will abruptly come to an end, and wait for the node to come up. To allow the Jobs to gracefully shut-down and be immediately re-allocated to other nodes, perform the following: To get the name of the node, run: kubectl get nodes Then run: kubectl drain <NODE-NAME> The command will tell Kubernetes to not schedule any new jobs on the node and evict all currently running Jobs. Kubernetes will attempt to immediately schedule evicted Jobs to other nodes. You can then safely shutdown the node. When the node is up again, run: kubectl uncordon <NODE-NAME> The command tells Kubernetes that the node is ready again to accept Jobs. Node Memory Management \u00b6 It is possible for researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine). Symptoms \u00b6 The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue appears on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes: Resource Reservation \u00b6 Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup . This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved : cpu : 100m memory : 1G systemReserved : cpu : 100m memory : 1G Eviction \u00b6 Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard : memory.available : \"500Mi\" # Default value for evictionHard on kubelet nodefs.available : \"10%\" nodefs.inodesFree : \"5%\" imagefs.available : \"15%\" Please note that specifying values for evictionHard will override the default values on kubelet which are of very high importance. For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ .","title":"Kubernetes Best Practices"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#safely-remove-a-node","text":"Every now and again, you may need to take down a node. Typically for maintenance purposes. The node about to be taken down may be running Jobs. Without additional preparations, these Jobs will abruptly come to an end, and wait for the node to come up. To allow the Jobs to gracefully shut-down and be immediately re-allocated to other nodes, perform the following: To get the name of the node, run: kubectl get nodes Then run: kubectl drain <NODE-NAME> The command will tell Kubernetes to not schedule any new jobs on the node and evict all currently running Jobs. Kubernetes will attempt to immediately schedule evicted Jobs to other nodes. You can then safely shutdown the node. When the node is up again, run: kubectl uncordon <NODE-NAME> The command tells Kubernetes that the node is ready again to accept Jobs.","title":"Safely Remove a Node"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#node-memory-management","text":"It is possible for researchers to over-allocate memory to the extent that, if not managed properly, will destabilize the chosen node (machine).","title":"Node Memory Management"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#symptoms","text":"The node enters the \"NotReady\" state, and won't be \"Ready\" again until the resource issues have been fixed. This issue appears on certain versions of kubelet (1.17.4 for example), that have a bug which causes kubelet to not recover properly when encountering certain errors, and must be restarted manually. SSH to the node and overall node access can be very slow. When running \"top\" command, Memory availability appears to be low. To make sure the node remains stable regardless of any pod resources issues, Kubernetes offers two features to control the way resources are managed on the nodes:","title":"Symptoms"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#resource-reservation","text":"Kubernetes offers two variables that can be configured as part of kubelet configuration file: systemReserved kubeReserved When configured, these two variables \"tell\" kubelet to preserve a certain amount of resources for system processes (kernel, sshd, .etc) and for Kubernetes node components (like kubelet) respectively. When configuring these variables alongside a third argument that is configured by default ( --enforce-node-allocatable), kubelet limits the amount of resources that can be consumed by pods on the node (Total Amount - kubeReseved - systemReserved), based on a Linux feature called cgroup . This limitation ensures that in any situation where the total amount of memory consumed by pods on a node grows above the allowed limit, Linux itself will start to evict pods that consume more resources than requested. This way, important processes are guaranteed to have a minimum amount of resources available. To configure, edit the file /etc/kubernetes/kubelet-config.yaml and add the following: kubeReserved : cpu : 100m memory : 1G systemReserved : cpu : 100m memory : 1G","title":"Resource Reservation"},{"location":"Administrator/Cluster-Setup/kubernetes-config-best-practices/#eviction","text":"Another argument that can be passed to kubelet is evictionHard, which specifies an absolute amount of memory that should always be available on the node. Setting this argument guarantees that critical processes might have extra room to expand above their reserved resources in case they need to and prevent starvation for those processes on the node. If the amount of memory available on the nodes drops below the configured value, kubelet will start to evict pods on the node. This enforcement is made by kubelet itself, and therefore less reliable, but it lowers the chance for resource issues on the node, and therefore recommended for use. To configure, please update the file /etc/kubernetes/kubelet-config.yaml with the following: evictionHard : memory.available : \"500Mi\" # Default value for evictionHard on kubelet nodefs.available : \"10%\" nodefs.inodesFree : \"5%\" imagefs.available : \"15%\" Please note that specifying values for evictionHard will override the default values on kubelet which are of very high importance. For further reading please refer to https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ .","title":"Eviction"},{"location":"Administrator/Cluster-Setup/limit-runai-install-to-specific-nodes/","text":"Why? \u00b6 Sometimes you have a Kubernetes cluster which is not dedicated to Run:AI. You can choose to limit Run:AI to install only on specific nodes. Note that if you do that, you will lose the benefits of Node visibility of the Run:AI Administration User Interface. How? \u00b6 Set a label on the Nodes you want Run:AI to run on: kubectl label node <node-name> run.ai/enable.scheduling=\"true\" Then run the following: kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/gpu-metrics-exporter/gpuLabel\", \"value\": \"run.ai/enable.scheduling\"}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/nvidia-device-plugin\", \"value\": {\"gpuLabel\": \"run.ai/enable.scheduling\"}}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/memory-manager/gpuLabel\", \"value\": \"run.ai/enable.scheduling\"}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/prometheus-operator/prometheus-node-exporter\", \"value\": {\"nodeSelector\": {\"run.ai/enable.scheduling\": \"true\"}}}]'","title":"Limit Install to Specific Nodes"},{"location":"Administrator/Cluster-Setup/limit-runai-install-to-specific-nodes/#why","text":"Sometimes you have a Kubernetes cluster which is not dedicated to Run:AI. You can choose to limit Run:AI to install only on specific nodes. Note that if you do that, you will lose the benefits of Node visibility of the Run:AI Administration User Interface.","title":"Why?"},{"location":"Administrator/Cluster-Setup/limit-runai-install-to-specific-nodes/#how","text":"Set a label on the Nodes you want Run:AI to run on: kubectl label node <node-name> run.ai/enable.scheduling=\"true\" Then run the following: kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/gpu-metrics-exporter/gpuLabel\", \"value\": \"run.ai/enable.scheduling\"}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/nvidia-device-plugin\", \"value\": {\"gpuLabel\": \"run.ai/enable.scheduling\"}}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/memory-manager/gpuLabel\", \"value\": \"run.ai/enable.scheduling\"}]' kubectl patch runaiconfig runai - n runai -- type = 'json' \\ - p = '[{\"op\": \"add\", \"path\": \"/spec/prometheus-operator/prometheus-node-exporter\", \"value\": {\"nodeSelector\": {\"run.ai/enable.scheduling\": \"true\"}}}]'","title":"How?"},{"location":"Administrator/Cluster-Setup/nfs-install/","text":"Installing Run:AI over network file storage \u00b6 Introduction \u00b6 Run:AI is storaging data on a filesystem. How this storage is managed differs according to the customer environment and usage. When the installation is for production purposes, then it is a good practice to setup the system such that if one node is down, the Run:AI software will seamlessly migrate to another node. For this, the storage has to reside on shared storage The Run:AI cluster installation is performed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for nfs nfs : enabled : true server : <IP-address> path : /path/to/folder Set enabled to true and provide the NFS IP Address and an existing folder that Run:AI can use.","title":"Configure NFS"},{"location":"Administrator/Cluster-Setup/nfs-install/#installing-runai-over-network-file-storage","text":"","title":"Installing Run:AI over network file storage"},{"location":"Administrator/Cluster-Setup/nfs-install/#introduction","text":"Run:AI is storaging data on a filesystem. How this storage is managed differs according to the customer environment and usage. When the installation is for production purposes, then it is a good practice to setup the system such that if one node is down, the Run:AI software will seamlessly migrate to another node. For this, the storage has to reside on shared storage The Run:AI cluster installation is performed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for nfs nfs : enabled : true server : <IP-address> path : /path/to/folder Set enabled to true and provide the NFS IP Address and an existing folder that Run:AI can use.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/","text":"Use OpenID Connect, LDAP or SAML for Authentication and Authorization \u00b6 Introduction \u00b6 Run:AI uses its a mechanism for authentication and authorization which is based on a third-party ( auth0 ). This is good as a baseline, but for enterprises, such a scheme is not scalable. For an enterprise, keeping separate Users and Roles systems requires manual work, is error-prone, and increases the attack vector. As such, organizations typically use an organizational directory to store users and roles, allowing a single point of change for multiple systems Run:AI uses the OpenID Connect protocol to allow organizations to integrate their authentication & authorization system with Run:AI. With such a connector, Run:AI no longer has a standalone login page. instead, it differs to the organization's directory for authenticating users and for retrieving their roles (authorization) OpenID provides simple wrappers for LDAP and SAML. LDAP and SAML are similar protocols. Most notably, LDAP which is the underlying protocol for Microsoft Active Directory as well as other directories. OpenID Connect Configuration \u00b6 With Run:AI OpenID Connect you synchronize: Users Users' groups The Run:AI login page is app.run.ai and is the point of access to all Run:AI customers using the default login mechanism. When enabling the Run:AI OpenID connector, your company will be allocated a subdomain e.g. company.app.run.ai. When the user is not yet authenticated, company.app.run.ai will automatically redirect to your generic company's authentication page. Post authentication, the user will be redirected back to company.app.run.ai and can start working. Installation and Configuration \u00b6 Your company will need to create an OpenID Connect provider. We recommend dex. After installing dex, you will want to create a client and perform the following configuration: Enter a redirect URL which has been provided to you by Run:AI Generate a unique secret . The secret should be sent to Run:AI If you are using LDAP or SAML, configure the relevant connector for dex Locate the authentication redirection URL . The redirection URL should to be sent to Run:AI Create a public key in order for Run:AI to be able validate OAuth tokens. The public key should be sent to Run:AI Users and Roles \u00b6 Now, go to the authorization page on Run:AI app and configure the required authorization using either specific users or groups in your organization.","title":"Authentication and Authorization"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/#use-openid-connect-ldap-or-saml-for-authentication-and-authorization","text":"","title":"Use OpenID Connect, LDAP or SAML for Authentication and Authorization"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/#introduction","text":"Run:AI uses its a mechanism for authentication and authorization which is based on a third-party ( auth0 ). This is good as a baseline, but for enterprises, such a scheme is not scalable. For an enterprise, keeping separate Users and Roles systems requires manual work, is error-prone, and increases the attack vector. As such, organizations typically use an organizational directory to store users and roles, allowing a single point of change for multiple systems Run:AI uses the OpenID Connect protocol to allow organizations to integrate their authentication & authorization system with Run:AI. With such a connector, Run:AI no longer has a standalone login page. instead, it differs to the organization's directory for authenticating users and for retrieving their roles (authorization) OpenID provides simple wrappers for LDAP and SAML. LDAP and SAML are similar protocols. Most notably, LDAP which is the underlying protocol for Microsoft Active Directory as well as other directories.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/#openid-connect-configuration","text":"With Run:AI OpenID Connect you synchronize: Users Users' groups The Run:AI login page is app.run.ai and is the point of access to all Run:AI customers using the default login mechanism. When enabling the Run:AI OpenID connector, your company will be allocated a subdomain e.g. company.app.run.ai. When the user is not yet authenticated, company.app.run.ai will automatically redirect to your generic company's authentication page. Post authentication, the user will be redirected back to company.app.run.ai and can start working.","title":"OpenID Connect Configuration"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/#installation-and-configuration","text":"Your company will need to create an OpenID Connect provider. We recommend dex. After installing dex, you will want to create a client and perform the following configuration: Enter a redirect URL which has been provided to you by Run:AI Generate a unique secret . The secret should be sent to Run:AI If you are using LDAP or SAML, configure the relevant connector for dex Locate the authentication redirection URL . The redirection URL should to be sent to Run:AI Create a public key in order for Run:AI to be able validate OAuth tokens. The public key should be sent to Run:AI","title":"Installation and Configuration"},{"location":"Administrator/Cluster-Setup/open-id-connectivity/#users-and-roles","text":"Now, go to the authorization page on Run:AI app and configure the required authorization using either specific users or groups in your organization.","title":"Users and Roles"},{"location":"Administrator/Cluster-Setup/proxy-server/","text":"Introduction \u00b6 Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\". In some organizations, outbound connectivity requires a proxy. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work. Run:AI Configuration \u00b6 The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. The Run:AI cluster installation is performed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for httpProxy httpProxy : enabled : false tlsCert : |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true and paste the contents of the certificate under tlsCert .","title":"Add an Internet Proxy Server "},{"location":"Administrator/Cluster-Setup/proxy-server/#introduction","text":"Run:AI is installed on GPU clusters. These clusters must have outbound internet connectivity to the Run:AI cloud. Details can be found here: Run-AI-GPU-Cluster-Prerequisites under \"Network Requirements\". In some organizations, outbound connectivity requires a proxy. Traffic originating from servers and browsers within the organizations flows through a gateway that inspects the traffic, calls the destination and returns the contents. Organizations sometimes employ a further security measure by signing packets with an organizational certificate. The software initiating the HTTP request must acknowledge this certificate, otherwise, it would interpret it as a man-in-the-middle attack. In-case the certificate is not trusted (or is a self-signed certificate), this certificate must be included in Run:AI configuration for outbound connectivity to work.","title":"Introduction"},{"location":"Administrator/Cluster-Setup/proxy-server/#runai-configuration","text":"The instructions below receive as input a certificate file from the organization and deploy it into the Run:AI cluster so that traffic originating in Run:AI will recognize the organizational proxy server. The Run:AI cluster installation is performed by accessing the Administrator User Interface at app.run.ai downloading a YAML file runai-operator.yaml and then applying it to Kubernetes. You must edit the YAML file. Search for httpProxy httpProxy : enabled : false tlsCert : |- -----BEGIN CERTIFICATE----- <CERTIFICATE_CONTENTS> -----END CERTIFICATE----- Set enabled to true and paste the contents of the certificate under tlsCert .","title":"Run:AI Configuration"},{"location":"Administrator/Presentations/Administrator-Onboarding-Presentation/","text":"","title":"Administrator Onboarding"},{"location":"Administrator/Researcher-Setup/cli-install/","text":"The Run:AI Command-line Interface (CLI) is one of the ways for a researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. The instructions below will guide you through the process of installing the CLI. Prerequisites \u00b6 Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ Helm. See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster Installation \u00b6 Kubernetes Configuration \u00b6 The Run:AI CLI needs to be connected to the Kubernetes Cluster containing the GPU nodes: Create a directory .kube . Copy the Kubernetes configuration file into the directory Create a shell variable to point to the above configuration file. Example: export KUBECONFIG =~/. kube / config Test the connection by running: kubectl get nodes Run:AI CLI Installation \u00b6 Download the latest release from the Run:AI releases page https://github.com/run-ai/runai-cli/releases Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai list Troubleshooting the CLI Installation \u00b6 See Troubleshooting a CLI installation Updating the Run:AI CLI \u00b6 To update the CLI to the latest version run: sudo runai update","title":"Install the CLI"},{"location":"Administrator/Researcher-Setup/cli-install/#prerequisites","text":"Kubectl (Kubernetes command-line interface) installed and configured to access your cluster. Please refer to https://kubernetes.io/docs/tasks/tools/install-kubectl/ Helm. See https://helm.sh/docs/intro/install/ on how to install Helm. Run:AI works with Helm version 3 only (not helm 2). A Kubernetes configuration file obtained from a computer previously connected to the Kubernetes cluster","title":"Prerequisites"},{"location":"Administrator/Researcher-Setup/cli-install/#installation","text":"","title":"Installation"},{"location":"Administrator/Researcher-Setup/cli-install/#kubernetes-configuration","text":"The Run:AI CLI needs to be connected to the Kubernetes Cluster containing the GPU nodes: Create a directory .kube . Copy the Kubernetes configuration file into the directory Create a shell variable to point to the above configuration file. Example: export KUBECONFIG =~/. kube / config Test the connection by running: kubectl get nodes","title":"Kubernetes Configuration"},{"location":"Administrator/Researcher-Setup/cli-install/#runai-cli-installation","text":"Download the latest release from the Run:AI releases page https://github.com/run-ai/runai-cli/releases Unarchive the downloaded file Install by running: sudo ./install-runai.sh To verify the installation run: runai list","title":"Run:AI CLI Installation"},{"location":"Administrator/Researcher-Setup/cli-install/#troubleshooting-the-cli-installation","text":"See Troubleshooting a CLI installation","title":"Troubleshooting the CLI Installation"},{"location":"Administrator/Researcher-Setup/cli-install/#updating-the-runai-cli","text":"To update the CLI to the latest version run: sudo runai update","title":"Updating the Run:AI CLI"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/","text":"When running the CLI you get an error an invalid configuration error \u00b6 When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided Solution \u00b6 Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory which contains a config file pointing to the Kubernetes cluster When running the CLI you get an error: open .../.kube/config.lock: permission denied \u00b6 When running any CLI command you get a permission denied error. Solution \u00b6 The user running the CLI does not have read permissions to the .kube directory","title":"Troubleshooting"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#when-running-the-cli-you-get-an-error-an-invalid-configuration-error","text":"When running any CLI command you get: FATA[0000] invalid configuration: no configuration has been provided","title":"When running the CLI you get an error an invalid configuration error"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution","text":"Your machine is not connected to the Kubernetes cluster. Make sure that you have a ~/.kube directory which contains a config file pointing to the Kubernetes cluster","title":"Solution"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#when-running-the-cli-you-get-an-error-open-kubeconfiglock-permission-denied","text":"When running any CLI command you get a permission denied error.","title":"When running the CLI you get an error: open .../.kube/config.lock: permission denied"},{"location":"Administrator/Researcher-Setup/cli-troubleshooting/#solution_1","text":"The user running the CLI does not have read permissions to the .kube directory","title":"Solution"},{"location":"Administrator/Researcher-Setup/docker-registry-config/","text":"Using a Docker Registry with Credentials \u00b6 Why? \u00b6 Some Docker images are stored in private docker registries. In order for the researcher to access the images, we will need to provide credentials for the registry. How? \u00b6 For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret_name may be any arbitrary string user_name and password are the repository user and password Note : the secret may take up to a minute to update in the system. Google Cloud Registry \u00b6 Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, login to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr - secret namespace : runai labels : runai / cluster - wide : \"true\" data : . dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes . io / dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Use a Private Docker Registry"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#using-a-docker-registry-with-credentials","text":"","title":"Using a Docker Registry with Credentials"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#why","text":"Some Docker images are stored in private docker registries. In order for the researcher to access the images, we will need to provide credentials for the registry.","title":"Why?"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#how","text":"For each private registry you must perform the following (The example below uses Docker Hub): kubectl create secret docker-registry <secret_name> -n runai \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=<user_name> --docker-password=<password> Then: kubectl label secret <secret_name> runai/cluster-wide=\"true\" -n runai secret_name may be any arbitrary string user_name and password are the repository user and password Note : the secret may take up to a minute to update in the system.","title":"How?"},{"location":"Administrator/Researcher-Setup/docker-registry-config/#google-cloud-registry","text":"Follow the steps below to access private images in the Google Container Registry (GCR): Create a service-account in GCP. Provide it Viewer permissions and download a JSON key. Under GCR, go to image and locate the domain name. Example GCR domains can be gcr.io , eu.gcr.io etc. On your local machine, login to docker with the new credentials: docker login -u _json_key -p \"$(cat <config.json>)\" <gcr-domain> Where <gcr-domain> is the GCR domain we have located, <config.json> is the GCP configuration file. This will generate an entry for the GCR domain in your ~/.docker/config.json file . Open the ~/.docker/config.json file. Copy the JSON structure under the GCR domain into a new file called ~/docker-config.json . When doing so, take care to remove all newlines . For example: {\"https://eu.gcr.io\": { \"auth\": \"<key>\"}} Convert the file into base64: cat ~/docker-config.json | base64 Create a new file called secret.yaml : apiVersion : v1 kind : Secret metadata : name : gcr - secret namespace : runai labels : runai / cluster - wide : \"true\" data : . dockerconfigjson : << PASTE_HERE_THE_LONG_BASE64_ENCODED_STRING >> type : kubernetes . io / dockerconfigjson Apply to Kubernetes by running the command: kubectl create -f ~/secret.yaml Test your settings by submitting a which references an image from the GCR repository","title":"Google Cloud Registry"},{"location":"Administrator/Researcher-Setup/docker-to-runai/","text":"Dockers, Images, and Kubernetes \u00b6 Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the researcher's workflow as follows. Image Repository \u00b6 If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web. Data \u00b6 Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command). Working with Containers \u00b6 Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference . Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation","title":"Switch from Docker to Run:AI "},{"location":"Administrator/Researcher-Setup/docker-to-runai/#dockers-images-and-kubernetes","text":"Researchers are typically proficient in working with Docker. Docker is an isolation level above the operating system which allows creating your own bundle of the operating system + deep learning environment and packaging it within a single file. The file is called a docker image . You create a container by starting a docker image on a machine. Run:AI is based on Kubernetes . At its core, Kubernetes is a an orchestration software above Docker: Among other things, it allows location abstraction as to where the actual container is running. This calls for some adaptation to the researcher's workflow as follows.","title":"Dockers, Images, and Kubernetes"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#image-repository","text":"If your Kubernetes cluster contains a single GPU node (machine), then your image can reside on the node itself (in which case, when runai submit workloads, the researcher must use the flag --local-image ). If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the image can no longer reside on the node itself. It must be relocated to an image repository. There are quite a few repository-as-a-service, most notably Docker hub . Alternatively, the organization can install a private repository on-premise. Day to day work with the image located remotely is almost identical to local work. The image name now contains its location. For example, nvcr.io/nvidia/pytorch:19.12-py_3 is a PyTorch image that is located in nvcr.io . This is the Nvidia image repository as found on the web.","title":"Image Repository"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#data","text":"Deep learning is about data. It can be your code, the training data, saved checkpoints, etc. If your Kubernetes cluster contains a single GPU node (machine), then your data can reside on the node itself. If your Kubernetes cluster contains more than a single node, then, to enable location abstraction, the data must sit outside the machine, typically on network storage. The storage must be uniformly mapped to your container when it starts (using the -v command).","title":"Data"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#working-with-containers","text":"Starting a container using docker usually involves a single command-line with multiple flags. A typical example: docker run --runtime=nvidia --shm-size 16G -it --rm -e HOSTNAME='hostname' \\ -v /raid/public/my_datasets:/root/dataset:ro -i nvcr.io/nvidia/pytorch:19.12-py3 The docker command docker run should be replaced with a Run:AI command runai submit . The flags are usually the same but some adaptation is required. A complete list of flags can be found here: runai submit . There are similar commands to get a shell into the container ( runai bash ), get the container logs ( runai logs ) and more. For a complete list see the Run:AI CLI reference .","title":"Working with Containers"},{"location":"Administrator/Researcher-Setup/docker-to-runai/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/limit-to-gpus-in-nodes/","text":"Why? \u00b6 In a gradual approach for incorporating Run:AI in a research operation, it is possible that some researchers are using the system with Run:AI and some without. It is, therefore, possible to limit Run:AI to use specific nodes and specific GPUs within these nodes. How? \u00b6 To configure restrictions on certain nodes: Get the names of the nodes where you want to limit Run:AI and the GPU indices inside these nodes where you want Run:AI to be enabled . For example. let\u2019s say you want Run:AI scheduling for GPUs 1 and 3 on node_2, GPUs 0 and 2 on node_4, and for all GPUs on every other node. Run the following command: kubectl create configmap nvidia - device - plugin - config - n runai \\ -- from - literal node_2 = \u201c 1 , 3 \u201d -- from - literal node_4 = \u201c 0 , 2 \" && \\ kubectl delete pod -n runai -l app=pod-gpu-metrics-exporter && \\ kubectl delete pod -n runai -l name=nvidia-device-plugin-ds Note : if names of nodes contain dashes/hyphens (\u2018-\u2019), they should be replaced with underscores (\u2018_\u2019) inside the command from step 2 (e.g if a node is called node-2, we will write it as node_2 in the command).","title":"Limit to gpus in nodes"},{"location":"Administrator/Researcher-Setup/limit-to-gpus-in-nodes/#why","text":"In a gradual approach for incorporating Run:AI in a research operation, it is possible that some researchers are using the system with Run:AI and some without. It is, therefore, possible to limit Run:AI to use specific nodes and specific GPUs within these nodes.","title":"Why?"},{"location":"Administrator/Researcher-Setup/limit-to-gpus-in-nodes/#how","text":"To configure restrictions on certain nodes: Get the names of the nodes where you want to limit Run:AI and the GPU indices inside these nodes where you want Run:AI to be enabled . For example. let\u2019s say you want Run:AI scheduling for GPUs 1 and 3 on node_2, GPUs 0 and 2 on node_4, and for all GPUs on every other node. Run the following command: kubectl create configmap nvidia - device - plugin - config - n runai \\ -- from - literal node_2 = \u201c 1 , 3 \u201d -- from - literal node_4 = \u201c 0 , 2 \" && \\ kubectl delete pod -n runai -l app=pod-gpu-metrics-exporter && \\ kubectl delete pod -n runai -l name=nvidia-device-plugin-ds Note : if names of nodes contain dashes/hyphens (\u2018-\u2019), they should be replaced with underscores (\u2018_\u2019) inside the command from step 2 (e.g if a node is called node-2, we will write it as node_2 in the command).","title":"How?"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/","text":"Why? \u00b6 In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation Configuring Node Groups \u00b6 To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value Using Node Groups via the CLI \u00b6 Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information Assigning Node Groups to a Project \u00b6 To assign specific node groups to a project see working with projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the project.","title":"Limit Run:AI to Specific Nodes"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#why","text":"In some business scenarios, you may want to direct the Run:AI scheduler to schedule a Workload to a specific node or a node group. For example, in some academic institutions, hardware is bought using a specific grant and thus \"belongs\" to a specific research group. Run:AI allows this \"taint\" by labeling a node, or a set of nodes and then during scheduling, using the flag --node-type <label> to force this allocation","title":"Why?"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#configuring-node-groups","text":"To configure a node group: Get the names of the nodes where you want to limit Run:AI. To get a list of nodes, run: kubectl get nodes For each node run the following: kubectl label node <node-name> run.ai/type=<label> The same value can be set to a single node, or for multiple nodes. A node can only be set with a single value","title":"Configuring Node Groups"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#using-node-groups-via-the-cli","text":"Use the node type label with the --node-type flag, such as: runai submit job1 ... --node-type \"my-nodes\" See the runai submit documentation for further information","title":"Using Node Groups via the CLI"},{"location":"Administrator/Researcher-Setup/limit-to-node-group/#assigning-node-groups-to-a-project","text":"To assign specific node groups to a project see working with projects . When the CLI flag is used in conjunction with Project-based affinity, the flag is used to refine the list of allowable node groups set in the project.","title":"Assigning Node Groups to a Project"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/","text":"Following is a step by step guide for getting a new researcher up to speed with Run:AI and Kubernetes. Change of Paradigms: from Docker to Kubernetes \u00b6 As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI. Setup the Run:AI Command-Line Interface \u00b6 Run:AI CLI needs to be installed on the researcher machine. This document provides step by step instructions. Provide the Researcher with a GPU Quota \u00b6 To submit workloads with Run:AI, the researcher must be provided with a \"project\" which contains a GPU quota. Please see Working with Projects document on how to create projects and set a quota. Provide access to the Run:AI Administration UI \u00b6 Some organizations would want to provide researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI ( app.run.ai) . See this document for further information on how to provide access. Schedule an Onboarding Session \u00b6 It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation .","title":"Introduction"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#change-of-paradigms-from-docker-to-kubernetes","text":"As part of Run:AI, the organization is typically moving from Docker-based workflows to Kubernetes. This document is an attempt to help the researcher with this paradigm shift. It explains the basic concepts and provides links for further information about the Run:AI CLI.","title":"Change of Paradigms: from Docker to Kubernetes"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#setup-the-runai-command-line-interface","text":"Run:AI CLI needs to be installed on the researcher machine. This document provides step by step instructions.","title":"Setup the Run:AI Command-Line Interface"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#provide-the-researcher-with-a-gpu-quota","text":"To submit workloads with Run:AI, the researcher must be provided with a \"project\" which contains a GPU quota. Please see Working with Projects document on how to create projects and set a quota.","title":"Provide the Researcher with a GPU Quota"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#provide-access-to-the-runai-administration-ui","text":"Some organizations would want to provide researchers with a more holistic view of what is happening in the cluster. You can do that by providing the appropriate access to the Run:AI Administration UI ( app.run.ai) . See this document for further information on how to provide access.","title":"Provide access to the Run:AI Administration UI"},{"location":"Administrator/Researcher-Setup/researcher-setup-intro/#schedule-an-onboarding-session","text":"It is highly recommended to schedule an onboarding session for researchers with a Run:AI customer success professional. Run:AI can help with the above transition, but adding to that, we at Run:AI have also acquired a large body of knowledge on data science best practices which can help streamline the researcher work as well as save money for the organization. Researcher onboarding material also appears in the Researcher Onboarding Presentation .","title":"Schedule an Onboarding Session"},{"location":"Administrator/Researcher-Setup/template-config/","text":"Note Templates are currently not supported. Configure Command-Line Interface Templates \u00b6 What are Templates? \u00b6 Templates are a way to reduce the number of flags required when using the Command-Line Interface to start workloads. Using Templates the researcher can: Review list of templates by running runai template list Review the contents of a specific template by running runai template get <template-name> Use a template by running runai submit --template <template-name> The purpose of this document is to provide the administrator with guidelines on how to create templates. Template Implementation \u00b6 CLI Templates are implemented as_ Kubernetes ConfigMaps . A Kubernetes ConfigMap is the standard way to save cluster-wide settings. To create a Run:AI CLI Template, you will need to save a ConfigMap on the cluster. The Run:AI CLI is then looking for ConfigMaps marked as Run:AI templates. Template Usage \u00b6 The Researcher can use the Run:AI CLI to Show a list of templates: runai template list Showing the properties of a specific template: runai template get <my-template> Use the template when submitting a workload runai submit <my-job> --template <my-template> For further details, see the Run:AI command-line reference template and submit functions. Template Syntax \u00b6 A template looks as follows: apiVersion : v1 kind : ConfigMap data : name : cli - template1 description : \"my first template\" values : | image : gcr . io /run-ai-demo/ quickstart gpu : 1 elastic : true metadata : name : cli - template1 namespace : runai labels : runai / template : \"true\" Notes: The template above set 3 defaults: a specific image, a default of 1 GPU and sets the \"elastic\" flag to true The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the runai template list command To store this template run: kubectl apply -f <template-file-name> For a complete list of template values, see the end of this document The Default Template \u00b6 The administrator can also set a default template that is always used on runai submit whenever a template is not specified. To create a default template use the annotation runai/default: \"true\". Example: apiVersion : v1 kind : ConfigMap data : name : cli - template1 description : \"my first template\" values : | volume : - '/path/on/host:/dest/container/path' metadata : name : cli - template1 namespace : runai annotations : runai / default : \"true\" labels : runai / template : \"true\" Flag Override Logic for Multi Values \u00b6 If your template specifies 1 GPU and the Researcher adds the --gpu flag for 2 GPUs, the system will use the Researcher's 2 GPU as an override. Some flags, such as volume, ports, args accept multiple values. Which bears the question XXX There are two override alternatives: The flag Syntax of all Values \u00b6 The following template sets all runai submit flags. apiVersion : v1 kind : ConfigMap data : name : \"all-cli-flags\" description : \"A sample showing all possible flag overrides via a template\" values : | alwaysPullImage : false # X args : - 'infinity' backoffLimit : 3 # X command : # X - 'sleep' cpu : 1.5 cpuLimit : 3 elastic : false # X environmentDefault : - 'LEARNING_RATE=0.25' - 'TEMP=302' gpu : 1 # X hostIPC : false # X hostNetwork : false # X image : ubuntu # X interactive : true # Yes, it created as an STS largeShm : false # X localImage : false # X memory : 1 G memoryLimit : 2 G nodeType : \"dgx-1\" # X portsDefault : - '80:8888' - 9090 preemptible : false # X project : \"team-ny\" runAsUser : true # ? largeShm : false # X serviceType : \"loadbalancer\" ttlAfterFinish : 30 # X volumeDefault : - '/etc:/dest/container/path' workingDir : \"/tmp\" metadata : name : cli - all - flags namespace : runai labels : runai / template : \"true\"","title":"Template config"},{"location":"Administrator/Researcher-Setup/template-config/#configure-command-line-interface-templates","text":"","title":"Configure Command-Line Interface Templates"},{"location":"Administrator/Researcher-Setup/template-config/#what-are-templates","text":"Templates are a way to reduce the number of flags required when using the Command-Line Interface to start workloads. Using Templates the researcher can: Review list of templates by running runai template list Review the contents of a specific template by running runai template get <template-name> Use a template by running runai submit --template <template-name> The purpose of this document is to provide the administrator with guidelines on how to create templates.","title":"What are Templates?"},{"location":"Administrator/Researcher-Setup/template-config/#template-implementation","text":"CLI Templates are implemented as_ Kubernetes ConfigMaps . A Kubernetes ConfigMap is the standard way to save cluster-wide settings. To create a Run:AI CLI Template, you will need to save a ConfigMap on the cluster. The Run:AI CLI is then looking for ConfigMaps marked as Run:AI templates.","title":"Template Implementation"},{"location":"Administrator/Researcher-Setup/template-config/#template-usage","text":"The Researcher can use the Run:AI CLI to Show a list of templates: runai template list Showing the properties of a specific template: runai template get <my-template> Use the template when submitting a workload runai submit <my-job> --template <my-template> For further details, see the Run:AI command-line reference template and submit functions.","title":"Template Usage"},{"location":"Administrator/Researcher-Setup/template-config/#template-syntax","text":"A template looks as follows: apiVersion : v1 kind : ConfigMap data : name : cli - template1 description : \"my first template\" values : | image : gcr . io /run-ai-demo/ quickstart gpu : 1 elastic : true metadata : name : cli - template1 namespace : runai labels : runai / template : \"true\" Notes: The template above set 3 defaults: a specific image, a default of 1 GPU and sets the \"elastic\" flag to true The label runai/template marks the ConfigMap as a Run:AI template. The name and description will show when using the runai template list command To store this template run: kubectl apply -f <template-file-name> For a complete list of template values, see the end of this document","title":"Template Syntax"},{"location":"Administrator/Researcher-Setup/template-config/#the-default-template","text":"The administrator can also set a default template that is always used on runai submit whenever a template is not specified. To create a default template use the annotation runai/default: \"true\". Example: apiVersion : v1 kind : ConfigMap data : name : cli - template1 description : \"my first template\" values : | volume : - '/path/on/host:/dest/container/path' metadata : name : cli - template1 namespace : runai annotations : runai / default : \"true\" labels : runai / template : \"true\"","title":"The Default Template"},{"location":"Administrator/Researcher-Setup/template-config/#flag-override-logic-for-multi-values","text":"If your template specifies 1 GPU and the Researcher adds the --gpu flag for 2 GPUs, the system will use the Researcher's 2 GPU as an override. Some flags, such as volume, ports, args accept multiple values. Which bears the question XXX There are two override alternatives: The flag","title":"Flag Override Logic for Multi Values"},{"location":"Administrator/Researcher-Setup/template-config/#syntax-of-all-values","text":"The following template sets all runai submit flags. apiVersion : v1 kind : ConfigMap data : name : \"all-cli-flags\" description : \"A sample showing all possible flag overrides via a template\" values : | alwaysPullImage : false # X args : - 'infinity' backoffLimit : 3 # X command : # X - 'sleep' cpu : 1.5 cpuLimit : 3 elastic : false # X environmentDefault : - 'LEARNING_RATE=0.25' - 'TEMP=302' gpu : 1 # X hostIPC : false # X hostNetwork : false # X image : ubuntu # X interactive : true # Yes, it created as an STS largeShm : false # X localImage : false # X memory : 1 G memoryLimit : 2 G nodeType : \"dgx-1\" # X portsDefault : - '80:8888' - 9090 preemptible : false # X project : \"team-ny\" runAsUser : true # ? largeShm : false # X serviceType : \"loadbalancer\" ttlAfterFinish : 30 # X volumeDefault : - '/etc:/dest/container/path' workingDir : \"/tmp\" metadata : name : cli - all - flags namespace : runai labels : runai / template : \"true\"","title":"Syntax of all Values"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/","text":"Rancher ( https://rancher.com/ ) is a software that manages Kubernetes clusters. Some customers provide Rancher to data scientists in order to launch workloads. This guide provides step by step instructions on how to launch workloads via Rancher. It assumes the reader has some familiarity with Rancher itself. There are other ways for data scientists to launch Workloads such as the Run:AI CLI or Kubeflow ( https://www.kubeflow.org/ ). The advantage of Rancher is the usage of a user interface. The disadvantage is that it exposes the data scientist to Kubernetes/Docker terminology that would otherwise remain hidden Types of Workloads \u00b6 We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Training session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports Terminology \u00b6 Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI projects\" (not to be confused with Rancher Projects). Each project contains a GPU quota. Each workload must be annotated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads Using Rancher to Launch Workloads \u00b6 Using your browser, navigate to Rancher Login to Rancher with your user name and password Click on the top left menu, go to the company's assigned cluster and default Rancher project (not to be confused with a Run:AI project) Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud a environment this would typically be a Layer-4 load balancer. On-premise environments depend on how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. * Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" * On the bottom and select show advanced options . Expand labels and labels and add 2 labels, adding the name of the user and the name of the project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Submit workloads via Rancher"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#types-of-workloads","text":"We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning run that has a start and a finish. A Training session can take from a few minutes to a couple of days. It can be interrupted in the midst and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Build workloads. Build workloads are interactive. They are used by data scientists to code a neural network and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. Coding is done by connecting a Jupyter notebook or PyCharm via TCP ports","title":"Types of Workloads"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#terminology","text":"Kubernetes Job - equivalent to the above definition of a Train workload. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" Kubernetes StatefulSet - equivalent to the above definition of Build workload. Suited for interactive sessions in which state is important in the sense that data not stored on a shared volume is gone when the session ends. StatefulSets must be manually stopped Kubernetes Labels - a method to add key-value pairs to a workload Kubernetes Node - a physical machine Kubernetes Scheduler - the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler Run:AI Project . The Run:AI scheduler schedules computing resources by associating Workloads with \"Run:AI projects\" (not to be confused with Rancher Projects). Each project contains a GPU quota. Each workload must be annotated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads","title":"Terminology"},{"location":"Administrator/tools/Launch-Workloads-using-Rancher-User-Interface/#using-rancher-to-launch-workloads","text":"Using your browser, navigate to Rancher Login to Rancher with your user name and password Click on the top left menu, go to the company's assigned cluster and default Rancher project (not to be confused with a Run:AI project) Press Deploy on the top right Add a Workload name Choose StatefulSet set for a build workload or Job for a train workload Select a docker image Select a Kubernetes Namespace (or remain with \"default\") Build workloads will typically require the assignment of TCP ports, for example, to externalize a jupyter notebook or a PyCharm editor. Select the ports that you want to expose. For each port select: (Optional) an informative name The internal port used by the software you want to connect to (e.g. Juypter notebook uses 8888 by default) The type of load balancer you want to use. For cloud a environment this would typically be a Layer-4 load balancer. On-premise environments depend on how your cluster was installed. Select a listening port which would be the external port you access through. Some load balancing solutions allow a random port. * Expand Node Scheduling and on the bottom right select \"show advanced options\". Under \"Scheduler\" write \"runai-scheduler\" * On the bottom and select show advanced options . Expand labels and labels and add 2 labels, adding the name of the user and the name of the project as follows: Expand \"Security and Host Config, at the bottom right add the number of requested GPUs Press \"Launch\" Wait for the Workload to launch. When done, you will see the list of exposed ports and can click on them to launch them in http Click on the Workload name, on the right you have a menu (3 vertical dots) which allow you to ssh into the Workload or view logs","title":"Using Rancher to Launch Workloads"},{"location":"Administrator/tools/dev-vscode/","text":"Use Visual Studio Code to work with a Run:AI Job \u00b6 Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from Visual Studio Code . Submit a Workload \u00b6 You will need your image to run an ssh server (e.g OpenSSH ). For the purposes of this document, we used a sample Docker Hub repository which runs sshd with root user and password: runai submit build-remote -i rastasheep/ubuntu-sshd:14.04 -g 1 --interactive \\ --command \"/usr/sbin/sshd\" --args \"-D\" --service-type=nodeport --port 30022:22 The job starts an sshd server on port 22. The job redirects the external port 30,022 to port 22 and uses a Node Port service type. Run: runai list Next to the job, under the \"Service URL\" column you will find the IP address and port. Visual Studio Code \u00b6 Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Work with Visual Studio Code"},{"location":"Administrator/tools/dev-vscode/#use-visual-studio-code-to-work-with-a-runai-job","text":"Once you launch a workload using Run:AI, you will want to connect to it. You can do so via command-line or via other tools such as a Jupyter Notebook This document is about accessing the remote container created by Run:AI, from Visual Studio Code .","title":"Use Visual Studio Code to work with a Run:AI Job"},{"location":"Administrator/tools/dev-vscode/#submit-a-workload","text":"You will need your image to run an ssh server (e.g OpenSSH ). For the purposes of this document, we used a sample Docker Hub repository which runs sshd with root user and password: runai submit build-remote -i rastasheep/ubuntu-sshd:14.04 -g 1 --interactive \\ --command \"/usr/sbin/sshd\" --args \"-D\" --service-type=nodeport --port 30022:22 The job starts an sshd server on port 22. The job redirects the external port 30,022 to port 22 and uses a Node Port service type. Run: runai list Next to the job, under the \"Service URL\" column you will find the IP address and port.","title":"Submit a Workload"},{"location":"Administrator/tools/dev-vscode/#visual-studio-code","text":"Under Visual Studio code install the Remote SSH extension. Create an ssh entry to the service by editing .ssh/config file or use the command Remote-SSH: Connect to Host... from the Command Palette. Enter the IP address and port from above (e.g. ssh root@35.34.212.12 -p 30022). User and password are root Using VS Code, install the Python extension on the remote machine Write your first python code and run it remotely.","title":"Visual Studio Code"},{"location":"Administrator/tools/launch-job-via-yaml/","text":"Launch a Run:AI Job via YAML \u00b6 The easiest way to submit jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases were you want to forgo the CLI and use direct YAML calls. A frequent scenario for using the Kubernetes YAML syntax to submit jobs is integrations . Researchers may already be working with an existing system that submits jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough. Terminology \u00b6 We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. The internal Kubernetes implementation of Train and Build are Kubernetes Job and Kubernetes StatesfulSet respectively: A Kubernetes Job is used for Train workloads. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" A Kubernetes StatefulSet is used for Build workloads. Build workloads are interactive sessions. StatefulSets do not end on their own. Instead they must be manually stopped Run:AI extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler The Run:AI scheduler schedules computing resources by associating Workloads with Run:AI Projects : A project is assigned with a GPU quota through the Run:AI Administrator user interface. Each workload must be associated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads Internally, Run:AI Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set. Submit Workloads \u00b6 Train jobs \u00b6 A Train job is equivalent to not using the CLI --interactive flag when calling runai submit . Assuming you have the following parameters: <JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart <PROJECT-NAME> . The name of the Project as created on the Administrator UI. Run: runai project list to see the list of currently available projects. <USER-NAME> User name running the Job. The name is used for display purposes only (not for authentication purposes). <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2 Copy the following into a file and change the parameters: apiVersion : batch/v1 kind : Job metadata : name : <JOB-NAME> spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler Run: kubectl apply -f <FILE-NAME> to submit the job. Note The runai submit CLI command includes many more flags. These flags can be correlated to Kubernetes API functions and added to the YAML above. Build jobs \u00b6 A Build job is equivalent to using the CLI --interactive flag when calling runai submit . Copy the following into a file and change the parameters: apiVersion : apps/v1 kind : \"StatefulSet\" metadata : name : <JOB-NAME> spec : serviceName : <JOB-NAME> replicas : 1 selector : matchLabels : release : <JOB-NAME> template : metadata : labels : user : <USER-NAME> release : <JOB-NAME> spec : schedulerName : runai-scheduler containers : - name : <JOB-NAME> command : - \"sleep\" args : - \"infinity\" image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> The YAML above contains a default command and arguments ( sleep --inifinty ) which you can replace. Run: kubectl apply -f <FILE-NAME> to submit the job. Using Fractional GPUs \u00b6 Jobs with Fractions requires a change in the above YAML. Specifically the limits section: limits : nvidia.com/gpu : <REQUESTED-GPUs> should be omitted and replaced with: spec : template : metadata : annotations : gpu-fraction : \"0.5\" where \"0.5\" is the requested GPU fraction. Delete Workloads \u00b6 To delete a Run:AI workload you need to delete the Job or StatefulSet according to the workload type kubectl delete job <JOB-NAME> or: kubectl delete sts <STS-NAME>","title":"Submit workloads via YAML"},{"location":"Administrator/tools/launch-job-via-yaml/#launch-a-runai-job-via-yaml","text":"The easiest way to submit jobs to the Run:AI GPU cluster is via the Run:AI Command-line interface (CLI). Still, the CLI is not a must. It is only a wrapper for a more detailed Kubernetes API syntax using YAML. There are cases were you want to forgo the CLI and use direct YAML calls. A frequent scenario for using the Kubernetes YAML syntax to submit jobs is integrations . Researchers may already be working with an existing system that submits jobs, and want to continue working with the same system. Though it is possible to call the Run:AI CLI from the customer's integration, it is sometimes not enough.","title":"Launch a Run:AI Job via YAML"},{"location":"Administrator/tools/launch-job-via-yaml/#terminology","text":"We differentiate between two types of Workloads: Train workloads. Training is characterized by a deep learning session that has a start and an end. A Training session can take anywhere from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. Training workloads typically utilize large percentages of GPU computing power and memory. Build workloads. Build workloads are interactive. They are used by data scientists to write machine learning code and test it against subsets of the data. Build workloads typically do not maximize usage of the GPU. The internal Kubernetes implementation of Train and Build are Kubernetes Job and Kubernetes StatesfulSet respectively: A Kubernetes Job is used for Train workloads. A Job has a distinctive \"end\" at which time the job is either \"Completed\" or \"Failed\" A Kubernetes StatefulSet is used for Build workloads. Build workloads are interactive sessions. StatefulSets do not end on their own. Instead they must be manually stopped Run:AI extends the Kubernetes Scheduler . A Kubernetes Scheduler is the software that determines which Workload to start on which node. Run:AI provides a custom scheduler named runai-scheduler The Run:AI scheduler schedules computing resources by associating Workloads with Run:AI Projects : A project is assigned with a GPU quota through the Run:AI Administrator user interface. Each workload must be associated with a project name and will receive resources according to the defined quota for the project and the currently running Workloads Internally, Run:AI Projects are implemented as Kubernetes namespaces. The scripts below assume that the code is being run after the relevant namespace has been set.","title":"Terminology"},{"location":"Administrator/tools/launch-job-via-yaml/#submit-workloads","text":"","title":"Submit Workloads"},{"location":"Administrator/tools/launch-job-via-yaml/#train-jobs","text":"A Train job is equivalent to not using the CLI --interactive flag when calling runai submit . Assuming you have the following parameters: <JOB-NAME> . The name of the Job. <IMAGE-NAME> . The name of the docker image to use. Example: gcr.io/run-ai-demo/quickstart <PROJECT-NAME> . The name of the Project as created on the Administrator UI. Run: runai project list to see the list of currently available projects. <USER-NAME> User name running the Job. The name is used for display purposes only (not for authentication purposes). <REQUESTED-GPUs> . An integer number of GPUs you request to be allocated for the Job. Examples: 1, 2 Copy the following into a file and change the parameters: apiVersion : batch/v1 kind : Job metadata : name : <JOB-NAME> spec : template : metadata : labels : user : <USER-NAME> spec : containers : - name : <JOB-NAME> image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> restartPolicy : Never schedulerName : runai-scheduler Run: kubectl apply -f <FILE-NAME> to submit the job. Note The runai submit CLI command includes many more flags. These flags can be correlated to Kubernetes API functions and added to the YAML above.","title":"Train jobs"},{"location":"Administrator/tools/launch-job-via-yaml/#build-jobs","text":"A Build job is equivalent to using the CLI --interactive flag when calling runai submit . Copy the following into a file and change the parameters: apiVersion : apps/v1 kind : \"StatefulSet\" metadata : name : <JOB-NAME> spec : serviceName : <JOB-NAME> replicas : 1 selector : matchLabels : release : <JOB-NAME> template : metadata : labels : user : <USER-NAME> release : <JOB-NAME> spec : schedulerName : runai-scheduler containers : - name : <JOB-NAME> command : - \"sleep\" args : - \"infinity\" image : <IMAGE-NAME> resources : limits : nvidia.com/gpu : <REQUESTED-GPUs> The YAML above contains a default command and arguments ( sleep --inifinty ) which you can replace. Run: kubectl apply -f <FILE-NAME> to submit the job.","title":"Build jobs"},{"location":"Administrator/tools/launch-job-via-yaml/#using-fractional-gpus","text":"Jobs with Fractions requires a change in the above YAML. Specifically the limits section: limits : nvidia.com/gpu : <REQUESTED-GPUs> should be omitted and replaced with: spec : template : metadata : annotations : gpu-fraction : \"0.5\" where \"0.5\" is the requested GPU fraction.","title":"Using Fractional GPUs"},{"location":"Administrator/tools/launch-job-via-yaml/#delete-workloads","text":"To delete a Run:AI workload you need to delete the Job or StatefulSet according to the workload type kubectl delete job <JOB-NAME> or: kubectl delete sts <STS-NAME>","title":"Delete Workloads"},{"location":"Researcher/overview-researcher/","text":"Overview: Researcher Documentation \u00b6 Researchers use Run:AI to submit jobs. As part of the Researcher documentation you will find: Walkthroughs which provide step-by-step guides to Run:AI technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:AI. Information about the Run:AI Scheduler . The Run:AI Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Introductory Presentations .","title":"Overview"},{"location":"Researcher/overview-researcher/#overview-researcher-documentation","text":"Researchers use Run:AI to submit jobs. As part of the Researcher documentation you will find: Walkthroughs which provide step-by-step guides to Run:AI technology. Command line interface reference documentation. Best Practices for Deep Learning with Run:AI. Information about the Run:AI Scheduler . The Run:AI Python Researcher Library which you can optionally use in your container to get additional reporting and further resource optimization. Introductory Presentations .","title":"Overview: Researcher Documentation"},{"location":"Researcher/Presentations/Researcher-Onboarding-Presentation/","text":"","title":"Researcher Onboarding"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/","text":"Introduction \u00b6 When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs. Requesting CPU & Memory \u00b6 When submitting a job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit CPU over allocation \u00b6 The number of CPUs your job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively. Memory over allocation \u00b6 The amount of Memory your job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your job may receive an out of memory exception and terminate. CPU and Memory limits \u00b6 You can limit your job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your job will never be allocated with more than the amount stated in the --cpu-limit flag If your job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. For further details on these flags see: runai submit Flag Defaults \u00b6 Defaults for --cpu flag \u00b6 If your job has not specified --cpu, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your job has only specified --gpu 2 and has not specified --cpu, then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change this default please contact Run:AI customer support. Defaults for --memory flag \u00b6 If your job has not specified --memory, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change this default please contact Run:AI customer support. Validating CPU & Memory Allocations \u00b6 To review a CPU & Memory allocations you need to look into Kubernetes. A Run:AI job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the job by running: runai get <JOB_NAME> the pod will appear under PODS . Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia . com / gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia . com / gpu : 2","title":"Allocation of CPU and Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#introduction","text":"When we discuss the allocation of deep learning compute resources, the discussion tends to focus on GPUs as the most critical resource. But there are two additional resources that are no less important: CPUs. Mostly needed for preprocessing and postprocessing tasks during a deep learning training run. Memory. Has a direct influence on the quantities of data a training run can process in batches. GPU servers tend to come installed with a significant amount of memory and CPUs.","title":"Introduction"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#requesting-cpu-memory","text":"When submitting a job, you can request a guaranteed amount of CPUs and memory by using the --cpu and --memory flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --memory 1G The system guarantees that if the job is scheduled, you will be able to receive this amount of CPU and memory. For further details on these flags see: runai submit","title":"Requesting CPU &amp; Memory"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-over-allocation","text":"The number of CPUs your job will receive is guaranteed to be the number defined using the --cpu flag. In practice, however, you may receive more CPUs than you have asked for: If you are currently alone on a node, you will receive all the node CPUs until such time when another workload has joined. However, when a second workload joins, each workload will receive a number of CPUs proportional to the number requested via the --cpu flag. For example, if the first workload asked for 1 CPU and the second for 3 CPUs, then on a node with 40 nodes, the workloads will receive 10 and 30 CPUs respectively.","title":"CPU over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#memory-over-allocation","text":"The amount of Memory your job will receive is guaranteed to be the number defined using the --memory flag. In practice, however, you may receive more memory than you have asked for. This is along the same lines as described with CPU over allocation above. It is important to note, however, that if you have used this memory over-allocation, and new workloads have joined, your job may receive an out of memory exception and terminate.","title":"Memory over allocation"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#cpu-and-memory-limits","text":"You can limit your job's allocation of CPU and memory by using the --cpu-limit and --memory-limit flags in the runai submit command. For example: runai submit job1 -i ubuntu --gpu 2 --cpu 12 --cpu-limit 24 \\ --memory 1G --memory-limit 4G The limit behavior is different for CPUs and memory. Your job will never be allocated with more than the amount stated in the --cpu-limit flag If your job tries to allocate more than the amount stated in the --memory-limit flag it will receive an out of memory exception. For further details on these flags see: runai submit","title":"CPU and Memory limits"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#flag-defaults","text":"","title":"Flag Defaults"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-cpu-flag","text":"If your job has not specified --cpu, the system will use a default. The default is cluster-wide and is defined as a ratio of GPUs to CPUs. Consider the default of 1:6. If your job has only specified --gpu 2 and has not specified --cpu, then the implied --cpu flag value is 12 CPUs. The system comes with a cluster-wide default of 1:1. To change this default please contact Run:AI customer support.","title":"Defaults for --cpu flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#defaults-for-memory-flag","text":"If your job has not specified --memory, the system will use a default. The default is cluster-wide and is proportional to the number of requested GPUs. The system comes with a cluster-wide default of 100MiB per GPU. To change this default please contact Run:AI customer support.","title":"Defaults for --memory flag"},{"location":"Researcher/Scheduling/Allocation-of-CPU-and-Memory/#validating-cpu-memory-allocations","text":"To review a CPU & Memory allocations you need to look into Kubernetes. A Run:AI job creates a Kubernetes pod . The pod declares its resource requests and limits. To see the memory and CPU consumption in Kubernetes: Get the pod name for the job by running: runai get <JOB_NAME> the pod will appear under PODS . Run: kubectl describe pod <POD_NAME> The information will appear under Requests and Limits . For example: Limits : nvidia . com / gpu : 2 Requests : cpu : 1 memory : 104857600 nvidia . com / gpu : 2","title":"Validating CPU &amp; Memory Allocations"},{"location":"Researcher/Scheduling/Job-Statuses/","text":"Introduction \u00b6 The runai submit function and its sibling the runai submit-mpi function submit Run:AI jobs for execution. A job has a status . Once a job is submitted it goes through a number of statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:AI specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job. Successful Flow \u00b6 A regular, training job which has no errors and executes without preemption would go through the following statuses: Pending - the job is waiting to be scheduled. ContainerCreating - the job has been scheduled, the Job docker image is now downloading. Running - the job is now executing. Succeeded - the job has finished with exit code 0 (success). The job can be preempted, in which case it can go through other statuses: Terminating - the job is now being preempted. Pending - the job is waiting in queue again to receive resources. An interactive job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see table below. Error flow \u00b6 A regular, training job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number or retires the job will enter a final status of Fail An interactive job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image which cannot be downloaded. There are special statuses for such jobs. See table below Status Table \u00b6 Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Administrator User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:AI Scheduler wasn't running when the job has finished. How to get more information \u00b6 The system (Kubernetes and Run:AI) store various events during the job's lifecycle. These events are helpful in diagnosing issues around job scheduling. To view these events run: runai get <job-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, jobs that have exited with an exit code different than zero may write an exit reason in this log. To see job logs run: runai logs <job-name> Distributed Training (mpi) jobs \u00b6 A distributed (mpi) job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Job Statuses"},{"location":"Researcher/Scheduling/Job-Statuses/#introduction","text":"The runai submit function and its sibling the runai submit-mpi function submit Run:AI jobs for execution. A job has a status . Once a job is submitted it goes through a number of statuses before ending in an End State . Most of these statuses originate in the underlying Kubernetes infrastructure, but some are Run:AI specific. The purpose of this document is to explain these statuses as well as the lifecycle of a Job.","title":"Introduction"},{"location":"Researcher/Scheduling/Job-Statuses/#successful-flow","text":"A regular, training job which has no errors and executes without preemption would go through the following statuses: Pending - the job is waiting to be scheduled. ContainerCreating - the job has been scheduled, the Job docker image is now downloading. Running - the job is now executing. Succeeded - the job has finished with exit code 0 (success). The job can be preempted, in which case it can go through other statuses: Terminating - the job is now being preempted. Pending - the job is waiting in queue again to receive resources. An interactive job, by definition, needs to be closed by the Researcher and will thus never reach the Succeeded status. Rather, it would be moved by the Researcher to status Deleted . For a further explanation of the additional statuses, see table below.","title":"Successful Flow"},{"location":"Researcher/Scheduling/Job-Statuses/#error-flow","text":"A regular, training job may encounter an error inside the running process (exit code is non-zero). In which case the following will happen: The job enters an Error status and then immediately tries to reschedule itself for another attempted run. The reschedule can happen on another node in the system. After a specified number or retires the job will enter a final status of Fail An interactive job, enters an Error status and then moves immediately to CrashLoopBackOff trying to reschedule itself. The reschedule attempt has no 'back-off' limit and will continue to retry indefinitely Jobs may be submitted with an image which cannot be downloaded. There are special statuses for such jobs. See table below","title":"Error flow"},{"location":"Researcher/Scheduling/Job-Statuses/#status-table","text":"Below is a list of statuses. For each status the list shows: Name End State - this status is the final status in the lifecycle of the Job Resource Allocation - when the Job is in this status, does the system allocate resources to it Description Color - Status color as can be seen in the Administrator User Interface Job list p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 13.3px Arial; color: #000000; -webkit-text-stroke: #000000; min-height: 15.0px} span.s1 {font-kerning: none} table.t1 {border-collapse: collapse; table-layout: fixed} td.td1 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td2 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td3 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td4 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td5 {width: 93.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #000000 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td6 {width: 172.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td7 {width: 48.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td8 {width: 82.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td9 {width: 456.0px; background-color: #ffffff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td10 {width: 93.0px; background-color: #599b3e; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td11 {width: 172.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td12 {width: 48.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td13 {width: 82.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td14 {width: 456.0px; background-color: #f0f0f0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td15 {width: 93.0px; background-color: #fd8608; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td16 {width: 93.0px; background-color: #0000ff; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td17 {width: 93.0px; background-color: #afafaf; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td18 {width: 93.0px; background-color: #fb0007; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td19 {width: 172.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #000000; padding: 2.0px 3.0px 2.0px 3.0px} td.td20 {width: 48.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td21 {width: 82.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td22 {width: 456.0px; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} td.td23 {width: 93.0px; background-color: #d0d0d0; border-style: solid; border-width: 1.0px 1.0px 1.0px 1.0px; border-color: #c1c1c1 #000000 #000000 #c1c1c1; padding: 2.0px 3.0px 2.0px 3.0px} Status End State Resource Allocation Description Color Running Yes Job is running successfully Terminating Yes Pod is being evicted at the moment (e.g. due to an over-quota allocation, the reason will be written once eviction finishes). A new pod will be created shortly ContainerCreating Yes Image is being pulled from registry. Pending - Job is pending. Possible reasons: - Not enough resources - Waiting in Queue (over quota etc). Succeeded Yes - An Unattended (training) Job has ran and finished successfully. Deleted Yes - Job has been deleted. TimedOut Yes - Interactive job has reached the defined timeout of the project. Preempted Yes - Interactive preemptible job has been evicted. ContainerCannotRun Yes - Container has failed to start running. This is typically a problem within the docker image itself. Error Yes for interactive only The job has returned an exit code different than zero. It is now waiting for another run attempt (retry). Fail Yes - Job has failed after a number of retries (according to \"--backoffLimit\" field) and will not be trying again. CrashLoopBackOff Yes Interactive Only: During backoff after Error, before a retry attempt to run pod on the same node. ErrImagePull, ImagePullBackOff Yes Failing to retrieve docker image Unknown Yes - The Run:AI Scheduler wasn't running when the job has finished.","title":"Status Table"},{"location":"Researcher/Scheduling/Job-Statuses/#how-to-get-more-information","text":"The system (Kubernetes and Run:AI) store various events during the job's lifecycle. These events are helpful in diagnosing issues around job scheduling. To view these events run: runai get <job-name> Sometimes, useful information can be found by looking at logs emitted from the process running inside the container. For example, jobs that have exited with an exit code different than zero may write an exit reason in this log. To see job logs run: runai logs <job-name>","title":"How to get more information"},{"location":"Researcher/Scheduling/Job-Statuses/#distributed-training-mpi-jobs","text":"A distributed (mpi) job, which has no errors will be slightly more complicated and has additional statuses associated with it. Distributed jobs start with an \"init container\" which sets the stage for a distributed run. When the init container finishes, the main \"launcher\" container is created. The launcher is responsible for coordinating between the different workers Workers run and do the actual work. A successful flow of distribute training would look as: Additional Statuses: Status End State Resource Allocation Description Color Init:<number A>/<number B> Yes The Pod has B Init Containers, and A have completed so far. PodInitializing Yes The pod has finished executing Init Containers. The system is creating the main 'launcher' container Init:Error An Init Container has failed to execute. Init:CrashLoopBackOff An Init Container has failed repeatedly to execute","title":"Distributed Training (mpi) jobs"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/","text":"Introduction \u00b6 At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works. Terminology \u00b6 Workload Types \u00b6 Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads.Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the researcher is to save checkpoints and allow the code to restore from the last checkpoint. Projects \u00b6 Projects are quota entities that associate a project name with a deserved GPU quota as well as other preferences. A researcher submitting a workload must associate a project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on projects and how to configure them, see: Working with Projects Departments \u00b6 A Department is a second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Project quota. For further information on departments and how to configure them, see: Working with Departments Pods \u00b6 Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparamter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory. Basic Scheduling Concepts \u00b6 Interactive vs. Unattended \u00b6 The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted. Guaranteed Quota and Over-Quota \u00b6 Every new workload is associated with a Project. The project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a project does not end up with over-quota made fully of interactive workloads. For additional details see below Scheduler Details \u00b6 Allocation & Preemption \u00b6 The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' project and continues with the same flow until it finishes attempting to schedule all workloads Reclaim \u00b6 During the above process, there may be a pending workload whose project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another project which has gone over-quota while preserving fairness between projects. Fairness \u00b6 The Run:AI scheduler determines fairness between multiple over-quota projects according to their GPU quota. Consider for example two projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different projects regardless of the time they entered the system. The fairness works according to the relative portion of GPU quota for each project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both projects go over quota, project A will receive 75% (=3/(1+3)) of the idle GPUs and project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new job is submitted to the system or existing job ends. This fairness equivalence will also be maintained amongst running jobs. The scheduler will preempt training sessions to maintain this equivalence Bin-packing & Consolidation \u00b6 Part of an efficient scheduler is the ability to eliminate defragmentation: The first step in avoiding defragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate jobs on demand. If a workload cannot be allocated due to defragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload. Elasticity \u00b6 Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of consolidation helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload. Advanced \u00b6 GPU Fractions \u00b6 Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB or memory, then the job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memorty exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made in fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Elasticity is not supported with fractions. Hyperparameter Optimization supports fractions. Distributed Training \u00b6 Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:AI spawns an additional launcher processs which manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what is typically multiple nodes, before the job can actually start. If one pod is preempted, the others are also immediately preempted. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:AI system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:AI see here Hyperparameter Optimization \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With HPO, the researcher provides a single script which is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods are exactly as described in the Scheduler Details section above for Jobs. For more information on Hyperparameter Optimization in Run:AI see here","title":"The Run:AI Scheduler"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#introduction","text":"At the heart of the Run:AI solution is the Run:AI scheduler. The scheduler is the gatekeeper of your organization's hardware resources. It makes decisions on resource allocations according to pre-created rules. The purpose of this document is to describe the Run:AI scheduler and explain how resource management works.","title":"Introduction"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#terminology","text":"","title":"Terminology"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#workload-types","text":"Run:AI differentiates between two types of deep learning workloads: Interactive build workloads. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads typically do not tax the GPU for a long duration. There are also typically real users behind an interactive workload that need an immediate scheduling response. Unattended (or \"non-interactive\") training workloads.Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. Training workloads typically utilize large percentages of the GPU. During the execution, the researcher can examine the results. A Training session can take anything from a few minutes to a couple of weeks. It can be interrupted in the middle and later restored. It follows that a good practice for the researcher is to save checkpoints and allow the code to restore from the last checkpoint.","title":"Workload Types"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#projects","text":"Projects are quota entities that associate a project name with a deserved GPU quota as well as other preferences. A researcher submitting a workload must associate a project with any workload request. The Run:AI scheduler will then compare the request against the current allocations and the project's deserved quota and determine whether the workload can be allocated with resources or whether it should remain in a pending state. For further information on projects and how to configure them, see: Working with Projects","title":"Projects"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#departments","text":"A Department is a second hierarchy of resource allocation above Project . A Department quota supersedes a Project quota in the sense that if the sum of Project quotas for Department A exceeds the Department quota -- the scheduler will use the Department quota rather than the Project quota. For further information on departments and how to configure them, see: Working with Departments","title":"Departments"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#pods","text":"Pods are units of work within a Job. Typically, each Job has a single Pod. However, in some scenarios (see Hyperparamter Optimization and Distribute Training below) there will be multiple Pods per Job. All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.","title":"Pods"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#basic-scheduling-concepts","text":"","title":"Basic Scheduling Concepts"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#interactive-vs-unattended","text":"The Researcher uses the --interactive flag to specify whether the workload is an unattended \"train\" workload or an interactive \"build\" workload. Interactive workloads will get precedence over unattended workloads. Unattended workloads can be preempted when the scheduler determines a more urgent need for resources. Interactive workloads are never preempted.","title":"Interactive vs. Unattended"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#guaranteed-quota-and-over-quota","text":"Every new workload is associated with a Project. The project contains a deserved GPU quota. During scheduling: If the newly required resources, together with currently used resources, end up within the project's quota, then the workload is ready to be scheduled as part of the guaranteed quota. If the newly required resources together with currently used resources end up above the project's quota, the workload will only be scheduled if there are 'spare' GPU resources. There are nuances in this flow which are meant to ensure that a project does not end up with over-quota made fully of interactive workloads. For additional details see below","title":"Guaranteed Quota and Over-Quota"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#scheduler-details","text":"","title":"Scheduler Details"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#allocation-preemption","text":"The Run:AI scheduler wakes up periodically to perform allocation tasks on pending workloads: The scheduler looks at each Project separately and selects the most 'deprived' Project. For this deprived project it chooses a single workload to work on: Interactive workloads are tried first, but only up to the project's guaranteed quota. If such a workload exists, it is scheduled even if it means preempting a running unattended workload in this Project. Else, it looks for an unattended workload and schedules it on guaranteed quota or over-quota. The scheduler then recalculates the next 'deprived' project and continues with the same flow until it finishes attempting to schedule all workloads","title":"Allocation &amp; Preemption"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#reclaim","text":"During the above process, there may be a pending workload whose project is below the deserved capacity. Still, it cannot be allocated due to the lack of GPU resources. The scheduler will then look for alternative allocations at the expense of another project which has gone over-quota while preserving fairness between projects.","title":"Reclaim"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#fairness","text":"The Run:AI scheduler determines fairness between multiple over-quota projects according to their GPU quota. Consider for example two projects, each spawning a significant amount of workloads (e.g. for Hyperparameter tuning) all of which wait in the queue to be executed. The Run:AI Scheduler allocates resources while preserving fairness between the different projects regardless of the time they entered the system. The fairness works according to the relative portion of GPU quota for each project. To further illustrate that, suppose that: Project A has been allocated with a quota of 3 GPUs. Project B has been allocated with a quota of 1 GPU. Then, if both projects go over quota, project A will receive 75% (=3/(1+3)) of the idle GPUs and project B will receive 25% (=1/(1+3)) of the idle GPUs. This ratio will be recalculated every time a new job is submitted to the system or existing job ends. This fairness equivalence will also be maintained amongst running jobs. The scheduler will preempt training sessions to maintain this equivalence","title":"Fairness"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#bin-packing-consolidation","text":"Part of an efficient scheduler is the ability to eliminate defragmentation: The first step in avoiding defragmentation is bin packing: try and fill nodes (machines) up before allocating workloads to new machines. The next step is to consolidate jobs on demand. If a workload cannot be allocated due to defragmentation, the scheduler will try and move unattended workloads from node to node in order to get the required amount of GPUs to schedule the pending workload.","title":"Bin-packing &amp; Consolidation"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#elasticity","text":"Run:AI Elasticity is explained here . In essence, it allows unattended workloads to shrink or expand based on the cluster's availability. Shrinking happens when the scheduler is unable to schedule an elastic unattended workload and no amount of consolidation helps. The scheduler then divides the requested GPUs by half again and again and tries to reschedule. Shrink jobs will expand when enough GPUs will be available. Expanding happens when the scheduler finds spare GPU resources, enough to double the amount of GPUs for an elastic workload.","title":"Elasticity"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#advanced","text":"","title":"Advanced"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#gpu-fractions","text":"Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. One important thing to note is that fraction scheduling divides up GPU memory . As such the GPU memory is divided up between jobs. If a Job asks for 0.5 GPU, and the GPU has 32GB or memory, then the job will see only 16GB. An attempt to allocate more than 16GB will result in an out-of-memorty exception. GPU Fractions are scheduled as regular GPUs in the sense that: Allocation is made in fractions such that the total of the GPU allocation for a single GPU is smaller or equal to 1. Preemption is available for non-interactive workloads. Bin-packing & Consolidation work the same for fractions. Support: Elasticity is not supported with fractions. Hyperparameter Optimization supports fractions.","title":"GPU Fractions"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#distributed-training","text":"Distributed Training, is the ability to split the training of a model among multiple processors. It is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. Each such split is a pod (see definition above). Run:AI spawns an additional launcher processs which manages and coordinates the other worker pods. Distribute Training utilizes a practice sometimes known as Gang Scheduling : The scheduler must ensure that multiple pods are started on what is typically multiple nodes, before the job can actually start. If one pod is preempted, the others are also immediately preempted. Gang Scheduling essentially prevents scenarios where part of the pods are scheduled while other pods belonging to the same job are pending for resources to become available; scenarios that can cause deadlock situations and major inefficiencies in cluster utilization. The Run:AI system provides: Inter-pod communication. Command-line interface to access logs and an interactive shell. For more information on Distributed Training in Run:AI see here","title":"Distributed Training"},{"location":"Researcher/Scheduling/The-Run-AI-Scheduler/#hyperparameter-optimization","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With HPO, the researcher provides a single script which is used with multiple, varying, parameters. Each run is a pod (see definition above). Unlike Gang Scheduling, with HPO, pods are independent . They are scheduled independently, started and end independently, and if preempted, the other pods are unaffected. The scheduling behavior for individual pods are exactly as described in the Scheduler Details section above for Jobs. For more information on Hyperparameter Optimization in Run:AI see here","title":"Hyperparameter Optimization"},{"location":"Researcher/Scheduling/auto-delete-jobs/","text":"Introduction \u00b6 Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old jobs by running the command: runai list You can delete the job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a job for automatic deletion some period of time after its finish. Important Deleting a job, deletes the container behind it, and with it all related information such as job logs. Data that was saved by the researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface Enable Automatic Deletion in Cluster (Admin only) \u00b6 In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray https://github.com/kubernetes-sigs/kubespray ). Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to/etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube - apiserver ..... - -- feature - gates = TTLAfterFinished = true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube - controller - manager ..... - -- feature - gates = TTLAfterFinished = true Automatic Deletion \u00b6 When starting the job, add the flag --ttl-after-finish duration . duration is the duration, post job finish, after which the job is automatically deleted. Example durations are: 5s, 2m, 3h, 4d etc. For example, the following call will delete the job 2 hours after job finish: runai submit myjob1 --ttl-after-finish 2h","title":"Automatically Delete Finished Workloads"},{"location":"Researcher/Scheduling/auto-delete-jobs/#introduction","text":"Jobs can be started via Kubeflow, Run:AI CLI, Rancher or via direct Kubernetes API. When jobs are finished (successfully or failing), their resource allocation is taken away, but they remain in the system. You can see old jobs by running the command: runai list You can delete the job manually by running: runai delete run3 But this may not be scalable for a production system. It is possible to flag a job for automatic deletion some period of time after its finish. Important Deleting a job, deletes the container behind it, and with it all related information such as job logs. Data that was saved by the researcher on a shared drive is not affected. The Job is also not deleted from the Run:AI user interface","title":"Introduction"},{"location":"Researcher/Scheduling/auto-delete-jobs/#enable-automatic-deletion-in-cluster-admin-only","text":"In order for automatic deletion to work, the On-premise Kubernetes cluster needs to be modified. The feature relies on a Kubernetes feature gate \" TTLAfterFinished \" Note : different Kubernetes distributions have different locations and methods to add feature flags. The instructions below are an example based on Kubespray https://github.com/kubernetes-sigs/kubespray ). Refer to the documentation of your Kubernetes distribution. Open a shell on the Kubernetes master cd to/etc/kubernetes/manifests vi kube-apiserver.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube - apiserver ..... - -- feature - gates = TTLAfterFinished = true vi kube-controller-manager.yaml add --feature-gates=TTLAfterFinished=true to the following location: spec : containers : - command : - kube - controller - manager ..... - -- feature - gates = TTLAfterFinished = true","title":"Enable Automatic Deletion in Cluster (Admin only)"},{"location":"Researcher/Scheduling/auto-delete-jobs/#automatic-deletion","text":"When starting the job, add the flag --ttl-after-finish duration . duration is the duration, post job finish, after which the job is automatically deleted. Example durations are: 5s, 2m, 3h, 4d etc. For example, the following call will delete the job 2 hours after job finish: runai submit myjob1 --ttl-after-finish 2h","title":"Automatic Deletion"},{"location":"Researcher/Walkthroughs/Run-AI-Walkthroughs/","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm, or similar and accesses GPU resources directly . Build workloads typically do not maximize usage of the GPU. Unattended \"training\" sessions. Training is characterized by a deep learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results . A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU. Follow a Walk-through for each using the links below Unattended training sessions Interactive build sessions Interactive build sessions with externalized services Using GPU Fractions Distributed Training Hyperparameter Optimization Over-Quota, Basic Fairness & Bin Packing Fairness Elasticity","title":"Run:AI Walkthroughs"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/","text":"Walk-through: Launch Interactive Build Workloads with Connected Ports \u00b6 Introduction \u00b6 This walk-through is an extension of Walk-through Start and Use Interactive Build Workloads When starting a container with the Run:AI Command-Line Interface (CLI), it is possible to expose internal ports to the container user. Exposing a Container Port \u00b6 There are a number of alternative ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> Contact your administrator to see which methods are available in your cluster Port Forwarding, Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command-line run: runai project set team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 \\ --interactive --service-type=portforward --port 8888:8888 \\ --args=\"--NotebookApp.base_url=jupyter1\" --command=start-notebook.sh The job is based on a generic Jupyter notebook docker image jupyter/base-notebook We named the job jupyter1 . Note that in this Jupyter implementation, the name of the job should also be copied to the Notebook base URL. Note the interactive flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the runai submit command is not stopped Open the Jupyter notebook \u00b6 Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook. Ingress, Step by Step Walk-through \u00b6 Note: Ingress must be set up by your administrator prior to usage. For more information see: Exposing Ports from Researcher Containers Using Ingress Setup \u00b6 Perform the setup steps for port forwarding above. Run Workload \u00b6 At the command-line run: runai project set team-a runai submit test-ingress -i jupyter/base-notebook -g 1 \\ --interactive --service-type=ingress --port 8888 \\ --args=\"--NotebookApp.base_url=team-a-test-ingress\" --command=start-notebook.sh An ingress service URL will be created, run: runai list You will see the service URL with which to access the Jupyter notebook Important With ingress, Run:AI creates an access URL whose domain is uniform (and is IP which serves as the access point to the cluster). The rest of the path is unique and is build as: <project-name>-<job-name> . Thus, with the example above, we must set the Jupyter notebook base URL to respond to the service at team-a-test-ingress See Also \u00b6 Develop on Run:AI using Visual Studio Code","title":"Build Workloads with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#walk-through-launch-interactive-build-workloads-with-connected-ports","text":"","title":"Walk-through: Launch Interactive Build Workloads with Connected Ports"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#introduction","text":"This walk-through is an extension of Walk-through Start and Use Interactive Build Workloads When starting a container with the Run:AI Command-Line Interface (CLI), it is possible to expose internal ports to the container user.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#exposing-a-container-port","text":"There are a number of alternative ways to expose ports in Kubernetes: NodePort - Exposes the Service on each Node\u2019s IP at a static port (the NodePort). You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort> regardless of which node the container actually resides. LoadBalancer - Useful for cloud environments. Exposes the Service externally using a cloud provider\u2019s load balancer. Ingress - Allows access to Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services. Port Forwarding - Simple port forwarding allows access to the container via localhost:<Port> Contact your administrator to see which methods are available in your cluster","title":"Exposing a Container Port"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#port-forwarding-step-by-step-walk-through","text":"","title":"Port Forwarding, Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload","text":"At the command-line run: runai project set team-a runai submit jupyter1 -i jupyter/base-notebook -g 1 \\ --interactive --service-type=portforward --port 8888:8888 \\ --args=\"--NotebookApp.base_url=jupyter1\" --command=start-notebook.sh The job is based on a generic Jupyter notebook docker image jupyter/base-notebook We named the job jupyter1 . Note that in this Jupyter implementation, the name of the job should also be copied to the Notebook base URL. Note the interactive flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. In this example, we have chosen the simplest scheme to expose ports which is port forwarding. We temporarily expose port 8888 to localhost as long as the runai submit command is not stopped","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#open-the-jupyter-notebook","text":"Open the following in the browser http://localhost:8888/jupyter1 You should see a Jupyter notebook.","title":"Open the Jupyter notebook"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#ingress-step-by-step-walk-through","text":"Note: Ingress must be set up by your administrator prior to usage. For more information see: Exposing Ports from Researcher Containers Using Ingress","title":"Ingress, Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#setup_1","text":"Perform the setup steps for port forwarding above.","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#run-workload_1","text":"At the command-line run: runai project set team-a runai submit test-ingress -i jupyter/base-notebook -g 1 \\ --interactive --service-type=ingress --port 8888 \\ --args=\"--NotebookApp.base_url=team-a-test-ingress\" --command=start-notebook.sh An ingress service URL will be created, run: runai list You will see the service URL with which to access the Jupyter notebook Important With ingress, Run:AI creates an access URL whose domain is uniform (and is IP which serves as the access point to the cluster). The rest of the path is unique and is build as: <project-name>-<job-name> . Thus, with the example above, we must set the Jupyter notebook base URL to respond to the service at team-a-test-ingress","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build-ports/#see-also","text":"Develop on Run:AI using Visual Studio Code","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-build/","text":"Walk-through: Launch Interactive Build Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walk-through you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article. Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command-line run: runai project set team-a runai submit build1 -i python -g 1 --interactive --command sleep --args infinity The job is based on a sample docker image python We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is --command sleep --args infinity . You must provide a command or the container will start and then exit immediately. Follow up on the job's status by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai get build1 Get a Shell to the container \u00b6 Run: runai bash build1 This should provide a direct shell into the computer View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai Under \"Jobs\" you can view the new Workload: Stop Workload \u00b6 Run the following: runai delete build1 This would stop the training workload. You can verify this by running runai list again. Next Steps \u00b6 Expose internal ports to your interactive build workload: Walk-through Launch an Interactive Build Workload with Connected Ports . Follow the Walk-through: Walk-through Launch Unattended Training Workloads .","title":"Build Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-build/#walk-through-launch-interactive-build-workloads","text":"","title":"Walk-through: Launch Interactive Build Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-build/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walk-through you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning Build workload Open an ssh session to the Build workload Stop the Build workload It is also possible to open ports to specific services within the container. See \"Next Steps\" at the end of this article.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-build/#prerequisites","text":"To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-build/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-build/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-build/#run-workload","text":"At the command-line run: runai project set team-a runai submit build1 -i python -g 1 --interactive --command sleep --args infinity The job is based on a sample docker image python We named the job build1 . Note the interactive flag which means the job will not have a start or end. It is the researcher's responsibility to close the job. The job is assigned to team-a with an allocation of a single GPU. The command provided is --command sleep --args infinity . You must provide a command or the container will start and then exit immediately. Follow up on the job's status by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running A full list of Job statuses can be found here To get additional status on your job run: runai get build1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#get-a-shell-to-the-container","text":"Run: runai bash build1 This should provide a direct shell into the computer","title":"Get a Shell to the container"},{"location":"Researcher/Walkthroughs/walkthrough-build/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai Under \"Jobs\" you can view the new Workload:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-build/#stop-workload","text":"Run the following: runai delete build1 This would stop the training workload. You can verify this by running runai list again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-build/#next-steps","text":"Expose internal ports to your interactive build workload: Walk-through Launch an Interactive Build Workload with Connected Ports . Follow the Walk-through: Walk-through Launch Unattended Training Workloads .","title":"Next Steps"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/","text":"Walk-through: Launch Distributed Training Workloads \u00b6 Introduction \u00b6 Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod ( https://eng.uber.com/horovod/ ) is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a walk-through of such a scenario. Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Training Distributed Workload \u00b6 At the command-line run: runai project set team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed We named the job dist The job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed . the image contains a startup script that runs a deep learning Horovod-based workload. The script runs the following Horovod command: horovodrun -np %RUNAI_MPI_NUM_WORKERS% \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod Where RUNAI_MPI_NUM_WORKERS is a Run:AI environment variable containing the number of worker processes provided to the runai submit-mpi command (in this example it's 2). Follow up on the job's status by running: runai list The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai get dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist Run an Interactive Distributed Workload \u00b6 It is also possible to run a distributed training job as \"interactive\". This is useful if you want to test your distributed training job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed \\ --command=\"sh\" --args=\"-c\" --args=\"sleep infinity\" --interactive When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed session. For examples, with Horovod: horovodrun - np 2 python scripts / tf_cnn_benchmarks / tf_cnn_benchmarks . py \\ -- model = resnet20 -- num_batches = 1000000 -- data_name cifar10 \\ -- data_dir / cifar10 -- batch_size = 64 -- variable_update = horovod See Also \u00b6 The source code of the image used in the walk-through in Github For a full list of the submit-mpi options see runai submit-mpi","title":"Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#walk-through-launch-distributed-training-workloads","text":"","title":"Walk-through: Launch Distributed Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#introduction","text":"Distributed Training is the ability to split the training of a model among multiple processors. Each processor is called a worker node . Worker nodes work in parallel to speed up model training. Distributed Training should not be confused with multi-GPU training. Multi-GPU training is the allocation of more than a single GPU to your workload which runs on a single container . Getting Distributed Training to work is more complex than multi-GPU training as it requires syncing of data and timing between the different workers. However, it is often a necessity when multi-GPU training no longer applies; typically when you require more GPUs than exist on a single node. There are a number of Deep Learning frameworks that support Distributed Training. Horovod ( https://eng.uber.com/horovod/ ) is a good example. Run:AI provides the ability to run, manage, and view Distributed Training workloads. The following is a walk-through of such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#prerequisites","text":"To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#setup","text":"Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-training-distributed-workload","text":"At the command-line run: runai project set team-a runai submit-mpi dist --processes = 2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed We named the job dist The job is assigned to team-a There will be two worker processes (--processes=2), each allocated with a single GPU (-g 1) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-distributed . the image contains a startup script that runs a deep learning Horovod-based workload. The script runs the following Horovod command: horovodrun -np %RUNAI_MPI_NUM_WORKERS% \\ python scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \\ --model = resnet20 --num_batches = 1000000 --data_name cifar10 \\ --data_dir /cifar10 --batch_size = 64 --variable_update = horovod Where RUNAI_MPI_NUM_WORKERS is a Run:AI environment variable containing the number of worker processes provided to the runai submit-mpi command (in this example it's 2). Follow up on the job's status by running: runai list The result: The Run:AI scheduler ensures that all processes can run together. You can see the list of workers as well as the main \"launcher\" process by running: runai get dist You will see two worker processes (pods) their status and on which node they run: To see the merged logs of all pods run: runai logs dist Finally, you can delete the distributed training workload by running: runai delete dist","title":"Run Training Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#run-an-interactive-distributed-workload","text":"It is also possible to run a distributed training job as \"interactive\". This is useful if you want to test your distributed training job before committing on a long, unattended training session. To run such a session use: runai submit-mpi dist-int --processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed \\ --command=\"sh\" --args=\"-c\" --args=\"sleep infinity\" --interactive When the workers are running run: runai bash dist-int This will provide shell access to the launcher process. From there, you can run your distributed session. For examples, with Horovod: horovodrun - np 2 python scripts / tf_cnn_benchmarks / tf_cnn_benchmarks . py \\ -- model = resnet20 -- num_batches = 1000000 -- data_name cifar10 \\ -- data_dir / cifar10 -- batch_size = 64 -- variable_update = horovod","title":"Run an Interactive Distributed Workload"},{"location":"Researcher/Walkthroughs/walkthrough-distributed-training/#see-also","text":"The source code of the image used in the walk-through in Github For a full list of the submit-mpi options see runai submit-mpi","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/","text":"Walk-through: Elasticity, Dynamically Stretch or Compress Workload's GPU Allocation \u00b6 Introduction \u00b6 Elasticity allows unattended, train-based workloads to shrink or expand based on the cluster's availability. Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. Expanding a training job allows your workload to run on more GPUs than the researcher code was originally written for. Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Run:AI Python Researcher Library installed on a docker image Step by Step Walk-through \u00b6 Setup \u00b6 A GPU cluster with a single node of 2 GPUs. If the cluster contains more than one node, use Node affinity to simulate a single node or use more filler jobs as described below. If the cluster nodes contain more than 2 GPUs, you can create an interactive job on a different project to consume the remaining GPUs. Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Expansion \u00b6 At the command-line run: runai project set team-a runai submit elastic1 -i gcr.io/run-ai-demo/quickstart -g 1 --elastic This would start an unattended training job for team-a The job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the job elastic1 and have requested 1 GPU for the job The flag --elastic enables the Elasticity feature Follow up on the job's progress by running: runai list The result: Discussion The Job has requested 1 GPU, but has been allocated with 2, as 2 are available right now. The code needs to be ready to accept more GPUs than it requested, otherwise, the GPUs will not be utilized. The Run:AI Elasticity library helps with expanding the job effectively. Add a filler class: runai submit filler1 -i ubuntu --command sleep --args infinity -g 1 --interactive runai list The result: Discussion An interactive job (filler1) needs to be scheduled. The elastic job is now reduced to the originally requested single-GPU. Finally, delete the jobs: runai delete elastic1 filler1 Shrinking \u00b6 At the command-line run: runai submit filler2 -i ubuntu --command sleep --args infinity -g 1 --interactive runai submit elastic2 -i gcr.io/run-ai-demo/quickstart -g 2 --elastic This would start a filler job on 1 GPU and attempt to start another unattended job with 2 GPUs Follow up on the job's progress by running: runai list The result: Discussion Since only a single GPU remains unallocated, under normal circumstances, the job should not start. However, the --elastic flag tells the system to allocate a single GPU instead. Delete the filler job and list the jobs again: runai delete filler2 runai list The result: Discussion With the filler job gone, the elastic job has more room to expand, which it does. Finally, delete the job: runai delete elastic2 See Also \u00b6 For more information on the elasticity module of the Researcher python library, see Researcher library : Elasticity","title":"Elasticity"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#walk-through-elasticity-dynamically-stretch-or-compress-workloads-gpu-allocation","text":"","title":"Walk-through: Elasticity, Dynamically Stretch or Compress Workload's GPU Allocation"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#introduction","text":"Elasticity allows unattended, train-based workloads to shrink or expand based on the cluster's availability. Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. Expanding a training job allows your workload to run on more GPUs than the researcher code was originally written for.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#prerequisites","text":"To complete this walk-through you must have: Run:AI software installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Run:AI Python Researcher Library installed on a docker image","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#setup","text":"A GPU cluster with a single node of 2 GPUs. If the cluster contains more than one node, use Node affinity to simulate a single node or use more filler jobs as described below. If the cluster nodes contain more than 2 GPUs, you can create an interactive job on a different project to consume the remaining GPUs. Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#expansion","text":"At the command-line run: runai project set team-a runai submit elastic1 -i gcr.io/run-ai-demo/quickstart -g 1 --elastic This would start an unattended training job for team-a The job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the job elastic1 and have requested 1 GPU for the job The flag --elastic enables the Elasticity feature Follow up on the job's progress by running: runai list The result: Discussion The Job has requested 1 GPU, but has been allocated with 2, as 2 are available right now. The code needs to be ready to accept more GPUs than it requested, otherwise, the GPUs will not be utilized. The Run:AI Elasticity library helps with expanding the job effectively. Add a filler class: runai submit filler1 -i ubuntu --command sleep --args infinity -g 1 --interactive runai list The result: Discussion An interactive job (filler1) needs to be scheduled. The elastic job is now reduced to the originally requested single-GPU. Finally, delete the jobs: runai delete elastic1 filler1","title":"Expansion"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#shrinking","text":"At the command-line run: runai submit filler2 -i ubuntu --command sleep --args infinity -g 1 --interactive runai submit elastic2 -i gcr.io/run-ai-demo/quickstart -g 2 --elastic This would start a filler job on 1 GPU and attempt to start another unattended job with 2 GPUs Follow up on the job's progress by running: runai list The result: Discussion Since only a single GPU remains unallocated, under normal circumstances, the job should not start. However, the --elastic flag tells the system to allocate a single GPU instead. Delete the filler job and list the jobs again: runai delete filler2 runai list The result: Discussion With the filler job gone, the elastic job has more room to expand, which it does. Finally, delete the job: runai delete elastic2","title":"Shrinking"},{"location":"Researcher/Walkthroughs/walkthrough-elasticity/#see-also","text":"For more information on the elasticity module of the Researcher python library, see Researcher library : Elasticity","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/","text":"Walk-through: Launch Workloads with GPU Fractions \u00b6 Introduction \u00b6 Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 jobs running on the same GPU, meaning you could do eight times the work with the same hardware. Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 1 GPU to the project Run Workload \u00b6 At the command-line run: runai project set team - a runai submit frac05 - i gcr . io / run - ai - demo / quickstart - g 0 . 5 --interactive runai submit frac03 - i gcr . io / run - ai - demo / quickstart - g 0 . 3 The jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the job will not automatically finish. Rather, it is the researcher's responsibility to delete the job. Fractions support both Interactive and non-interactive jobs. The jobs are assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Note that both jobs were allocated to the same node. When both jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#walk-through-launch-workloads-with-gpu-fractions","text":"","title":"Walk-through: Launch Workloads with GPU Fractions"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#introduction","text":"Run:AI provides a Fractional GPU sharing system for containerized workloads on Kubernetes. The system supports workloads running CUDA programs and is especially suited for lightweight AI tasks such as inference and model building. The fractional GPU system transparently gives data science and AI engineering teams the ability to run multiple workloads simultaneously on a single GPU, enabling companies to run more workloads such as computer vision, voice recognition and natural language processing on the same hardware, lowering costs. Run:AI\u2019s fractional GPU system effectively creates virtualized logical GPUs, with their own memory and computing space that containers can use and access as if they were self-contained processors. This enables several workloads to run in containers side-by-side on the same GPU without interfering with each other. The solution is transparent, simple, and portable; it requires no changes to the containers themselves. A typical use-case could see 2-8 jobs running on the same GPU, meaning you could do eight times the work with the same hardware.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#prerequisites","text":"To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#setup","text":"Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 1 GPU to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-fractions/#run-workload","text":"At the command-line run: runai project set team - a runai submit frac05 - i gcr . io / run - ai - demo / quickstart - g 0 . 5 --interactive runai submit frac03 - i gcr . io / run - ai - demo / quickstart - g 0 . 3 The jobs are based on a sample docker image gcr.io/run-ai-demo/quickstart the image contains a startup script that runs a deep learning TensorFlow-based workload. We named the jobs frac05 and frac03 respectively. Note that fractions may or may not use the --interactive flag. Setting the flag means that the job will not automatically finish. Rather, it is the researcher's responsibility to delete the job. Fractions support both Interactive and non-interactive jobs. The jobs are assigned to team-a with an allocation of a single GPU. Follow up on the job's status by running: runai list The result: Note that both jobs were allocated to the same node. When both jobs are running, bash into one of them: runai bash frac05 Now, inside the container, run: nvidia-smi The result: Notes: The total memory is circled in red. It should be 50% of the GPUs memory size. In the picture above we see 8GB which is half of the 16GB of Tesla V100 GPUs. The script running on the container is limited by 8GB. In this case, TensorFlow, which tends to allocate almost all of the GPU memory has allocated 7.7GB RAM (and not close to 16 GB). Overallocation beyond 8GB will lead to an out-of-memory exception","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/","text":"Walk-through: Hyperparameter Optimization \u00b6 Introduction \u00b6 Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are a number of strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the later does an exhaustive search from a list of pre-selected values. Run:AI provides the ability to run, manage, and view HPO runs. The following is a walk-through of such a scenario. Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project On shared storage create a library to store HPO results. E.g. /nfs/john/hpo Pods \u00b6 With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory. HPO Sample Code \u00b6 The Walk-through code can be found in github.com/run-ai/docs . The code uses the Run:AI Researcher python library . Below are some highlights of the code: # import Run:AI HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:AI HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/hpo\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ]) Run an HPO Workload \u00b6 At the command-line run: runai project set team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo We named the job hpo1 The job is assigned to team-a The job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /hpo . The running pods will use the directory to sync hyperparameters and save results. Follow up on the job's status by running: runai list The result: Follow up on the job's pods by running: runai get hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit jobs on another project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai get hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001} Examine the Results \u00b6 The Run:AI HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is an snapshot of the file for two experiments with two epochs each: creationTime : 24 /08/ 2020 08 : 50 : 06 experiments : - config : batch_size : 32 lr : 1 id : 1 modificationTime : 24 /08/ 2020 08 : 50 : 06 reports : - epoch : 0 metrics : acc : 0.09814 loss : 2.310984723968506 val_acc : 0.1 val_loss : 2.3098626373291014 reportTime : 24 /08/ 2020 08 : 52 : 11 - epoch : 1 metrics : acc : 0.09914 loss : 2.30994320602417 val_acc : 0.1 val_loss : 2.3110838134765626 reportTime : 24 /08/ 2020 08 : 54 : 10 - config : batch_size : 32 lr : 0.1 id : 2 modificationTime : 24 /08/ 2020 08 : 50 : 36 reports : - epoch : 0 metrics : acc : 0.11012 loss : 2.2979678358459474 val_acc : 0.1667 val_loss : 2.268467852783203 reportTime : 24 /08/ 2020 08 : 52 : 44 - epoch : 1 metrics : acc : 0.2047 loss : 2.0894255745697023 val_acc : 0.2833 val_loss : 1.8615504817962647 reportTime : 24 /08/ 2020 08 : 54 : 45 Finally, you can delete the HPO job by running: runai delete hpo1 See Also \u00b6 For further information on the Run:AI HPO support library see: The Run:AI HPO Support Library Sample code in Github","title":"Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#walk-through-hyperparameter-optimization","text":"","title":"Walk-through: Hyperparameter Optimization"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#introduction","text":"Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter can be a parameter whose value is used to control the learning process, to define the model architecture or the data pre-processing process, etc. Example hyperparameters: learning rate, batch size, different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while, and then examine results to decide what works best. There are a number of strategies for searching the hyperparameter space. Most notable are Random search and Grid search . The former, as its name implies, selects parameters at random while the later does an exhaustive search from a list of pre-selected values. Run:AI provides the ability to run, manage, and view HPO runs. The following is a walk-through of such a scenario.","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#prerequisites","text":"To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#setup","text":"Open the Run:AI user interface at app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project On shared storage create a library to store HPO results. E.g. /nfs/john/hpo","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#pods","text":"With HPO, we introduce the concept of Pods . Pods are units of work within a Job. Typically, each Job has a single Pod. However, with HPO as well as with Distributed Training there are multiple Pods per Job. Pods are independent All Pods execute with the same arguments as added via runai submit . E.g. The same image name, the same code script, the same number of Allocated GPUs, memory.","title":"Pods"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#hpo-sample-code","text":"The Walk-through code can be found in github.com/run-ai/docs . The code uses the Run:AI Researcher python library . Below are some highlights of the code: # import Run:AI HPO library import runai.hpo # select Random search or grid search strategy = runai . hpo . Strategy . GridSearch # initialize the Run:AI HPO library. Send the NFS directory used for sync runai . hpo . init ( \"/hpo\" ) # pick a configuration for this HPO experiment # we pass the options of all hyperparameters we want to test # `config` will hold a single value for each parameter config = runai . hpo . pick ( grid = dict ( batch_size = [ 32 , 64 , 128 ], lr = [ 1 , 0.1 , 0.01 , 0.001 ]), strategy = strategy ) .... # Use the selected configuration within your code optimizer = keras . optimizers . SGD ( lr = config [ 'lr' ])","title":"HPO Sample Code"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#run-an-hpo-workload","text":"At the command-line run: runai project set team-a runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo We named the job hpo1 The job is assigned to team-a The job will be complete when 12 pods will run ( --completions 12 ), each allocated with a single GPU ( -g 1 ) At most, there will be 3 pods running concurrently ( --parallelism 3 ) The job is based on a sample docker image gcr.io/run-ai-demo/quickstart-hpo . The image contains a startup script that selects a set of hyperparameters and then uses them, as described above. The command maps a shared volume /nfs/john/hpo to a directory in the container /hpo . The running pods will use the directory to sync hyperparameters and save results. Follow up on the job's status by running: runai list The result: Follow up on the job's pods by running: runai get hpo1 You will see 3 running pods currently executing: Once the 3 pods are done, they will be replaced by new ones from the 12 completions . This process will continue until all 12 have run. You can also submit jobs on another project until only 2 GPUs remain. This will preempt 1 pod and will henceforth limit the HPO job to run on 2 pods only. Preempted pods will be picked up and ran later. You can see logs of specific pods by running : runai logs hpo1 --pod <POD-NAME> where <<POD-NAME>> is a pod name as appears above in the runai get hpo1 output The logs will contain a couple of lines worth noting: Picked HPO experiment #4 ... Using HPO directory /hpo Using configuration: {'batch_size': 32, 'lr': 0.001}","title":"Run an HPO Workload"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#examine-the-results","text":"The Run:AI HPO library saves the experiment variations and the experiment results to a single file, making it easier to pick the best HPO run. The file can be found in the shared folder. Below is an snapshot of the file for two experiments with two epochs each: creationTime : 24 /08/ 2020 08 : 50 : 06 experiments : - config : batch_size : 32 lr : 1 id : 1 modificationTime : 24 /08/ 2020 08 : 50 : 06 reports : - epoch : 0 metrics : acc : 0.09814 loss : 2.310984723968506 val_acc : 0.1 val_loss : 2.3098626373291014 reportTime : 24 /08/ 2020 08 : 52 : 11 - epoch : 1 metrics : acc : 0.09914 loss : 2.30994320602417 val_acc : 0.1 val_loss : 2.3110838134765626 reportTime : 24 /08/ 2020 08 : 54 : 10 - config : batch_size : 32 lr : 0.1 id : 2 modificationTime : 24 /08/ 2020 08 : 50 : 36 reports : - epoch : 0 metrics : acc : 0.11012 loss : 2.2979678358459474 val_acc : 0.1667 val_loss : 2.268467852783203 reportTime : 24 /08/ 2020 08 : 52 : 44 - epoch : 1 metrics : acc : 0.2047 loss : 2.0894255745697023 val_acc : 0.2833 val_loss : 1.8615504817962647 reportTime : 24 /08/ 2020 08 : 54 : 45 Finally, you can delete the HPO job by running: runai delete hpo1","title":"Examine the Results"},{"location":"Researcher/Walkthroughs/walkthrough-hpo/#see-also","text":"For further information on the Run:AI HPO support library see: The Run:AI HPO Support Library Sample code in Github","title":"See Also"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/","text":"Walk-through: Over-Quota and Bin Packing \u00b6 Goals \u00b6 The goal of this walk-through is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:AI canonical image gcr.io/run-ai-demo/quickstart Part I: Over-quota \u00b6 Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized. Part 2: Basic Fairness via Preemption \u00b6 Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one job, must be preempted : moved out to allow team-b to grow. Run:AI scheduler chooses to preempt job a1 . It is important that unattended jobs will save checkpoints . This will ensure that whenever job a1 resume, it will do so from where it left off. Part 3: Bin Packing \u00b6 Run the following command: runai delete a2 -p team-a a1 is now going to start running again. Run: runai list -A You have two jobs that are running on the first node and one job that is running alone the second node. Choose one of the two job from the full node and delete it: runai delete <job-name> -p <project> The status now is: Now, run a 2 GPU job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that job a1 has been preempted and then restarted on the second node, in order to clear space fo the new a2 job. This is bin-packing or consolidation","title":"Over-Quota, Basic Fairness & Bin-Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#walk-through-over-quota-and-bin-packing","text":"","title":"Walk-through: Over-Quota and Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#goals","text":"The goal of this walk-through is to explain the concepts of over-quota and bin-packing (consolidation) and how they help in maximizing cluster utilization: Show the simplicity of resource provisioning, and how resources are abstracted from users. Show how the system eliminates compute bottlenecks by allowing teams/users to go over their resource quota if there are free GPUs in the cluster.","title":"Goals"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each 2 Projects: team-a and team-b with 2 allocated GPUs each Run:AI canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-i-over-quota","text":"Run the following commands: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a has 3 GPUs allocated. Which is over its quota by 1 GPU. The system allows this over-quota as long as there are available resources The system is at full capacity with all GPUs utilized.","title":"Part I: Over-quota"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-2-basic-fairness-via-preemption","text":"Run the following command: runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion team-a can no longer remain in over-quota. Thus, one job, must be preempted : moved out to allow team-b to grow. Run:AI scheduler chooses to preempt job a1 . It is important that unattended jobs will save checkpoints . This will ensure that whenever job a1 resume, it will do so from where it left off.","title":"Part 2: Basic Fairness via Preemption"},{"location":"Researcher/Walkthroughs/walkthrough-overquota/#part-3-bin-packing","text":"Run the following command: runai delete a2 -p team-a a1 is now going to start running again. Run: runai list -A You have two jobs that are running on the first node and one job that is running alone the second node. Choose one of the two job from the full node and delete it: runai delete <job-name> -p <project> The status now is: Now, run a 2 GPU job: runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 2 -p team-a The status now is: Discussion Note that job a1 has been preempted and then restarted on the second node, in order to clear space fo the new a2 job. This is bin-packing or consolidation","title":"Part 3: Bin Packing"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/","text":"Walk-through: Queue Fairness \u00b6 Goal \u00b6 The goal of this walk-through is to explain fairness . The over-quota walk-through shows basic fairness where allocated GPUs per project are adhered to such that if a project is in over-quota, its job will be preempted once another project requires its resources. This walk-through is about queue fairness . It shows that jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in project A has submitted 50 jobs and soon after that, a person in project B has submitted 25 jobs, the jobs in the queue will be processed fairly. Setup and configuration: \u00b6 4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:AI canonical image gcr.io/run-ai-demo/quickstart Part I: Immediate Displacement of Over-Quota \u00b6 Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the projects. Part 2: Queue Fairness \u00b6 Now lets start deleting jobs. Alternatively, you can wait for jobs to complete. runai delete b2 -p team-b Discussion As the quotas are equal (1 for each project, the remaining pending jobs will get scheduled one by one alternating between projects, regardless of the time in which they were submitted.","title":"Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#walk-through-queue-fairness","text":"","title":"Walk-through: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#goal","text":"The goal of this walk-through is to explain fairness . The over-quota walk-through shows basic fairness where allocated GPUs per project are adhered to such that if a project is in over-quota, its job will be preempted once another project requires its resources. This walk-through is about queue fairness . It shows that jobs will be scheduled fairly regardless of the time they have been submitted. As such, if a person in project A has submitted 50 jobs and soon after that, a person in project B has submitted 25 jobs, the jobs in the queue will be processed fairly.","title":"Goal"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#setup-and-configuration","text":"4 GPUs on 2 machines with 2 GPUs each. 2 Projects: team-a and team-b with 1 allocated GPU each. Run:AI canonical image gcr.io/run-ai-demo/quickstart","title":"Setup and configuration:"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-i-immediate-displacement-of-over-quota","text":"Run the following commands: runai submit a1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a runai submit a4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-a System status after run: Discussion team-a, even though it has a single GPU as quota, is now using all 4 GPUs. Run the following commands: runai submit b1 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b2 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b3 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b runai submit b4 -i gcr.io/run-ai-demo/quickstart -g 1 -p team-b System status after run: Discussion Two team-b jobs have immediately displaced team-a. team-a and team-b each have a quota of 1 GPU, thus the remaining over-quota (2 GPUs) is distributed equally between the projects.","title":"Part I: Immediate Displacement of Over-Quota"},{"location":"Researcher/Walkthroughs/walkthrough-queue-fairness/#part-2-queue-fairness","text":"Now lets start deleting jobs. Alternatively, you can wait for jobs to complete. runai delete b2 -p team-b Discussion As the quotas are equal (1 for each project, the remaining pending jobs will get scheduled one by one alternating between projects, regardless of the time in which they were submitted.","title":"Part 2: Queue Fairness"},{"location":"Researcher/Walkthroughs/walkthrough-train/","text":"Walk-through: Launch Unattended Training Workloads \u00b6 Introduction \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walk-through you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload View training status and resource consumption using the Run:AI user interface and the Run:AI CLI View training logs Stop the training Prerequisites \u00b6 To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface Step by Step Walk-through \u00b6 Setup \u00b6 Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project Run Workload \u00b6 At the command-line run: runai project set team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training job for team-a with an allocation of a single GPU. The job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the job train1 Follow up on the job's progress by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running Succeeded - the job has ended A full list of Job statuses can be found here To get additional status on your job run: runai get train1 View Logs \u00b6 Run the following: runai logs train1 You should see a log of a running deep learning session: View status on the Run:AI User Interface \u00b6 Go to https://app.run.ai Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization: Stop Workload \u00b6 Run the following: runai delete train1 This would stop the training workload. You can verify this by running runai list again. Next Steps \u00b6 Follow the Walk-through: Launch Interactive Workloads Use your own containers to run an unattended training workload","title":"Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-train/#walk-through-launch-unattended-training-workloads","text":"","title":"Walk-through: Launch Unattended Training Workloads"},{"location":"Researcher/Walkthroughs/walkthrough-train/#introduction","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter notebook, remote PyCharm or similar and accesses GPU resources directly. Unattended \"training\" sessions. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the customer can examine the results. With this Walk-through you will learn how to: Use the Run:AI command-line interface (CLI) to start a deep learning training workload View training status and resource consumption using the Run:AI user interface and the Run:AI CLI View training logs Stop the training","title":"Introduction"},{"location":"Researcher/Walkthroughs/walkthrough-train/#prerequisites","text":"To complete this walk-through you must have: Run:AI software is installed on your Kubernetes cluster. See: Installing Run:AI on an on-premise Kubernetes Cluster Run:AI CLI installed on your machine. See: Installing the Run:AI Command-Line Interface","title":"Prerequisites"},{"location":"Researcher/Walkthroughs/walkthrough-train/#step-by-step-walk-through","text":"","title":"Step by Step Walk-through"},{"location":"Researcher/Walkthroughs/walkthrough-train/#setup","text":"Open the Run:AI user interface at https://app.run.ai Login Go to \"Projects\" Add a project named \"team-a\" Allocate 2 GPUs to the project","title":"Setup"},{"location":"Researcher/Walkthroughs/walkthrough-train/#run-workload","text":"At the command-line run: runai project set team-a runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 This would start an unattended training job for team-a with an allocation of a single GPU. The job is based on a sample docker image gcr.io/run-ai-demo/quickstart . We named the job train1 Follow up on the job's progress by running: runai list The result: Typical statuses you may see: ContainerCreating - The docker container is being downloaded from the cloud repository Pending - the job is waiting to be scheduled Running - the job is running Succeeded - the job has ended A full list of Job statuses can be found here To get additional status on your job run: runai get train1","title":"Run Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-logs","text":"Run the following: runai logs train1 You should see a log of a running deep learning session:","title":"View Logs"},{"location":"Researcher/Walkthroughs/walkthrough-train/#view-status-on-the-runai-user-interface","text":"Go to https://app.run.ai Under \"Jobs\" you can view the new Workload: The image we used for training includes the Run:AI Training library. Among other features, this library allows the reporting of metrics from within the deep learning job. Metrics such as progress, accuracy, loss, and epoch and step numbers. Progress can be seen in the status column above. To see other metrics, press the settings wheel on the top right and select additional deep learning metrics from the list Under Nodes you can see node utilization:","title":"View status on the Run:AI User Interface"},{"location":"Researcher/Walkthroughs/walkthrough-train/#stop-workload","text":"Run the following: runai delete train1 This would stop the training workload. You can verify this by running runai list again.","title":"Stop Workload"},{"location":"Researcher/Walkthroughs/walkthrough-train/#next-steps","text":"Follow the Walk-through: Launch Interactive Workloads Use your own containers to run an unattended training workload","title":"Next Steps"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/","text":"Best Practice: From Bare Metal to Docker Images \u00b6 Introduction \u00b6 Some researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task. Why Use Docker Images? \u00b6 Docker images and 'containerization' in general provide a level of abstraction which, by large, frees developers and researchers from the mundane tasks of 'setting up an environment'. The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image. Moving a Data Science Environment to Docker \u00b6 A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run Training data \u00b6 Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the researcher is currently using, allowing the researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine. Machine Learning Code and Inputs \u00b6 As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system. Code Dependencies \u00b6 Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically. ML Lifecycle: Build and Train \u00b6 Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running. Build Workloads \u00b6 With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service). Training Workloads \u00b6 For training workloads you can use a well-known image (e.g. the nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command-line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training job. For further information on how to set up and parameterize a training workload via docker or Run:AI see Converting your Workload to use Unattended Training Execution .","title":"Bare-Metal to Docker Images"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#best-practice-from-bare-metal-to-docker-images","text":"","title":"Best Practice: From Bare Metal to Docker Images"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#introduction","text":"Some researchers do data-science on bare metal . The term bare-metal relates to connecting to a server and working directly on its operating system and disks. This is the fastest way to start working, but it introduces problems when the data science organization scales: More researchers mean that the machine resources need to be efficiently shared Researchers need to collaborate and share data, code, and results To overcome that, people working on bare-metal typically write scripts to gather data, code and code dependencies. This soon becomes an overwhelming task.","title":"Introduction"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#why-use-docker-images","text":"Docker images and 'containerization' in general provide a level of abstraction which, by large, frees developers and researchers from the mundane tasks of 'setting up an environment'. The image is an operating system by itself and thus the 'environment' is by large, a part of the image. When a docker image is instantiated, it creates a container . A container is the running manifestation of a docker image.","title":"Why Use Docker Images?"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#moving-a-data-science-environment-to-docker","text":"A data science environment typically includes: Training data Machine Learning (ML) code and inputs Libraries: Code dependencies that must be installed before the ML code can be run","title":"Moving a Data Science Environment to Docker"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#training-data","text":"Training data is usually significantly large (from several Gigabytes to Petabytes) and is read-only in nature. Thus, training data is typically left outside of the docker image. Instead, the data is mounted onto the image when it is instantiated. Mounting a volume allows the code within the container to access the data as though it was within a directory on the local file system. The best practice is to store the training data on a shared file system. This allows the data to be accessed uniformly on whichever machine the researcher is currently using, allowing the researcher to easily migrate between machines. Organizations without a shared file system typically write scripts to copy data from machine to machine.","title":"Training data"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#machine-learning-code-and-inputs","text":"As a rule, code needs to be saved and versioned in a code repository . There are two alternative practices: The code resides in the image and is being periodically pulled from the repository. This practice requires building a new container image each time a change is introduced to the code. When a shared file system exists, the code can reside outside the image on a shared disk and mounted via a volume onto the container. Both practices are valid. Inputs to machine learning models and artifacts of training sessions, like model checkpoints, are also better stored in and loaded from a shared file system.","title":"Machine Learning Code and Inputs"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#code-dependencies","text":"Any code has code dependencies. These libraries must be installed for the code to run. As the code is changing, so do the dependencies. ML Code is typically python and python dependencies are typically declared together in a single requirements.txt file which is saved together with the code. The best practice is to have your docker startup script (see below) run this file using pip install -r requirements.txt . This allows the flexibility of adding and removing code dependencies dynamically.","title":"Code Dependencies"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#ml-lifecycle-build-and-train","text":"Deep learning workloads can be divided into two generic types: Interactive \"build\" sessions. With these types of workloads, the data scientist opens an interactive session, via bash, Jupyter Notebook, remote PyCharm or similar and accesses GPU resources directly. Build workloads are typically meant for debug and development sessions. Unattended \"training\" sessions. Training is characterized by a machine learning run that has a start and a finish. With these types of workloads, the data scientist prepares a self-running workload and sends it for execution. During the execution, the data scientist can examine the results. A Training session can take from a few minutes to a couple of days. It can be interrupted in the middle and later restored (though the data scientist should save checkpoints for that purpose). Training workloads typically utilize large percentages of the GPU and at the end of the run automatically frees the resources. Getting your docker ready is also a matter of which type of workload you are currently running.","title":"ML Lifecycle: Build and Train"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#build-workloads","text":"With \"build\" you are actually coding and debugging small experiments. You are interactive . In that mode, you can typically take a well known standard image (e.g. https://ngc.nvidia.com/ catalog/containers/nvidia: tensorflow ) and use it directly. Start a docker container by running: docker run -it .... \"the well known image\" -v /where/my/code/resides bash You get a shell prompt to a container with a mounted volume of where your code is. You can then install your prerequisites and run your code via ssh. You can also access the container remotely from tools such as PyCharm, Jupyter Notebook and more. In this case, the docker image needs to be customized to install the \"server software\" (e.g. a Jupyter Notebook service).","title":"Build Workloads"},{"location":"Researcher/best-practices/From-Bare-Metal-to-using-Docker-Images/#training-workloads","text":"For training workloads you can use a well-known image (e.g. the nvidia-tensorflow image from the link above) but more often then not, you want to create your own docker image. The best practice is to use the well-known image (e.g. nvidia-tensorflow from above) as a base image and add your own customizations on top of it. To achieve that, you create a Dockerfile . A Dockerfile is a declarative way to build a docker image and is built in layers. e.g.: Base image is nvidia-tensorflow Install popular software (Optional) Run a script The script can be part of the image or can be provided as part of the command-line to run the docker. It will typically include additional dependencies to install as well as a reference to the ML code to be run. Best practice for running training workloads is to test the container image in a \"build\" session and then send it for execution as a training job. For further information on how to set up and parameterize a training workload via docker or Run:AI see Converting your Workload to use Unattended Training Execution .","title":"Training Workloads"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/","text":"Best Practice: Save Deep-Learning Checkpoints \u00b6 Introduction \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs). How to Save Checkpoints \u00b6 TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch). Where to Save Checkpoints \u00b6 It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node. When to Save Checkpoints \u00b6 Save Periodically \u00b6 It is a best practice to save checkpoints at intervals. For example, every epoch. Save on Exit Signal \u00b6 If periodic checkpoints are not enough, you can use a_ signal-hook_ provided by Run:AI (via Kubernetes). The hook is python code that is called before your job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the job to return later. exit ( 1 ) if __name__ == \"__main__\" : signal . signal ( signal . SIGTERM , graceful_exit_handler ) # rest of code By default, you will have 30 seconds to save your checkpoints. Resuming using Saved Checkpoints \u00b6 A Run:AI unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist If saved checkpoints exist, load them and start the run using these checkpoints","title":"Save Deep Learning Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#best-practice-save-deep-learning-checkpoints","text":"","title":"Best Practice: Save Deep-Learning Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#introduction","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save the state of your run at various checkpoints and start a workload from the latest checkpoint (typically between epochs).","title":"Introduction"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#how-to-save-checkpoints","text":"TensorFlow, PyTorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for PyTorch).","title":"How to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#where-to-save-checkpoints","text":"It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node.","title":"Where to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#when-to-save-checkpoints","text":"","title":"When to Save Checkpoints"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#save-periodically","text":"It is a best practice to save checkpoints at intervals. For example, every epoch.","title":"Save Periodically"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#save-on-exit-signal","text":"If periodic checkpoints are not enough, you can use a_ signal-hook_ provided by Run:AI (via Kubernetes). The hook is python code that is called before your job is suspended and allows you to save your checkpoints as well as other state data you may wish to store. import signal import time def graceful_exit_handler ( signum , frame ): # save your checkpoints to shared storage # exit with status \"1\" is important for the job to return later. exit ( 1 ) if __name__ == \"__main__\" : signal . signal ( signal . SIGTERM , graceful_exit_handler ) # rest of code By default, you will have 30 seconds to save your checkpoints.","title":"Save on Exit Signal"},{"location":"Researcher/best-practices/Saving-Deep-Learning-Checkpoints/#resuming-using-saved-checkpoints","text":"A Run:AI unattended workload that is resumed, will run the same startup script as on the first run. It is the responsibility of the script developer to add code that: Checks if saved checkpoints exist If saved checkpoints exist, load them and start the run using these checkpoints","title":"Resuming using Saved Checkpoints"},{"location":"Researcher/best-practices/convert-to-unattended/","text":"Best Practice: Convert your Workload to Run Unattended \u00b6 Motivation \u00b6 Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyperparameter optmization runs. Best Practices \u00b6 Docker Image \u00b6 A docker container is based on a docker image. Some researchers use generic images such as ones provided by Nvidia (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow ). Others, use generic images as the base image to a more customized image using Dockerfiles https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ . Realizing that researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice: Create a Startup Script \u00b6 All the commands you run inside the interactive job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the researcher to add/remove libraries without needing changes to the image itself. Support Variance Between Different Runs \u00b6 Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case. Checkpoints \u00b6 Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: Saving Deep Learning Checkpoints . Running the Job \u00b6 Using runai submit , drop the flag --interactive . For submitting a job using the script created above, please use --command , and pass arguments and/or environment variables using the runai submit flags --args and --environment . Example with Environment variables: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ -e 'EPOCHS=30' \\ -e 'LEARNING_RATE=0.02' Example with Command-line arguments: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ --args=' number-of-epochs=30' \\ --args= 'batch-size=64' Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:AI CLI. Attached Files \u00b6 The 3 relevant files mentioned in this document can be downloaded from Github See Also \u00b6 See the unattended training walk-through: Launch Unattended Training Workloads","title":"Convert a Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practice-convert-your-workload-to-run-unattended","text":"","title":"Best Practice: Convert your Workload to Run Unattended"},{"location":"Researcher/best-practices/convert-to-unattended/#motivation","text":"Run:AI allows non-interactive training workloads to extend beyond guaranteed quotas and into over-quota as long as computing resources are available. To achieve this flexibility, the system needs to be able to safely stop a workload and restart it again later. This requires researchers to switch workloads from running interactively, to running unattended, thus allowing Run:AI to pause/resume the run. Unattended workloads are good for long-duration runs, or sets of smaller hyperparameter optmization runs.","title":"Motivation"},{"location":"Researcher/best-practices/convert-to-unattended/#best-practices","text":"","title":"Best Practices"},{"location":"Researcher/best-practices/convert-to-unattended/#docker-image","text":"A docker container is based on a docker image. Some researchers use generic images such as ones provided by Nvidia (e.g. https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow ). Others, use generic images as the base image to a more customized image using Dockerfiles https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ . Realizing that researchers are not always proficient with building docker files, as a best practice you will want to: Use the same docker image both for interactive and unattended jobs. In this way, you can keep the difference between both methods of invocation to a minimum. This can be a stock image from Nvidia or a custom image. Leave some degree of flexibility which allows the researcher to add/remove python dependencies without re-creating images. As such we recommend the following best practice:","title":"Docker Image"},{"location":"Researcher/best-practices/convert-to-unattended/#create-a-startup-script","text":"All the commands you run inside the interactive job after it has been allocated should be gathered into a single script. The script will be provided with the command-line at the start of the unattended execution (see the section running the job below). This script should be kept next to your code, on a shared network drive (e.g. /nfs/john ). An example of a very common startup script start.sh will be: pip install -r requirements.txt ... python training.py The first line of this script is there to make sure that all required python libraries are installed prior to the training script execution, it also allows the researcher to add/remove libraries without needing changes to the image itself.","title":"Create a Startup Script"},{"location":"Researcher/best-practices/convert-to-unattended/#support-variance-between-different-runs","text":"Your training script must be flexible enough to support variance in execution without changing the code. For example, you will want to change the number of epochs to run, apply a different set of hyperparameters, etc. There are two ways to handle this in your script. You can use one or both methods: Your script can read arguments passed to the script: python training.py --number-of-epochs=30 In which case, change your start.sh script to: pip install -r requirements.txt ... python training.py $@ Your script can read from environment variables during script execution. In case you use environment variables, they will be passed to the training script automatically. No special action is required in this case.","title":"Support Variance Between Different Runs"},{"location":"Researcher/best-practices/convert-to-unattended/#checkpoints","text":"Run:AI can pause unattended executions, giving your GPU resources to another workload. When the time comes, Run:AI will give you back the resources and restore your workload. Thus, it is a good practice to save your weights at various checkpoints and start a workload from the latest checkpoint (typically between epochs). TensorFlow, Pytorch, and others have mechanisms to help save checkpoints (e.g. https://www.tensorflow.org/guide/checkpoint for TensorFlow and https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html for Pytorch). It is important to save the checkpoints to network storage and not the machine itself. When your workload resumes, it can, in all probability, be allocated to a different node (machine) than the original node For more information on best practices for saving checkpoints, see: Saving Deep Learning Checkpoints .","title":"Checkpoints"},{"location":"Researcher/best-practices/convert-to-unattended/#running-the-job","text":"Using runai submit , drop the flag --interactive . For submitting a job using the script created above, please use --command , and pass arguments and/or environment variables using the runai submit flags --args and --environment . Example with Environment variables: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ -e 'EPOCHS=30' \\ -e 'LEARNING_RATE=0.02' Example with Command-line arguments: runai submit train1 -i nvcr.io/nvidia/tensorflow:20.03-tf1-py3 \\ --project my-project -v /nfs/john:/mydir -g 1 \\ --command ./startup.sh --working-dir /mydir/ \\ --args=' number-of-epochs=30' \\ --args= 'batch-size=64' Please refer to Command-Line Interface, runai submit for a list of all arguments accepted by the Run:AI CLI.","title":"Running the Job"},{"location":"Researcher/best-practices/convert-to-unattended/#attached-files","text":"The 3 relevant files mentioned in this document can be downloaded from Github","title":"Attached Files"},{"location":"Researcher/best-practices/convert-to-unattended/#see-also","text":"See the unattended training walk-through: Launch Unattended Training Workloads","title":"See Also"},{"location":"Researcher/cli-reference/Introduction/","text":"The Run:AI Command-line Interface (CLI) is one of the ways for a researcher to send deep learning workloads, acquire GPU-based containers, list jobs, etc. To install and configure the Run:AI CLI see Researcher Setup - Start Here","title":"Introduction"},{"location":"Researcher/cli-reference/runai-attach/","text":"Description \u00b6 Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the job has multiple pods the job will attach to the first pod unless otherwise set. Synopsis \u00b6 runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h] Options \u00b6 <job-name> the name of the job to run the command in --no-stdin Do not attach STDIN. --no-tty Do not allocate a pseudo-TTY --pod string Attach to a specific pod within the job. To find the list of pods run runai get <job-name> and then use the pod name with the --pod flag. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --help | -h Show help text. Output \u00b6 None","title":"runai attach"},{"location":"Researcher/cli-reference/runai-attach/#description","text":"Attach to a running Job. The command attaches to the standard input, output, and error streams of a running Job. If the job has multiple pods the job will attach to the first pod unless otherwise set.","title":"Description"},{"location":"Researcher/cli-reference/runai-attach/#synopsis","text":"runai attach <job-name> [--no-stdin ] [--no-tty] [--pod string] . [--loglevel value] [--help | -h]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-attach/#options","text":"<job-name> the name of the job to run the command in --no-stdin Do not attach STDIN. --no-tty Do not allocate a pseudo-TTY --pod string Attach to a specific pod within the job. To find the list of pods run runai get <job-name> and then use the pod name with the --pod flag.","title":"Options"},{"location":"Researcher/cli-reference/runai-attach/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-attach/#output","text":"None","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/","text":"Description \u00b6 Get a bash session inside a running job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command. Synopsis \u00b6 runai bash job-name [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> the name of the job to run the command in --pod string Specify a pod of a running job. To get a list of the pods of a specific job, run \"runai get \" command Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text Output \u00b6 The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in running state yet. See also \u00b6 Build Workloads: Walk-through Start and Use Interactive Build Workloads .","title":"runai bash"},{"location":"Researcher/cli-reference/runai-bash/#description","text":"Get a bash session inside a running job This command is a shortcut to runai exec ( runai exec -it job-name bash ). See runai exec for full documentation of the exec command.","title":"Description"},{"location":"Researcher/cli-reference/runai-bash/#synopsis","text":"runai bash job-name [ --pod string ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-bash/#options","text":"<job-name> the name of the job to run the command in --pod string Specify a pod of a running job. To get a list of the pods of a specific job, run \"runai get \" command","title":"Options"},{"location":"Researcher/cli-reference/runai-bash/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-bash/#output","text":"The command will access the container that should be currently running in the current cluster and attempt to create a command-line shell based on bash. The command will return an error if the container does not exist or has not been in running state yet.","title":"Output"},{"location":"Researcher/cli-reference/runai-bash/#see-also","text":"Build Workloads: Walk-through Start and Use Interactive Build Workloads .","title":"See also"},{"location":"Researcher/cli-reference/runai-cluster/","text":"Description \u00b6 Set the current cluster Show a list of available clusters Synopsis \u00b6 runai cluster set <cluster-name> [ --loglevel value ] [ --help | -h ] runai cluster list [ --loglevel value ] [ --help | -h ] Options \u00b6 <cluster-name> the name of the cluster you want to set as current Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 With these two commands you can show a list of available clusters as well as to be able to set a current cluster. The set command internally changes the default Kubernetes cluster","title":"runai cluster"},{"location":"Researcher/cli-reference/runai-cluster/#description","text":"Set the current cluster Show a list of available clusters","title":"Description"},{"location":"Researcher/cli-reference/runai-cluster/#synopsis","text":"runai cluster set <cluster-name> [ --loglevel value ] [ --help | -h ] runai cluster list [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-cluster/#options","text":"<cluster-name> the name of the cluster you want to set as current","title":"Options"},{"location":"Researcher/cli-reference/runai-cluster/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-cluster/#output","text":"With these two commands you can show a list of available clusters as well as to be able to set a current cluster. The set command internally changes the default Kubernetes cluster","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/","text":"Description \u00b6 Delete a training job and its associated pods. Note that once you delete a job, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able access logs. Any data saved on the container and not stored on a shared location will be lost. Synopsis \u00b6 runai delete <job-name> [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> the name of the job to run the command in --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text Output \u00b6 The job will be deleted and not available via the command runai list The job will not be deleted from the Run:AI user interface Job list See Also \u00b6 Build Workloads: Walk-through Start and Use Interactive Build Workloads . Training Workloads: Walk-through Start and Use Unattended Training Workloads .","title":"runai delete"},{"location":"Researcher/cli-reference/runai-delete/#description","text":"Delete a training job and its associated pods. Note that once you delete a job, its entire data will be gone: You will no longer be able to enter it via bash. You will no longer be able access logs. Any data saved on the container and not stored on a shared location will be lost.","title":"Description"},{"location":"Researcher/cli-reference/runai-delete/#synopsis","text":"runai delete <job-name> [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-delete/#options","text":"<job-name> the name of the job to run the command in --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text","title":"Options"},{"location":"Researcher/cli-reference/runai-delete/#output","text":"The job will be deleted and not available via the command runai list The job will not be deleted from the Run:AI user interface Job list","title":"Output"},{"location":"Researcher/cli-reference/runai-delete/#see-also","text":"Build Workloads: Walk-through Start and Use Interactive Build Workloads . Training Workloads: Walk-through Start and Use Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-exec/","text":"Description \u00b6 Execute a command inside a running job Note: to execute a bash command, you can also use the shorthand runai bash Synopsis \u00b6 runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --stdin | -i Keep STDIN open even if not attached --tty | -t Allocate a pseudo-TTY Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text Output \u00b6 The command will run in the context of the container See Also \u00b6","title":"runai exec"},{"location":"Researcher/cli-reference/runai-exec/#description","text":"Execute a command inside a running job Note: to execute a bash command, you can also use the shorthand runai bash","title":"Description"},{"location":"Researcher/cli-reference/runai-exec/#synopsis","text":"runai exec <job-name> <command> [ --stdin | -i ] [ --tty | -t ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-exec/#options","text":"<job-name> the name of the job to run the command in <command> the command itself (e.g. bash ) --stdin | -i Keep STDIN open even if not attached --tty | -t Allocate a pseudo-TTY","title":"Options"},{"location":"Researcher/cli-reference/runai-exec/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-exec/#output","text":"The command will run in the context of the container","title":"Output"},{"location":"Researcher/cli-reference/runai-exec/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-get/","text":"Description \u00b6 Display details of a training job Synopsis \u00b6 runai get <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> the name of the job to run the command in -o | --output Output format. One of: json|yaml|wide Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project, use: runai project set <project-name> . --help | -h Show help text Output \u00b6 The command will show the job properties and status as well as lifecycle events and list of pods See Also \u00b6","title":"runai get"},{"location":"Researcher/cli-reference/runai-get/#description","text":"Display details of a training job","title":"Description"},{"location":"Researcher/cli-reference/runai-get/#synopsis","text":"runai get <job-name> [ --output value | -o value ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-get/#options","text":"<job-name> the name of the job to run the command in -o | --output Output format. One of: json|yaml|wide","title":"Options"},{"location":"Researcher/cli-reference/runai-get/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project, use: runai project set <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-get/#output","text":"The command will show the job properties and status as well as lifecycle events and list of pods","title":"Output"},{"location":"Researcher/cli-reference/runai-get/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-list/","text":"Description \u00b6 Show list of jobs Synopsis \u00b6 runai list <job-name> [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 <job-name> the name of the job to run the command in --all-projects | -A Show jobs from all projects Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text Output \u00b6 A list of jobs will show. To show details for a specific job see runai get See Also \u00b6","title":"runai list"},{"location":"Researcher/cli-reference/runai-list/#description","text":"Show list of jobs","title":"Description"},{"location":"Researcher/cli-reference/runai-list/#synopsis","text":"runai list <job-name> [ --all-projects | -A ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-list/#options","text":"<job-name> the name of the job to run the command in --all-projects | -A Show jobs from all projects","title":"Options"},{"location":"Researcher/cli-reference/runai-list/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-list/#output","text":"A list of jobs will show. To show details for a specific job see runai get","title":"Output"},{"location":"Researcher/cli-reference/runai-list/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-logs/","text":"Description \u00b6 Show the logs of a job Synopsis \u00b6 runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ] Options \u00b6 --follow | -f Stream the logs. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai get job-name . --instance (string) | -i (string) Show logs for a specific instance in cases where a job contains multiple pods. --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together. --since-time (date-time) Return logs after specified date. Date format should beRFC3339, example: 2020-01-26T15:00:00Z . --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text. Output \u00b6 The command will show the logs of the first process in the container. For training jobs, this would be the command run at startup. For interactive jobs, the command may not show anything. See Also \u00b6 Training Workloads: Walk-through Start and Use Unattended Training Workloads .","title":"runai logs"},{"location":"Researcher/cli-reference/runai-logs/#description","text":"Show the logs of a job","title":"Description"},{"location":"Researcher/cli-reference/runai-logs/#synopsis","text":"runai logs <job-name> [ --follow | -f ] [ --pod string | -p string ] [ --since duration ] [ --since-time date-time ] [ --tail int | -t int ] [ --timestamps ] [ --loglevel value ] [ --project string | -p string ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-logs/#options","text":"--follow | -f Stream the logs. --pod | -p Specify a specific pod name. When a Job fails, it may start a couple of times in an attempt to succeed. The flag allows you to see the logs of a specific instance (called 'pod'). Get the name of the pod by running runai get job-name . --instance (string) | -i (string) Show logs for a specific instance in cases where a job contains multiple pods. --since (duration) Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to all logs. The flags since and since-time cannot be used together. --since-time (date-time) Return logs after specified date. Date format should beRFC3339, example: 2020-01-26T15:00:00Z . --tail (int) | -t (int) # of lines of recent log file to display. --timestamps Include timestamps on each line in the log output.","title":"Options"},{"location":"Researcher/cli-reference/runai-logs/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --project | -p (string) Specify the project to which the command applies. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-logs/#output","text":"The command will show the logs of the first process in the container. For training jobs, this would be the command run at startup. For interactive jobs, the command may not show anything.","title":"Output"},{"location":"Researcher/cli-reference/runai-logs/#see-also","text":"Training Workloads: Walk-through Start and Use Unattended Training Workloads .","title":"See Also"},{"location":"Researcher/cli-reference/runai-project/","text":"Description \u00b6 Set a default project Show a list of available projects Synopsis \u00b6 runai project set <project-name> [ --loglevel value ] [ --help | -h ] runai project list [ --loglevel value ] [ --help | -h ] Options \u00b6 <project-name> the name of the project you want to set as default Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 With these two commands you can show a list of available projects as well as to be able to set a default project, thus, removing the need to use the --project flag on other CLI commands.","title":"runai project"},{"location":"Researcher/cli-reference/runai-project/#description","text":"Set a default project Show a list of available projects","title":"Description"},{"location":"Researcher/cli-reference/runai-project/#synopsis","text":"runai project set <project-name> [ --loglevel value ] [ --help | -h ] runai project list [ --loglevel value ] [ --help | -h ]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-project/#options","text":"<project-name> the name of the project you want to set as default","title":"Options"},{"location":"Researcher/cli-reference/runai-project/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-project/#output","text":"With these two commands you can show a list of available projects as well as to be able to set a default project, thus, removing the need to use the --project flag on other CLI commands.","title":"Output"},{"location":"Researcher/cli-reference/runai-submit-mpi/","text":"Description \u00b6 Submit a Distributed Training (MPI) Run:AI job for execution Synopsis \u00b6 runai submit-mpi <job-name> [ --always-pull-image ] [ --args stringArray ] [ --attach ] [ --backoffLimit int ] [ --command stringArray ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --gpu double | -g double ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --stdin ] [ --template string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 start an unattended mpi training job of name dist1, based on project team-a using a quickstart-distributed image: runai submit-mpi dist1 --num-processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed (see: distributed training walk-through ). Options \u00b6 <job-name> the name of the job. Aliases and Shortcuts \u00b6 --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system. --template string Templates are currently not supported. Container Related \u00b6 --always-pull-image stringArray When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a panelty of performance degradation on job start. --args stringArray Arguments to pass to the command running on container start. Use together with --command . Example: --command sleep --args 10000 . --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command stringArray Command to run at container start. Use together with --args . Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ). --image string | -i string Image to use when creating the container for this Job --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY --working-dir string Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit double Limitations on the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocate to the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit string CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. Network \u00b6 --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference documentation. --host-network Use the host's networkstack inside the container. For further information see docker run reference documentation. Job Lifecycle \u00b6 --processes int Number of distributed training processes. The default is 1. Access Control \u00b6 --create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. --prevent-privilege-escalation Prevent the job\u2019s container and all launched processes from gaining additional privileges after the job starts. Default is false . For more information see Privilege Escalation . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. Scheduling \u00b6 --node-type string Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: Working with Projects . Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text Output \u00b6 The command will attempt to submit an mpi job. You can follow up on the job by running runai list or runai get job-name See Also \u00b6 See Walk-through document Walk-through: Running Distributed Training .","title":"runai submit-mpi"},{"location":"Researcher/cli-reference/runai-submit-mpi/#description","text":"Submit a Distributed Training (MPI) Run:AI job for execution","title":"Description"},{"location":"Researcher/cli-reference/runai-submit-mpi/#synopsis","text":"runai submit-mpi <job-name> [ --always-pull-image ] [ --args stringArray ] [ --attach ] [ --backoffLimit int ] [ --command stringArray ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --environment stringArray | -e stringArray ] [ --gpu double | -g double ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --node-type string ] [ --prevent-privilege-escalation ] [ --processes int ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --stdin ] [ --template string ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] Syntax notes: Options with value type of stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit-mpi/#examples","text":"start an unattended mpi training job of name dist1, based on project team-a using a quickstart-distributed image: runai submit-mpi dist1 --num-processes=2 -g 1 \\ -i gcr.io/run-ai-demo/quickstart-distributed (see: distributed training walk-through ).","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit-mpi/#options","text":"<job-name> the name of the job.","title":"Options"},{"location":"Researcher/cli-reference/runai-submit-mpi/#aliases-and-shortcuts","text":"--interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system. --template string Templates are currently not supported.","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit-mpi/#container-related","text":"--always-pull-image stringArray When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a panelty of performance degradation on job start. --args stringArray Arguments to pass to the command running on container start. Use together with --command . Example: --command sleep --args 10000 . --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command stringArray Command to run at container start. Use together with --args . Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ). --image string | -i string Image to use when creating the container for this Job --local-image Use a local image for this job. A local image is an image which exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY --working-dir string Starts the container with the specified directory as the current directory.","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit-mpi/#resource-allocation","text":"--cpu double CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit double Limitations on the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocate to the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit string CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit-mpi/#storage","text":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively.","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit-mpi/#network","text":"--host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference documentation. --host-network Use the host's networkstack inside the container. For further information see docker run reference documentation.","title":"Network"},{"location":"Researcher/cli-reference/runai-submit-mpi/#job-lifecycle","text":"--processes int Number of distributed training processes. The default is 1.","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit-mpi/#access-control","text":"--create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. --prevent-privilege-escalation Prevent the job\u2019s container and all launched processes from gaining additional privileges after the job starts. Default is false . For more information see Privilege Escalation . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc.","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit-mpi/#scheduling","text":"--node-type string Allows defining specific Nodes (machines) or a group of Nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: Working with Projects .","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit-mpi/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit-mpi/#output","text":"The command will attempt to submit an mpi job. You can follow up on the job by running runai list or runai get job-name","title":"Output"},{"location":"Researcher/cli-reference/runai-submit-mpi/#see-also","text":"See Walk-through document Walk-through: Running Distributed Training .","title":"See Also"},{"location":"Researcher/cli-reference/runai-submit/","text":"Description \u00b6 Submit a Run:AI job for execution Synopsis \u00b6 runai submit <job-name> [ --always-pull-image ] [ --args stringArray ] [ --attach ] [ --backoffLimit int ] [ --command stringArray ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --elastic ] [ --environment stringArray | -e stringArray ] [ --gpu double | -g double ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --jupyter ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --service-type string | -s string ] [ --stdin ] [ --template string ] [ --ttl-after-finish duration ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice. Examples \u00b6 All examples assume a Run:AI project has been set using runai project set <project-name> . Start an interactive job: runai submit build1 -i python -g 1 --interactive --command sleep --args infinity (see: build walk-through ). Externalize ports: runai submit build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --command \"/usr/sbin/sshd\" --args \"-D\" --service-type=nodeport --port 30022:22 (see: build walk-through with ports ). Start a Training job runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training walk-through ). Use GPU Fractions runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions walk-through ). Hyperparameter Optimization runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization walk-through ). Options \u00b6 <job-name> - the name of the job. Aliases and Shortcuts \u00b6 --interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system. --jupyter Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit jup1 --jupyter -g 0.5 --service-type=ingress will start an interactive session named jup1 and use an ingress load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list to find the URL for the notebook. --template string Templates are currently not supported. Container Related \u00b6 --always-pull-image stringArray When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a panelty of performance degradation on job start. --args stringArray Arguments to pass to the command running on container start. Use together with --command . Example: --command script.py --args 10000 --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command stringArray Command to run at container start. Use together with --args . Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ) --image string | -i string Image to use when creating the container for this Job --local-image Use a local image for this job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY. --working-dir string Starts the container with the specified directory as the current directory. Resource Allocation \u00b6 --cpu double CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit double Limitations on the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocate to the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit string CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit. Storage \u00b6 --pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. Network \u00b6 --host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference . --host-network Use the host's network stack inside the container. For further information see docker run reference . --port stringArray Expose ports from the Job container. Used together with --service-type . Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --service-type string | -s string Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport , ingress. Use the command runai list to obtain the endpoint to use the service when the job is running. Different service methods have different endpoint structure. Job Lifecycle \u00b6 --backoffLimit int The number of times the job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --completions int The number of successful pods required for this job to be completed. Used for Hyperparameter optimization . Use together with --parallelism . --parallelism int The number of pods this job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions . --ttl-after-finish duration Define the duration, post job finish, after which the job is automatically deleted (5s, 2m, 3h, etc). Note: This setting must first be enabled at the cluster level. See Automatically Delete Jobs After Job Finish . Access Control \u00b6 --create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. --prevent-privilege-escalation Prevent the job\u2019s container and all launched processes from gaining additional privileges after the job starts. Default is false . For more information see Privilege Escalation . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc. Scheduling \u00b6 --elastic Mark the job as elastic. For further information on Elasticity see Elasticity Dynamically Stretch Compress Jobs According to GPU Availability . --node-type string Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: Working with Projects . --preemptible Mark an interactive job as preemptible. Preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text. Output \u00b6 The command will attempt to submit a job. You can follow up on the job by running runai list or runai get job-name -e Note that the submit call may use templates to provide defaults to any of the above flags. See Also \u00b6 See any of the Walk-through documents here: Run:AI Walk-through","title":"runai submit"},{"location":"Researcher/cli-reference/runai-submit/#description","text":"Submit a Run:AI job for execution","title":"Description"},{"location":"Researcher/cli-reference/runai-submit/#synopsis","text":"runai submit <job-name> [ --always-pull-image ] [ --args stringArray ] [ --attach ] [ --backoffLimit int ] [ --command stringArray ] [ --completions int ] [ --cpu double ] [ --cpu-limit double ] [ --create-home-dir ] [ --elastic ] [ --environment stringArray | -e stringArray ] [ --gpu double | -g double ] [ --host-ipc ] [ --host-network ] [ --image string | -i string ] [ --interactive ] [ --jupyter ] [ --large-shm ] [ --local-image ] [ --memory string ] [ --memory-limit string ] [ --node-type string ] [ --parallelism int ] [ --port stringArray ] [ --preemptible ] [ --prevent-privilege-escalation ] [ --pvc [ StorageClassName ] :Size:ContainerMountPath: [ ro ]] [ --run-as-user ] [ --service-type string | -s string ] [ --stdin ] [ --template string ] [ --ttl-after-finish duration ] [ --tty | -t ] [ --volume stringArray | -v stringArray ] [ --working-dir ] [ --loglevel string ] [ --project string | -p string ] [ --help | -h ] Syntax notes: Flags of type stringArray mean that you can add multiple values. You can either separate values with a comma or add the flag twice.","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-submit/#examples","text":"All examples assume a Run:AI project has been set using runai project set <project-name> . Start an interactive job: runai submit build1 -i python -g 1 --interactive --command sleep --args infinity (see: build walk-through ). Externalize ports: runai submit build-remote -i rastasheep/ubuntu-sshd:14.04 --interactive \\ --command \"/usr/sbin/sshd\" --args \"-D\" --service-type=nodeport --port 30022:22 (see: build walk-through with ports ). Start a Training job runai submit train1 -i gcr.io/run-ai-demo/quickstart -g 1 (see: training walk-through ). Use GPU Fractions runai submit frac05 -i gcr.io/run-ai-demo/quickstart -g 0.5 (see: GPU fractions walk-through ). Hyperparameter Optimization runai submit hpo1 -i gcr.io/run-ai-demo/quickstart-hpo -g 1 \\ --parallelism 3 --completions 12 -v /nfs/john/hpo:/hpo (see: hyperparameter optimization walk-through ).","title":"Examples"},{"location":"Researcher/cli-reference/runai-submit/#options","text":"<job-name> - the name of the job.","title":"Options"},{"location":"Researcher/cli-reference/runai-submit/#aliases-and-shortcuts","text":"--interactive Mark this Job as Interactive. Interactive jobs are not terminated automatically by the system. --jupyter Shortcut for running a Jupyter notebook container. Uses a pre-created image and a default notebook configuration. Example: runai submit jup1 --jupyter -g 0.5 --service-type=ingress will start an interactive session named jup1 and use an ingress load balancer to connect to it. The output of the command is an access token for the notebook. Run runai list to find the URL for the notebook. --template string Templates are currently not supported.","title":"Aliases and Shortcuts"},{"location":"Researcher/cli-reference/runai-submit/#container-related","text":"--always-pull-image stringArray When starting a container, always pull the image from the registry, even if the image is cached on the running node. This is useful when you are re-saving updates to the image using the same tag, but may incur a panelty of performance degradation on job start. --args stringArray Arguments to pass to the command running on container start. Use together with --command . Example: --command script.py --args 10000 --attach Default is false. If set to true, wait for the Pod to start running. When the pod starts running, attach to the Pod. The flag is equivalent to the command runai attach . The --attach flag also sets --tty and --stdin to true. --command stringArray Command to run at container start. Use together with --args . Example: --command script.py --args 10000 -e stringArray | --environment stringArray Define environment variables to be set in the container. To set multiple values add the flag multiple times ( -e BATCH_SIZE=50 -e LEARNING_RATE=0.2 ) or separate by a comma ( -e BATCH_SIZE:50,LEARNING_RATE:0.2 ) --image string | -i string Image to use when creating the container for this Job --local-image Use a local image for this job. A local image is an image that exists on all local servers of the Kubernetes Cluster. --stdin Keep stdin open for the container(s) in the pod, even if nothing is attached. -t, --tty Allocate a pseudo-TTY. --working-dir string Starts the container with the specified directory as the current directory.","title":"Container Related"},{"location":"Researcher/cli-reference/runai-submit/#resource-allocation","text":"--cpu double CPU units to allocate for the job (0.5, 1, .etc). The Job will receive at least this amount of CPU. Note that the Job will not be scheduled unless the system can guarantee this amount of CPUs to the job. --cpu-limit double Limitations on the number of CPU consumed by the job (0.5, 1, .etc). The system guarantees that this Job will not be able to consume more than this amount of GPUs. --gpu double | -g double Number of GPUs to allocate to the Job. The default is no allocated GPUs. the GPU value can be an integer or a fraction between 0 and 1. --large-shm Mount a large /dev/shm device. An shm is a shared file system mounted on RAM. --memory string CPU memory to allocate for this job (1G, 20M, .etc). The Job will receive at least this amount of memory. Note that the Job will not be scheduled unless the system can guarantee this amount of memory to the job. --memory-limit string CPU memory to allocate for this job (1G, 20M, .etc). The system guarantees that this Job will not be able to consume more than this amount of memory. The Job will receive an error when trying to allocate more memory than this limit.","title":"Resource Allocation"},{"location":"Researcher/cli-reference/runai-submit/#storage","text":"--pvc [Storage_Class_Name]:Size:Container_Mount_Path:[ro] --pvc Pvc_Name:Container_Mount_Path:[ro] Mount a persistent volume claim of Network Attached Storage into a container. The 2 syntax types of this command are mutually exclusive. You can either use the first or second form, but not a mixture of both. Storage_Class_Name is a storage class name which can be obtained by running kubectl get storageclasses.storage.k8s.io . This parameter may be omitted if there is a single storage class in the system, or you are using the default storage class. Size is the volume size you want to allocate. See Kubernetes documentation for how to specify volume sizes Container_Mount_Path . A path internal to the container where the storage will be mounted Pvc_Name . The name of a pre-existing Persistent Volume Claim to mount into the container Examples: --pvc :3Gi:/tmp/john:ro - Allocate 3GB from the default Storage class. Mount it to /tmp/john as read-only --pvc my-storage:3Gi:/tmp/john:ro - Allocate 3GB from the my-storage storage class. Mount it to /tmp/john as read-only --pvc :3Gi:/tmp/john - Allocate 3GB from the default storage class. Mount it to /tmp/john as read-write --pvc my-pvc:/tmp/john - Use a Persistent Volume Claim named my-pvc . Mount it to /tmp/john as read-write --pvc my-pvc-2:/tmp/john:ro - Use a Persistent Volume Claim named my-pvc-2 . Mount it to /tmp/john as read-only --volume stringArray | -v stringArray Volume to mount into the container. Example -v /raid/public/john/data:/root/data:ro The flag may optionally be suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively.","title":"Storage"},{"location":"Researcher/cli-reference/runai-submit/#network","text":"--host-ipc Use the host's ipc namespace. Controls whether the pod containers can share the host IPC namespace. IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. For further information see docker run reference . --host-network Use the host's network stack inside the container. For further information see docker run reference . --port stringArray Expose ports from the Job container. Used together with --service-type . Examples: --port 8080:80 --service-type loadbalancer --port 8080 --service-type ingress --service-type string | -s string Service exposure method for interactive Job. Options are: portforward , loadbalancer , nodeport , ingress. Use the command runai list to obtain the endpoint to use the service when the job is running. Different service methods have different endpoint structure.","title":"Network"},{"location":"Researcher/cli-reference/runai-submit/#job-lifecycle","text":"--backoffLimit int The number of times the job will be retried before failing. The default is 6. This flag will only work with training workloads (when the --interactive flag is not specified). --completions int The number of successful pods required for this job to be completed. Used for Hyperparameter optimization . Use together with --parallelism . --parallelism int The number of pods this job tries to run in parallel at any time. Used for Hyperparameter optimization . Use together with --completions . --ttl-after-finish duration Define the duration, post job finish, after which the job is automatically deleted (5s, 2m, 3h, etc). Note: This setting must first be enabled at the cluster level. See Automatically Delete Jobs After Job Finish .","title":"Job Lifecycle"},{"location":"Researcher/cli-reference/runai-submit/#access-control","text":"--create-home-dir Create a temporary home directory for the user in the container. Data saved in this directory will not be saved when the container exits. The flag is set by default to true when the --run-as-user flag is used, and false if not. --prevent-privilege-escalation Prevent the job\u2019s container and all launched processes from gaining additional privileges after the job starts. Default is false . For more information see Privilege Escalation . --run-as-user Run in the context of the current user running the Run:AI command rather than the root user. While the default container user is root (same as in Docker), this command allows you to submit a job running under your Linux user. This would manifest itself in access to operating system resources, in the owner of new folders created under shared directories etc.","title":"Access Control"},{"location":"Researcher/cli-reference/runai-submit/#scheduling","text":"--elastic Mark the job as elastic. For further information on Elasticity see Elasticity Dynamically Stretch Compress Jobs According to GPU Availability . --node-type string Allows defining specific nodes (machines) or a group of nodes on which the workload will run. To use this feature your administrator will need to label nodes as explained here: Limit a Workload to a Specific Node Group . This flag can be used in conjunction with Project-based affinity. In this case, the flag is used to refine the list of allowable node groups set in the project. For more information see: Working with Projects . --preemptible Mark an interactive job as preemptible. Preemptible jobs can be scheduled above guaranteed quota but may be reclaimed at any time.","title":"Scheduling"},{"location":"Researcher/cli-reference/runai-submit/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\"). --project | -p (string) Specify the project to which the command applies. Run:AI Projects are used by the scheduler to calculate resource eligibility. By default, commands apply to the default project. To change the default project use runai project set <project-name> . --help | -h Show help text.","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-submit/#output","text":"The command will attempt to submit a job. You can follow up on the job by running runai list or runai get job-name -e Note that the submit call may use templates to provide defaults to any of the above flags.","title":"Output"},{"location":"Researcher/cli-reference/runai-submit/#see-also","text":"See any of the Walk-through documents here: Run:AI Walk-through","title":"See Also"},{"location":"Researcher/cli-reference/runai-template/","text":"Note Templates are currently not supported Description \u00b6 Templates are a way to reduce the amount of flags required when running the command runai submit . A template is added by the administrator and is out of scope for this article. A researcher can: Review list of templates by running runai template list Review the contents of a specific template by running runai template get <template-name> Use a template by running runai submit --template <template-name> The administrator can also set a default template which is always used on runai submit whenever a template is not specified. Synopsis \u00b6 runai template get <template-name> runai template list Options \u00b6 <template-name> the name of the template to run the command on runai template list will show the list of existing templates. Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") Output \u00b6 runai template list will show a list of templates. Example: runai template get to get the template details Use the template: runai submit my-pytorch1 --template pytorch-default See Also \u00b6 See: Configure Command-Line Interface Templates on how to configure templates.","title":"Runai template"},{"location":"Researcher/cli-reference/runai-template/#description","text":"Templates are a way to reduce the amount of flags required when running the command runai submit . A template is added by the administrator and is out of scope for this article. A researcher can: Review list of templates by running runai template list Review the contents of a specific template by running runai template get <template-name> Use a template by running runai submit --template <template-name> The administrator can also set a default template which is always used on runai submit whenever a template is not specified.","title":"Description"},{"location":"Researcher/cli-reference/runai-template/#synopsis","text":"runai template get <template-name> runai template list","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-template/#options","text":"<template-name> the name of the template to run the command on runai template list will show the list of existing templates.","title":"Options"},{"location":"Researcher/cli-reference/runai-template/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\")","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-template/#output","text":"runai template list will show a list of templates. Example: runai template get to get the template details Use the template: runai submit my-pytorch1 --template pytorch-default","title":"Output"},{"location":"Researcher/cli-reference/runai-template/#see-also","text":"See: Configure Command-Line Interface Templates on how to configure templates.","title":"See Also"},{"location":"Researcher/cli-reference/runai-top-node/","text":"Description \u00b6 Show list of nodes (machines) and their properties Synopsis \u00b6 runai top node [--help | -h] Options \u00b6 Global Flags \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 Shows a list of nodes and their properties See Also \u00b6","title":"runai top node"},{"location":"Researcher/cli-reference/runai-top-node/#description","text":"Show list of nodes (machines) and their properties","title":"Description"},{"location":"Researcher/cli-reference/runai-top-node/#synopsis","text":"runai top node [--help | -h]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-top-node/#options","text":"","title":"Options"},{"location":"Researcher/cli-reference/runai-top-node/#global-flags","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Global Flags"},{"location":"Researcher/cli-reference/runai-top-node/#output","text":"Shows a list of nodes and their properties","title":"Output"},{"location":"Researcher/cli-reference/runai-top-node/#see-also","text":"","title":"See Also"},{"location":"Researcher/cli-reference/runai-update/","text":"Description \u00b6 Find and install the latest version of the runai command-line utility. On mac and Linux the command must be run with sudo: sudo runai update Synopsis \u00b6 runai update [--loglevel value] [--help | -h] Options \u00b6 --loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text Output \u00b6 Update of the runai command-line interface See Also \u00b6","title":"runai update"},{"location":"Researcher/cli-reference/runai-update/#description","text":"Find and install the latest version of the runai command-line utility. On mac and Linux the command must be run with sudo: sudo runai update","title":"Description"},{"location":"Researcher/cli-reference/runai-update/#synopsis","text":"runai update [--loglevel value] [--help | -h]","title":"Synopsis"},{"location":"Researcher/cli-reference/runai-update/#options","text":"--loglevel (string) Set the logging level. One of: debug|info|warn|error (default \"info\") --help | -h Show help text","title":"Options"},{"location":"Researcher/cli-reference/runai-update/#output","text":"Update of the runai command-line interface","title":"Output"},{"location":"Researcher/cli-reference/runai-update/#see-also","text":"","title":"See Also"},{"location":"Researcher/researcher-library/researcher-library-overview/","text":"Overview: The Run:AI Researcher Library \u00b6 Introduction \u00b6 Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability. Support for experiment management when performing hyperparameter optimization The library is open-source and can be reviewed here . Installing the Run:AI Researcher Library \u00b6 In your command-line run: pip install runai Run:AI Researcher Library Modules \u00b6 To review details on the specific Run:AI Researcher Library modules see: Reporting via the Run:AI Researcher Library Elasticity, Dynamically Stretch/Compress Jobs According to GPU Availability Hyperparameter optimization support","title":"Overview"},{"location":"Researcher/researcher-library/researcher-library-overview/#overview-the-runai-researcher-library","text":"","title":"Overview: The Run:AI Researcher Library"},{"location":"Researcher/researcher-library/researcher-library-overview/#introduction","text":"Run:AI provides a python library that can optionally be installed within your docker image and activated during the deep learning session. When installed, the library provides: Additional progress reporting and metrics Ability to dynamically stretch and compress jobs according to GPU availability. Support for experiment management when performing hyperparameter optimization The library is open-source and can be reviewed here .","title":"Introduction"},{"location":"Researcher/researcher-library/researcher-library-overview/#installing-the-runai-researcher-library","text":"In your command-line run: pip install runai","title":"Installing the Run:AI Researcher Library"},{"location":"Researcher/researcher-library/researcher-library-overview/#runai-researcher-library-modules","text":"To review details on the specific Run:AI Researcher Library modules see: Reporting via the Run:AI Researcher Library Elasticity, Dynamically Stretch/Compress Jobs According to GPU Availability Hyperparameter optimization support","title":"Run:AI Researcher Library Modules"},{"location":"Researcher/researcher-library/rl-elasticity/","text":"Researcher Library: Dynamically Stretch or Compress Workload's GPU Allocation \u00b6 Introduction \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability. Expanding a Workload \u00b6 Expanding a training job allows your workload to run on more GPUs than the researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing workloads to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs. Shrinking a Workload \u00b6 Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a researcher to run a workload, albeit slower than intended, and let it automatically expand when GPUs become available at a later time. Shrinking a training job uses an algorithm called Gradient Accumulation . For more information about the algorithm see: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa Installation \u00b6 Install the Run:AI Python library using the following command: pip install runai Code \u00b6 In your python code, if using Keras, add: import runai.elastic.keras If using PyTorch, add: import runai.elastic.torch Initizalization \u00b6 To initialize the module, you need two parameters: Maximum GPU batch size - The maximum batch size that your job can use on a single GPU (in terms of GPU memory). Without Elasticity, running with batch sizes larger than this number will cause a memory overflow. This number will be used by the Run:AI elasticity module for determining whether to use Gradient Accumulation or not. Global batch size - The desired batch size. Of course, if this number is larger than the Maximum GPU batch size defined above, the model will not fit into a single GPU. The elasticity module will then use Gradient Accumulation and multiple GPUs to run your code. Call the init() method from the imported module and pass these two arguments. For example, if you are using PyTorch, use the following line: runai.elastic.torch.init(256, 64) Where 256 is the global batch size and 64 is the maximum GPU batch size . Usage \u00b6 Keras \u00b6 Create a Keras model: model = runai.elastic.keras.models.Model(model) PyTorch \u00b6 For PyTorch models, you'll need to wrap your Optimizer with Gradient Accumulation: optimizer = runai.ga.torch.optim.Optimizer(optimizer, runai.elastic.steps) Where runai.elastic.steps is the value of accumulated steps which is calculated when calling init above. In addition, you will need to data-parallelise your model. We recommend using the built-in nn.DataParallel() method: model = torch.nn.DataParallel(model) Running a Training Workload \u00b6 Run the training workload by using the \"elastic\" flag: When launching the job with the runai submit command use --elastic When launching a job via YAML code, use the label \"elastic\" with the value \"true\" For additional information on how to run elastic training workloads, see the following walkthrough . Limitationss \u00b6 Elasticity currently works with Keras-based or PyTorch-based deep learning code only. Any training job using Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Take care to save checkpoints in your code and have your code resume from the latest checkpoint rather than start from the beginning. See Also \u00b6 For additional documentation as well as Python examples see our GitHub repository","title":"Elasticity"},{"location":"Researcher/researcher-library/rl-elasticity/#researcher-library-dynamically-stretch-or-compress-workloads-gpu-allocation","text":"","title":"Researcher Library: Dynamically Stretch or Compress Workload's GPU Allocation"},{"location":"Researcher/researcher-library/rl-elasticity/#introduction","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The library contains an elasticity module which allows train workloads to shrink or expand based on the cluster's availability.","title":"Introduction"},{"location":"Researcher/researcher-library/rl-elasticity/#expanding-a-workload","text":"Expanding a training job allows your workload to run on more GPUs than the researcher code was originally written for. This is useful for maximizing the utilization of the cluster as a whole as well as allowing workloads to run faster if idle GPUs exist in the cluster. The extra GPUs will be automatically reclaimed if needed by other, prioritized jobs.","title":"Expanding a Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#shrinking-a-workload","text":"Shrinking a training job allows your workload to run on a smaller number of GPUs than the researcher code was originally written for. This is useful for maximizing utilization of the cluster as a whole as well as allowing a researcher to run a workload, albeit slower than intended, and let it automatically expand when GPUs become available at a later time. Shrinking a training job uses an algorithm called Gradient Accumulation . For more information about the algorithm see: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa","title":"Shrinking a Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#installation","text":"Install the Run:AI Python library using the following command: pip install runai","title":"Installation"},{"location":"Researcher/researcher-library/rl-elasticity/#code","text":"In your python code, if using Keras, add: import runai.elastic.keras If using PyTorch, add: import runai.elastic.torch","title":"Code"},{"location":"Researcher/researcher-library/rl-elasticity/#initizalization","text":"To initialize the module, you need two parameters: Maximum GPU batch size - The maximum batch size that your job can use on a single GPU (in terms of GPU memory). Without Elasticity, running with batch sizes larger than this number will cause a memory overflow. This number will be used by the Run:AI elasticity module for determining whether to use Gradient Accumulation or not. Global batch size - The desired batch size. Of course, if this number is larger than the Maximum GPU batch size defined above, the model will not fit into a single GPU. The elasticity module will then use Gradient Accumulation and multiple GPUs to run your code. Call the init() method from the imported module and pass these two arguments. For example, if you are using PyTorch, use the following line: runai.elastic.torch.init(256, 64) Where 256 is the global batch size and 64 is the maximum GPU batch size .","title":"Initizalization"},{"location":"Researcher/researcher-library/rl-elasticity/#usage","text":"","title":"Usage"},{"location":"Researcher/researcher-library/rl-elasticity/#keras","text":"Create a Keras model: model = runai.elastic.keras.models.Model(model)","title":"Keras"},{"location":"Researcher/researcher-library/rl-elasticity/#pytorch","text":"For PyTorch models, you'll need to wrap your Optimizer with Gradient Accumulation: optimizer = runai.ga.torch.optim.Optimizer(optimizer, runai.elastic.steps) Where runai.elastic.steps is the value of accumulated steps which is calculated when calling init above. In addition, you will need to data-parallelise your model. We recommend using the built-in nn.DataParallel() method: model = torch.nn.DataParallel(model)","title":"PyTorch"},{"location":"Researcher/researcher-library/rl-elasticity/#running-a-training-workload","text":"Run the training workload by using the \"elastic\" flag: When launching the job with the runai submit command use --elastic When launching a job via YAML code, use the label \"elastic\" with the value \"true\" For additional information on how to run elastic training workloads, see the following walkthrough .","title":"Running a Training Workload"},{"location":"Researcher/researcher-library/rl-elasticity/#limitationss","text":"Elasticity currently works with Keras-based or PyTorch-based deep learning code only. Any training job using Run:AI is subject to pause/resume episodes. Elasticity may increase these episodes, making it even more important to make your code resilient. Take care to save checkpoints in your code and have your code resume from the latest checkpoint rather than start from the beginning.","title":"Limitationss"},{"location":"Researcher/researcher-library/rl-elasticity/#see-also","text":"For additional documentation as well as Python examples see our GitHub repository","title":"See Also"},{"location":"Researcher/researcher-library/rl-hpo-support/","text":"Researcher Library: Hyperparameter Optimization Support \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing. Getting Started \u00b6 Prerequisites \u00b6 Run:AI HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml Installing \u00b6 Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3) Usage \u00b6 Import the runai.hpo package. import runai.hpo Initialize the Run:AI HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. runai.hpo.init('/path/to/nfs', 'model-abcd-hpo') Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai.hpo.Strategy.GridSearch Call the Run:AI HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai.hpo.pick( grid=dict( batch_size=[32, 64, 128], lr=[1, 0.1, 0.01, 0.001]), strategy=strategy) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai.hpo.report(epoch=5, metrics={ 'accuracy': 0.87 }) See Also \u00b6 See hyperparameter Optimization Walk-through","title":"HPO"},{"location":"Researcher/researcher-library/rl-hpo-support/#researcher-library-hyperparameter-optimization-support","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The hyperparameter optimization(HPO) support module of the library is helper library for hyperparameter optimization (HPO) experiments Hyperparameter optimization (HPO) is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. Example hyperparameters: Learning rate, Batch size, Different optimizers, number of layers. To search for good hyperparameters, Researchers typically start a series of small runs with different hyperparameter values, let them run for a while and then examine results to decide what works best. With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing.","title":"Researcher Library: Hyperparameter Optimization Support"},{"location":"Researcher/researcher-library/rl-hpo-support/#getting-started","text":"","title":"Getting Started"},{"location":"Researcher/researcher-library/rl-hpo-support/#prerequisites","text":"Run:AI HPO library is dependent on PyYAML . Install it using the command: pip install pyyaml","title":"Prerequisites"},{"location":"Researcher/researcher-library/rl-hpo-support/#installing","text":"Install the runai Python library using pip using the following command: pip install runai Make sure to use the correct pip installer (you might need to use pip3 for Python3)","title":"Installing"},{"location":"Researcher/researcher-library/rl-hpo-support/#usage","text":"Import the runai.hpo package. import runai.hpo Initialize the Run:AI HPO library with a path to a directory shared between all cluster nodes (typically using an NFS server). We recommend specifying a unique name for the experiment, the name will be used to create a sub-directory on the shared folder. runai.hpo.init('/path/to/nfs', 'model-abcd-hpo') Decide on an HPO strategy: Random search - randomly pick a set of hyperparameter values Grid search - pick the next set of hyperparameter values, iterating through all sets across multiple experiments strategy = runai.hpo.Strategy.GridSearch Call the Run:AI HPO library to specify a set of hyperparameters and pick a specific configuration for this experiment. config = runai.hpo.pick( grid=dict( batch_size=[32, 64, 128], lr=[1, 0.1, 0.01, 0.001]), strategy=strategy) Use the returned configuration in your code. For example: optimizer = keras.optimizers.SGD(lr=config['lr']) Metrics could be reported and saved in the experiment directory under the fule runai.yaml using runai.hpo.report . You should pass the epoch number and a dictionary with metrics to be reported. For example: runai.hpo.report(epoch=5, metrics={ 'accuracy': 0.87 })","title":"Usage"},{"location":"Researcher/researcher-library/rl-hpo-support/#see-also","text":"See hyperparameter Optimization Walk-through","title":"See Also"},{"location":"Researcher/researcher-library/rl-reporting/","text":"Researcher Library: Extended Reporting on Workload Progress \u00b6 The Run:AI Researcher Library is a python library you can add to your deep learning python code. The reporting module in the library will externalize information about the run which can then be available for users of the Run:AI user interface ( https://app.run.ai ) With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing. Sending Metrics \u00b6 Python Deep-Learning Code \u00b6 In your command-line run: pip install runai In your python code add: import runai.reporter To send a number-based metric report, write: reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reportMetric(\"accuracy\", 0.34) To send a text-based metric report, write: reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reportParameter(\"state\", \"Training Model\") Recommended Metrics to send \u00b6 For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric Each Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the job progress bar is computed by dividing these parameters. Automatic Sending of Metrics for Keras-Based Scripts \u00b6 For Keras based deep learning runs, there is a python code that automates the task of sending metrics. Install the library as above and reference runai.reporter from your code. Then write: runai.reporter.autolog() The above metrics will automatically be sent going forward. Adding the Metrics to the User interface \u00b6 The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send a metrics via the reporter library Run the workload once to send initial data. Go to Jobs list: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Reporting"},{"location":"Researcher/researcher-library/rl-reporting/#researcher-library-extended-reporting-on-workload-progress","text":"The Run:AI Researcher Library is a python library you can add to your deep learning python code. The reporting module in the library will externalize information about the run which can then be available for users of the Run:AI user interface ( https://app.run.ai ) With the reporter module, you can externalize information such as progress, accuracy, and loss over time/epoch and more. In addition, you can externalize custom metrics of your choosing.","title":"Researcher Library: Extended Reporting on Workload Progress"},{"location":"Researcher/researcher-library/rl-reporting/#sending-metrics","text":"","title":"Sending Metrics"},{"location":"Researcher/researcher-library/rl-reporting/#python-deep-learning-code","text":"In your command-line run: pip install runai In your python code add: import runai.reporter To send a number-based metric report, write: reportMetric(<reporter_metric_name>, <reporter_metric_value>) For example, reportMetric(\"accuracy\", 0.34) To send a text-based metric report, write: reportParameter(<reporter_param_name>, <reporter_param_value>) For example, reportParameter(\"state\", \"Training Model\")","title":"Python Deep-Learning Code"},{"location":"Researcher/researcher-library/rl-reporting/#recommended-metrics-to-send","text":"For the sake of uniformity with the Keras implementation (see below), we recommend sending the following metrics: Metric Type Frequency of Send Description accuracy numeric Each step Current accuracy of run loss numeric Each step Current result of loss function of run learning_rate numeric Once Defined learning rate of run step numeric Each Step Current step of run number_of_layers numeric Once Number of layers defined for the run optimizer_name text Once Name of Deep Learning Optimizer batch_size numeric Once Size of batch epoch numeric Each epoch Current Epoch number overall_epochs numeric Once Total number of epochs epoch and overall_epochs are especially important since the job progress bar is computed by dividing these parameters.","title":"Recommended Metrics to send"},{"location":"Researcher/researcher-library/rl-reporting/#automatic-sending-of-metrics-for-keras-based-scripts","text":"For Keras based deep learning runs, there is a python code that automates the task of sending metrics. Install the library as above and reference runai.reporter from your code. Then write: runai.reporter.autolog() The above metrics will automatically be sent going forward.","title":"Automatic Sending of Metrics for Keras-Based Scripts"},{"location":"Researcher/researcher-library/rl-reporting/#adding-the-metrics-to-the-user-interface","text":"The metrics show up in the Job list of the user interface. To add a metric to the UI Integrate the reporter library into your code Send a metrics via the reporter library Run the workload once to send initial data. Go to Jobs list: https://app.run.ai/jobs On the top right, use the settings wheel and select the metrics you have added","title":"Adding the Metrics to the User interface"},{"location":"home/components/","text":"Run:AI System Components \u00b6 Components \u00b6 Run:AI is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:AI Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Administrator User Interface The Run:AI Cluster \u00b6 The Run:AI Cluster contains: The Run:AI Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. Fractional GPU management. Responsible for the Run:AI Virtualization technology which allows Researchers to allocate parts of a GPU rather than a whole GPU The Run:AI agent. Responsible for sending Monitoring data to the Run:AI Cloud. Clusters require outbound network connectivity to the Run:AI Cloud. Kubernetes-Related Details \u00b6 The Run:AI cluster is installed as a Kubernetes Operator Run:AI is installed in its own namesapce runai Workloads are run in the context of Projects . Each project is a Kubernetes namespace with its own settings and access control. The Run:AI Cloud \u00b6 The Run:AI Cloud is the basis of the Administrator User Interface. The Run:AI cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:AI clusters.","title":"System Components"},{"location":"home/components/#runai-system-components","text":"","title":"Run:AI System Components"},{"location":"home/components/#components","text":"Run:AI is installed over a Kubernetes Cluster Researchers submit Machine Learning workloads via the Run:AI Command-Line Interface (CLI), or directly by sending YAML files to Kubernetes. Administrators monitor and set priorities via the Administrator User Interface","title":"Components"},{"location":"home/components/#the-runai-cluster","text":"The Run:AI Cluster contains: The Run:AI Scheduler which extends the Kubernetes scheduler. It uses business rules to schedule workloads sent by Researchers. Fractional GPU management. Responsible for the Run:AI Virtualization technology which allows Researchers to allocate parts of a GPU rather than a whole GPU The Run:AI agent. Responsible for sending Monitoring data to the Run:AI Cloud. Clusters require outbound network connectivity to the Run:AI Cloud.","title":"The Run:AI Cluster"},{"location":"home/components/#kubernetes-related-details","text":"The Run:AI cluster is installed as a Kubernetes Operator Run:AI is installed in its own namesapce runai Workloads are run in the context of Projects . Each project is a Kubernetes namespace with its own settings and access control.","title":"Kubernetes-Related Details"},{"location":"home/components/#the-runai-cloud","text":"The Run:AI Cloud is the basis of the Administrator User Interface. The Run:AI cloud aggregates monitoring information from multiple tenants (customers). Each customer may manage multiple Run:AI clusters.","title":"The Run:AI Cloud"},{"location":"home/whats-new/","text":"September 6th, 2020 \u00b6 We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Walk-through September 3rd, 2020 \u00b6 GPU Fractions now run in training and not only interactive. GPU Fractions training jobs can be preempted, bin-packed and consolidated like any integer jobs. See Run:AI Scheduler Fraction for more. August 10th, 2020 \u00b6 Run:AI Now supports Distributed Training and Gang Scheduling. For further information , see the Launch Distributed Training Workloads Walkthrough. August 4th, 2020 \u00b6 There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments July 28th, 2020 \u00b6 You can now enforce a cluster-wise setting which mandates all containers running using the Run:AI CLI to run as non root . For further information, see Enforce non-root Containers July 21th, 2020 \u00b6 It is now possible to mount a Persistent Storage Claim using the Run:AI CLI. See the --pvc flag in the runai submit CLI flag June 13th, 2020 \u00b6 New Settings for the Allocation of CPU and Memory \u00b6 It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory June 3rd, 2020 \u00b6 Node Group Affinity \u00b6 Projects now support Node Affinity. This feature allows the administrator to assign specific projects to run only on specific nodes (machines). Example use cases: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects Limit Duration of Interactive Jobs \u00b6 Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. For further information on how to set up duration limits see: Working with Projects May 24th, 2020 \u00b6 Kubernetes Operators \u00b6 Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: Upgrading a Run:AI Cluster Installation and Deleting a a Run:AI Cluster Installation March 3rd, 2020 \u00b6 Admin Overview Dashboard \u00b6 A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Whats New"},{"location":"home/whats-new/#september-6th-2020","text":"We released a module that helps the Researcher perform Hyperparameter optimization (HPO). HPO is about running many smaller experiments with varying parameters to help determine the optimal parameter set Hyperparameter Optimization Walk-through","title":"September 6th, 2020"},{"location":"home/whats-new/#september-3rd-2020","text":"GPU Fractions now run in training and not only interactive. GPU Fractions training jobs can be preempted, bin-packed and consolidated like any integer jobs. See Run:AI Scheduler Fraction for more.","title":"September 3rd, 2020"},{"location":"home/whats-new/#august-10th-2020","text":"Run:AI Now supports Distributed Training and Gang Scheduling. For further information , see the Launch Distributed Training Workloads Walkthrough.","title":"August 10th, 2020"},{"location":"home/whats-new/#august-4th-2020","text":"There is now an optional second level of Project hierarchy called Departments . For further information on how to configure and use Departments, see Working with Departments","title":"August 4th, 2020"},{"location":"home/whats-new/#july-28th-2020","text":"You can now enforce a cluster-wise setting which mandates all containers running using the Run:AI CLI to run as non root . For further information, see Enforce non-root Containers","title":"July 28th, 2020"},{"location":"home/whats-new/#july-21th-2020","text":"It is now possible to mount a Persistent Storage Claim using the Run:AI CLI. See the --pvc flag in the runai submit CLI flag","title":"July 21th, 2020"},{"location":"home/whats-new/#june-13th-2020","text":"","title":"June 13th, 2020"},{"location":"home/whats-new/#new-settings-for-the-allocation-of-cpu-and-memory","text":"It is now possible to set limits for CPU and memory as well as to establish defaults based on the ratio of GPU to CPU and GPU to memory. For further information see: Allocation of CPU and Memory","title":"New Settings for the Allocation of CPU and Memory"},{"location":"home/whats-new/#june-3rd-2020","text":"","title":"June 3rd, 2020"},{"location":"home/whats-new/#node-group-affinity","text":"Projects now support Node Affinity. This feature allows the administrator to assign specific projects to run only on specific nodes (machines). Example use cases: The project team needs specialized hardware (e.g. with enough memory) The project team is the owner of specific hardware which was acquired with a specialized budget We want to direct build/interactive workloads to work on weaker hardware and direct longer training/unattended workloads to faster nodes For further information see: Working with Projects","title":"Node Group Affinity"},{"location":"home/whats-new/#limit-duration-of-interactive-jobs","text":"Researchers frequently forget to close Interactive jobs. This may lead to a waste of resources. Some organizations prefer to limit the duration of interactive jobs and close them automatically. For further information on how to set up duration limits see: Working with Projects","title":"Limit Duration of Interactive Jobs"},{"location":"home/whats-new/#may-24th-2020","text":"","title":"May 24th, 2020"},{"location":"home/whats-new/#kubernetes-operators","text":"Cluster installation now works with Kubernetes Operators . Operators make it easy to install, update, and delete a Run:AI cluster. For further information see: Upgrading a Run:AI Cluster Installation and Deleting a a Run:AI Cluster Installation","title":"Kubernetes Operators"},{"location":"home/whats-new/#march-3rd-2020","text":"","title":"March 3rd, 2020"},{"location":"home/whats-new/#admin-overview-dashboard","text":"A new admin overview dashboard which shows a more holistic view of multiple clusters. Applicable for customers with more than one cluster.","title":"Admin Overview Dashboard"}]}